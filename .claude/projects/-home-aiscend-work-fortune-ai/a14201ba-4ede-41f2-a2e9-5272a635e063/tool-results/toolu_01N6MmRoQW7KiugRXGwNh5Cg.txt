     1→from __future__ import annotations
     2→
     3→import json
     4→import os
     5→import re
     6→from string import Template
     7→from typing import Any, Dict, List, Optional, Tuple
     8→
     9→from fastapi import APIRouter, Request
    10→from fastapi.responses import JSONResponse
    11→from pydantic import BaseModel, Field
    12→
    13→from api.deps import require_auth, require_csrf
    14→from common.a2ui import validate_a2ui
    15→from services import chat_service
    16→from services import glm_client
    17→from services import bazi_facts
    18→from services import kb_service
    19→from services import agent_service_client
    20→from services import soul_os
    21→from stores import fortune_db
    22→
    23→
    24→router = APIRouter(prefix="/api/chat", tags=["chat"])
    25→
    26→
    27→def _ok(data: Dict[str, Any] | None = None) -> JSONResponse:
    28→    return JSONResponse({"ok": True, "data": data or {}})
    29→
    30→
    31→def _err(status: int, code: str, message: str, detail: Optional[Dict[str, Any]] = None) -> JSONResponse:
    32→    return JSONResponse(
    33→        {"ok": False, "error": {"code": code, "message": message, "detail": detail or {}}},
    34→        status_code=status,
    35→    )
    36→
    37→
    38→class ChatSendRequest(BaseModel):
    39→    session_id: Optional[str] = Field(None)
    40→    text: str = Field(..., min_length=1)
    41→
    42→
    43→_UUID_RE = re.compile(r"^[0-9a-fA-F-]{36}$")
    44→
    45→
    46→SYSTEM_PROMPT_TEMPLATE = Template("""你是 Fortune AI 的对话 Agent，角色是积极心理学教练（Performance Coach）。
    47→
    48→【产品定位】
    49→人生导航 / 陪伴 / 提升。系统和交互保持“有效而极简”。
    50→
    51→【你的优先级（不可逆）】
    52→Coach > Teaching Assistant > Customer Support > Sales
    53→
    54→【硬性规则（必须遵守）】
    55→1) 禁止恐吓、羞辱、宿命论断言；负面信息必须紧接“你可以做什么”的行动处方。
    56→2) 禁止自行计算八字事实；只能基于提供的 facts + evidence 输出。
    57→3) 每次输出必须包含：结论(conclusion) + 依据(why) + ≤3条处方(prescriptions) + 承诺邀请(commitment_ask)。
    58→4) 时间窗口默认只给干预窗口（intervention）；forecast 只能条件句+低置信度。
    59→5) 输出必须是 A2UI JSON，且第一组件必须是 markdown_text。
    60→6) 必须给出可点击 actions（start_task / schedule_task / open_panel / opt_out）。
    61→
    62→【语言风格 persona_style】
    63→standard：清晰、中性、专业
    64→warm：共情、支持性（默认）
    65→roast：轻毒舌但不羞辱、不对人格做负面定性
    66→
    67→【输入（系统已注入）】
    68→persona_style: $persona_style
    69→user_context: $user_context_json
    70→facts: $facts_json
    71→evidence: $evidence_json
    72→
    73→【输出（必须严格仅输出 JSON）】
    74→返回 A2UI JSON，结构：
    75→- meta.summary：一句话摘要
    76→- ui_components[0]：markdown_text（必须，包含：结论要点/依据/处方/时间窗口/边界/承诺邀请）
    77→- ui_components[1]：action_buttons（必须，按钮结构：{label, action:{type, ...}}）
    78→
    79→严格按以下字段名输出（不要用 content/actions/action_type/payload 等变体；不要代码块；不要额外文本）：
    80→{
    81→  "meta": {"summary": "..."},
    82→  "ui_components": [
    83→    {"type":"markdown_text","title":"教练回复","data":"...markdown..."},
    84→    {"type":"action_buttons","title":"下一步","data":[
    85→      {"label":"开始（2-5分钟）","action":{"type":"start_task","task_id":"..."}},
    86→      {"label":"加入今日计划","action":{"type":"schedule_task","task_id":"..."}},
    87→      {"label":"打开功能区","action":{"type":"open_panel","panel":"bento"}},
    88→      {"label":"先不需要","action":{"type":"opt_out"}}
    89→    ]}
    90→  ]
    91→}
    92→""")
    93→
    94→
    95→def _load_user_context(user_id: int) -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:
    96→    profile = fortune_db.fetch_one(
    97→        "SELECT name, gender, birthday_local, tz_offset_hours, location FROM fortune_user WHERE user_id=%s AND deleted_at IS NULL",
    98→        (int(user_id),),
    99→    ) or {}
   100→    prefs = fortune_db.fetch_one(
   101→        "SELECT persona_style, push_enabled, push_time, quiet_hours_start, quiet_hours_end, chat_backend FROM fortune_user_preferences WHERE user_id=%s",
   102→        (int(user_id),),
   103→    ) or {}
   104→    snap = fortune_db.fetch_one(
   105→        "SELECT facts, facts_hash, compute_version FROM fortune_bazi_snapshot WHERE user_id=%s ORDER BY created_at DESC LIMIT 1",
   106→        (int(user_id),),
   107→    ) or {}
   108→    return profile, prefs, snap
   109→
   110→
   111→def _write_agent_run_log(
   112→    user_id: int,
   113→    session_id: Optional[str],
   114→    agent_name: str,
   115→    prompt_version: str,
   116→    facts_hash: Optional[str],
   117→    input_data: Dict[str, Any],
   118→    output_data: Dict[str, Any],
   119→    tool_calls: Optional[List[Dict[str, Any]]] = None,
   120→    usage: Optional[Dict[str, Any]] = None,
   121→    latency_ms: Optional[int] = None,
   122→    error: Optional[str] = None,
   123→) -> None:
   124→    """
   125→    Write audit log to fortune_agent_run table.
   126→
   127→    REQ: REQ-AGENT-006 (Audit)
   128→    """
   129→    try:
   130→        fortune_db.execute(
   131→            """
   132→            INSERT INTO fortune_agent_run (
   133→                user_id, session_id, agent_name, prompt_version, facts_hash,
   134→                input, output, tool_calls, usage, latency_ms, error, created_at
   135→            ) VALUES (%s, %s::uuid, %s, %s, %s, %s::jsonb, %s::jsonb, %s::jsonb, %s::jsonb, %s, %s, now())
   136→            """,
   137→            [
   138→                user_id,
   139→                session_id if session_id else None,
   140→                agent_name,
   141→                prompt_version,
   142→                facts_hash,
   143→                json.dumps(input_data, ensure_ascii=False, default=str),
   144→                json.dumps(output_data, ensure_ascii=False, default=str),
   145→                json.dumps(tool_calls, ensure_ascii=False) if tool_calls else None,
   146→                json.dumps(usage, ensure_ascii=False) if usage else None,
   147→                latency_ms,
   148→                error,
   149→            ],
   150→        )
   151→    except Exception as e:
   152→        # Log but don't fail the request
   153→        import logging
   154→        logging.getLogger(__name__).warning("Failed to write agent run log: %s", str(e))
   155→
   156→
   157→def _a2ui_from_text(text: str) -> Dict[str, Any]:
   158→    raw = (text or "").strip()
   159→    obj = glm_client.extract_json_object(raw)
   160→    if isinstance(obj, dict):
   161→        norm = _normalize_a2ui(obj)
   162→        if isinstance(norm, dict):
   163→            try:
   164→                validate_a2ui(norm)
   165→                return norm
   166→            except Exception:
   167→                pass
   168→    # Fallback: wrap as markdown_text A2UI
   169→    summary = raw.splitlines()[0].strip()[:120] if raw else "输出"
   170→    return {
   171→        "meta": {"summary": summary},
   172→        "ui_components": [
   173→            {"type": "markdown_text", "title": "输出", "data": raw or "（空）"},
   174→            {"type": "action_buttons", "title": "下一步", "data": []},
   175→        ],
   176→    }
   177→
   178→
   179→def _normalize_action(action: Dict[str, Any]) -> Dict[str, Any]:
   180→    a = dict(action or {})
   181→    # common variants
   182→    if a.get("type") == "open_panel" and "panel" not in a and "panel_id" in a:
   183→        a["panel"] = a.get("panel_id")
   184→    return a
   185→
   186→
   187→def _normalize_action_button(btn: Any) -> Dict[str, Any]:
   188→    if not isinstance(btn, dict):
   189→        return {"label": str(btn), "action": {"type": "opt_out"}}
   190→    label = str(btn.get("label") or btn.get("text") or "下一步")
   191→    action = btn.get("action")
   192→    if isinstance(action, dict):
   193→        a = _normalize_action(action)
   194→        if not a.get("type"):
   195→            a["type"] = "opt_out"
   196→        return {"label": label, "action": a}
   197→    action_type = btn.get("action_type") or btn.get("type") or "opt_out"
   198→    payload = btn.get("payload") if isinstance(btn.get("payload"), dict) else {}
   199→    a = {"type": str(action_type)}
   200→    a.update(payload)
   201→    return {"label": label, "action": _normalize_action(a)}
   202→
   203→
   204→def _normalize_a2ui(obj: Dict[str, Any]) -> Optional[Dict[str, Any]]:
   205→    if not isinstance(obj, dict):
   206→        return None
   207→    meta = obj.get("meta")
   208→    if not isinstance(meta, dict):
   209→        meta = {}
   210→
   211→    comps = obj.get("ui_components")
   212→    if not isinstance(comps, list):
   213→        # tolerate alternative root key
   214→        comps = obj.get("components") if isinstance(obj.get("components"), list) else []
   215→
   216→    out_comps: List[Dict[str, Any]] = []
   217→    for c in comps:
   218→        if not isinstance(c, dict):
   219→            continue
   220→        t = str(c.get("type") or "").strip()
   221→        if not t:
   222→            continue
   223→        title = str(c.get("title") or ("教练回复" if t == "markdown_text" else "下一步" if t == "action_buttons" else "组件"))
   224→        data = c.get("data")
   225→        if data is None and "content" in c:
   226→            data = c.get("content")
   227→        if t == "action_buttons":
   228→            if data is None and "actions" in c:
   229→                data = c.get("actions")
   230→            if isinstance(data, list):
   231→                data = [_normalize_action_button(b) for b in data]
   232→            else:
   233→                data = []
   234→        if t == "markdown_text" and not isinstance(data, str):
   235→            data = "" if data is None else str(data)
   236→        out_comps.append({"type": t, "title": title, "data": data})
   237→
   238→    if not out_comps:
   239→        return None
   240→    # Ensure markdown_text is first component for renderer compatibility.
   241→    for i, c in enumerate(out_comps):
   242→        if c.get("type") == "markdown_text":
   243→            if i != 0:
   244→                out_comps.insert(0, out_comps.pop(i))
   245→            break
   246→
   247→    meta_out = dict(meta)
   248→    if not isinstance(meta_out.get("summary"), str):
   249→        meta_out["summary"] = str(meta_out.get("summary") or "")
   250→    return {"meta": meta_out, "ui_components": out_comps}
   251→
   252→
   253→_PRES_LINE_RE = re.compile(r"^\s*\d+[\.\)]\s*(.+?)\s*$")
   254→_BULLET_LINE_RE = re.compile(r"^\s*[-*]\s*(.+?)\s*$")
   255→_MD_INLINE_RE = re.compile(r"[*_`]+")
   256→
   257→
   258→def _strip_md(text: str) -> str:
   259→    s = _MD_INLINE_RE.sub("", text or "")
   260→    s = re.sub(r"\s+", " ", s).strip()
   261→    return s
   262→
   263→
   264→def _short_label(text: str, max_len: int = 14) -> str:
   265→    s = _strip_md(text)
   266→    s = re.sub(r"^\d+[\.\)]\s*", "", s).strip()
   267→    if len(s) > max_len:
   268→        s = s[: max_len - 3].rstrip() + "..."
   269→    return s or "行动"
   270→
   271→
   272→def _extract_prescriptions(md: str) -> List[str]:
   273→    lines = [ln.strip() for ln in (md or "").splitlines() if ln.strip()]
   274→    start: Optional[int] = None
   275→    for i, ln in enumerate(lines):
   276→        if "处方" in ln and not _PRES_LINE_RE.match(ln):
   277→            start = i + 1
   278→            break
   279→    if start is None:
   280→        return []
   281→    out: List[str] = []
   282→    for ln in lines[start:]:
   283→        if ln.startswith("#"):
   284→            break
   285→        m = _PRES_LINE_RE.match(ln) or _BULLET_LINE_RE.match(ln)
   286→        if m:
   287→            item = _strip_md(m.group(1))
   288→            if item:
   289→                out.append(item)
   290→        if len(out) >= 3:
   291→            break
   292→    return out
   293→
   294→
   295→def _insert_suggested_tasks(user_id: int, session_id: str, prescriptions: List[str]) -> List[Dict[str, Any]]:
   296→    tasks: List[Dict[str, Any]] = []
   297→    for p in prescriptions[:3]:
   298→        title = _strip_md(p) or "行动"
   299→        row = fortune_db.execute_returning_one(
   300→            """
   301→            INSERT INTO fortune_commitment (user_id, session_id, source, commitment_type, title, details, status)
   302→            VALUES (%s, %s, 'chat', 'start_task', %s, '{}'::jsonb, 'suggested')
   303→            RETURNING task_id
   304→            """,
   305→            (int(user_id), session_id, title[:200]),
   306→        )
   307→        if row and row.get("task_id"):
   308→            tasks.append({"task_id": str(row["task_id"]), "title": title})
   309→    return tasks
   310→
   311→
   312→def _ensure_action_buttons(a2ui: Dict[str, Any], suggested_tasks: List[Dict[str, Any]]) -> None:
   313→    buttons = []
   314→    for t in suggested_tasks[:2]:
   315→        title = _short_label(str(t.get("title") or ""))
   316→        buttons.append({"label": f"开始：{title}", "action": {"type": "start_task", "task_id": str(t["task_id"])}})
   317→    if suggested_tasks:
   318→        buttons.append({"label": "加入今日计划", "action": {"type": "schedule_task", "task_id": str(suggested_tasks[0]["task_id"])}})
   319→    buttons.append({"label": "打开功能区", "action": {"type": "open_panel", "panel": "bento"}})
   320→    buttons.append({"label": "先不需要", "action": {"type": "opt_out"}})
   321→
   322→    comps = a2ui.get("ui_components")
   323→    if not isinstance(comps, list):
   324→        a2ui["ui_components"] = comps = []
   325→
   326→    for c in comps:
   327→        if isinstance(c, dict) and c.get("type") == "action_buttons":
   328→            c["data"] = buttons
   329→            return
   330→
   331→    comps.append({"type": "action_buttons", "title": "下一步", "data": buttons})
   332→
   333→
   334→def _assistant_markdown(a2ui: Dict[str, Any]) -> str:
   335→    comps = a2ui.get("ui_components") or []
   336→    if isinstance(comps, list) and comps:
   337→        first = comps[0] if isinstance(comps[0], dict) else None
   338→        if first and isinstance(first.get("data"), str):
   339→            return first["data"]
   340→    return json.dumps(a2ui, ensure_ascii=False)
   341→
   342→
   343→@router.post("/send")
   344→def chat_send(req: ChatSendRequest, request: Request):
   345→    """
   346→    Send a chat message and get AI response.
   347→
   348→    Supports two backends based on user preference:
   349→    - fastapi: Direct LLM call (default, current implementation)
   350→    - agent_service: Vercel AI SDK orchestration (future)
   351→
   352→    Also writes audit log to fortune_agent_run for compliance.
   353→    """
   354→    import time as time_module
   355→
   356→    auth = require_auth(request)
   357→    require_csrf(request, auth)
   358→    user_id = int(auth["user_id"])
   359→
   360→    session_id = (req.session_id or "").strip()
   361→    if session_id and not _UUID_RE.match(session_id):
   362→        return _err(400, "invalid_request", "invalid_session_id")
   363→
   364→    if not session_id:
   365→        s = chat_service.create_session(user_id, title="")
   366→        session_id = s["session_id"]
   367→    else:
   368→        if not chat_service.get_session(user_id, session_id):
   369→            return _err(404, "not_found", "session_not_found")
   370→
   371→    user_text = req.text.strip()
   372→    if not user_text:
   373→        return _err(400, "invalid_request", "empty_text")
   374→
   375→    model_name = (os.getenv("FORTUNE_AI_GLM_MODEL") or "glm-4.7").strip()
   376→
   377→    # Save user message
   378→    chat_service.append_message(session_id=session_id, user_id=user_id, role="user", content=user_text, model=model_name)
   379→    chat_service.set_title_if_empty(session_id, user_text[:24])
   380→
   381→    profile, prefs, snap = _load_user_context(user_id)
   382→    if not snap or not snap.get("facts") or not snap.get("facts_hash"):
   383→        snap = bazi_facts.ensure_snapshot_for_user(user_id)
   384→
   385→    # 使用 soul_os 获取完整上下文
   386→    full_context = soul_os.get_full_context_for_chat(user_id, user_text)
   387→
   388→    # 反依赖检查
   389→    anti_dep = full_context.get("anti_dependency", {})
   390→    if anti_dep.get("should_intervene"):
   391→        # 返回反依赖干预消息
   392→        intervention_msg = anti_dep.get("intervention_message", "")
   393→        a2ui = {
   394→            "meta": {"summary": "让我们先行动起来"},
   395→            "ui_components": [
   396→                {"type": "markdown_text", "title": "教练提醒", "data": intervention_msg},
   397→                {"type": "action_buttons", "title": "下一步", "data": [
   398→                    {"label": "好的，我去做", "action": {"type": "opt_out"}},
   399→                    {"label": "打开任务", "action": {"type": "open_panel", "panel": "tasks"}},
   400→                ]},
   401→            ],
   402→        }
   403→        chat_service.append_message(
   404→            session_id=session_id,
   405→            user_id=user_id,
   406→            role="assistant",
   407→            content=intervention_msg,
   408→            a2ui=a2ui,
   409→            model="anti_dependency",
   410→        )
   411→        return _ok({
   412→            "session_id": session_id,
   413→            "assistant_message": {"role": "assistant", "a2ui": a2ui},
   414→            "suggested_tasks": [],
   415→            "backend": "anti_dependency",
   416→            "intervention_type": anti_dep.get("intervention_type", ""),
   417→        })
   418→
   419→    # Check chat backend preference
   420→    chat_backend = str(prefs.get("chat_backend") or "fastapi")
   421→    start_time = time_module.time()
   422→    persona_style = full_context.get("persona_style", "warm")
   423→    facts_obj = snap.get("facts") or {}
   424→    facts_hash = str(snap.get("facts_hash") or "")
   425→
   426→    if chat_backend == "agent_service":
   427→        # Try Agent Service backend
   428→        try:
   429→            # Build messages for agent service
   430→            hist = chat_service.get_recent_messages_for_llm(user_id, session_id, limit=10)
   431→            agent_messages = [{"role": m.get("role", "user"), "content": str(m.get("content") or "")} for m in hist]
   432→
   433→            agent_response = agent_service_client.call_agent_service_sync(
   434→                user_id=user_id,
   435→                session_id=session_id,
   436→                messages=agent_messages,
   437→                user_context={"profile": profile, "preferences": prefs},
   438→                facts=facts_obj,
   439→                persona_style=persona_style,
   440→            )
   441→
   442→            # Process response
   443→            a2ui = agent_response.a2ui or _a2ui_from_text(agent_response.text)
   444→            md = _assistant_markdown(a2ui)
   445→            prescriptions = _extract_prescriptions(md)
   446→            suggested_tasks = _insert_suggested_tasks(user_id, session_id, prescriptions)
   447→            _ensure_action_buttons(a2ui, suggested_tasks)
   448→
   449→            chat_service.append_message(
   450→                session_id=session_id,
   451→                user_id=user_id,
   452→                role="assistant",
   453→                content=md,
   454→                a2ui=a2ui,
   455→                model="agent_service",
   456→            )
   457→
   458→            # Write audit log
   459→            latency_ms = int((time_module.time() - start_time) * 1000)
   460→            _write_agent_run_log(
   461→                user_id=user_id,
   462→                session_id=session_id,
   463→                agent_name="coach",
   464→                prompt_version="agent_service",
   465→                facts_hash=facts_hash,
   466→                input_data={"user_text": user_text, "backend": "agent_service"},
   467→                output_data={"a2ui_summary": a2ui.get("meta", {}).get("summary", ""), "suggested_tasks": len(suggested_tasks)},
   468→                latency_ms=latency_ms,
   469→            )
   470→
   471→            return _ok(
   472→                {
   473→                    "session_id": session_id,
   474→                    "assistant_message": {"role": "assistant", "a2ui": a2ui},
   475→                    "suggested_tasks": suggested_tasks,
   476→                    "backend": "agent_service",
   477→                }
   478→            )
   479→
   480→        except agent_service_client.AgentServiceUnavailable:
   481→            # Fall back to FastAPI if agent service unavailable
   482→            pass
   483→        except agent_service_client.AgentServiceError as e:
   484→            # Log error and fall back
   485→            latency_ms = int((time_module.time() - start_time) * 1000)
   486→            _write_agent_run_log(
   487→                user_id=user_id,
   488→                session_id=session_id,
   489→                agent_name="coach",
   490→                prompt_version="agent_service",
   491→                facts_hash=facts_hash,
   492→                input_data={"user_text": user_text, "backend": "agent_service"},
   493→                output_data={},
   494→                latency_ms=latency_ms,
   495→                error=str(e),
   496→            )
   497→            # Fall back to FastAPI
   498→            pass
   499→
   500→    # === FastAPI Direct LLM Implementation ===
   501→    # 使用 soul_os 构建的 system prompt 和 evidence
   502→    user_context = full_context.get("user_context", {})
   503→    evidence = full_context.get("evidence", {})
   504→
   505→    user_context_json = json.dumps(user_context, ensure_ascii=False, default=str)
   506→    facts_json = json.dumps(facts_obj, ensure_ascii=False)
   507→    evidence_json = json.dumps(evidence, ensure_ascii=False)
   508→
   509→    # 使用 soul_os 生成的 system prompt
   510→    system = full_context.get("system_prompt", "")
   511→    if not system:
   512→        system = SYSTEM_PROMPT_TEMPLATE.safe_substitute(
   513→            persona_style=persona_style,
   514→            user_context_json=user_context_json,
   515→            facts_json=facts_json,
   516→            evidence_json=evidence_json,
   517→        )
   518→
   519→    # 从 evidence 中提取 kb_refs 用于审计
   520→    kb_refs = evidence.get("kb_refs", [])
   521→
   522→    # Build LLM messages from DB (last 10 turns)
   523→    hist = chat_service.get_recent_messages_for_llm(user_id, session_id, limit=10)
   524→    llm_messages = [{"role": "system", "content": system}]
   525→    for m in hist:
   526→        role = "assistant" if m.get("role") == "assistant" else "user"
   527→        llm_messages.append({"role": role, "content": str(m.get("content") or "")})
   528→
   529→    error_msg = None
   530→    try:
   531→        raw_text, raw = glm_client.call_chat_completions(messages=llm_messages)
   532→    except glm_client.GlmError as e:
   533→        error_msg = str(e)
   534→        # Write audit log for failed request
   535→        latency_ms = int((time_module.time() - start_time) * 1000)
   536→        _write_agent_run_log(
   537→            user_id=user_id,
   538→            session_id=session_id,
   539→            agent_name="coach",
   540→            prompt_version="v1.0",
   541→            facts_hash=facts_hash,
   542→            input_data={"user_text": user_text, "kb_refs": kb_refs},
   543→            output_data={},
   544→            latency_ms=latency_ms,
   545→            error=error_msg,
   546→        )
   547→        return _err(500, "glm_error", error_msg)
   548→
   549→    a2ui = _a2ui_from_text(raw_text)
   550→    md = _assistant_markdown(a2ui)
   551→    prescriptions = _extract_prescriptions(md)
   552→    suggested_tasks = _insert_suggested_tasks(user_id, session_id, prescriptions)
   553→    _ensure_action_buttons(a2ui, suggested_tasks)
   554→
   555→    usage = raw.get("usage") if isinstance(raw, dict) else {}
   556→    prompt_tokens = int(usage.get("prompt_tokens") or usage.get("input_tokens") or 0) if isinstance(usage, dict) else 0
   557→    completion_tokens = int(usage.get("completion_tokens") or usage.get("output_tokens") or 0) if isinstance(usage, dict) else 0
   558→
   559→    chat_service.append_message(
   560→        session_id=session_id,
   561→        user_id=user_id,
   562→        role="assistant",
   563→        content=md,
   564→        a2ui=a2ui,
   565→        model=str(raw.get("model") or model_name),
   566→        prompt_tokens=prompt_tokens,
   567→        completion_tokens=completion_tokens,
   568→    )
   569→
   570→    # Write audit log for successful request
   571→    latency_ms = int((time_module.time() - start_time) * 1000)
   572→    _write_agent_run_log(
   573→        user_id=user_id,
   574→        session_id=session_id,
   575→        agent_name="coach",
   576→        prompt_version="v1.0",
   577→        facts_hash=facts_hash,
   578→        input_data={"user_text": user_text, "kb_refs": kb_refs},
   579→        output_data={"a2ui_summary": a2ui.get("meta", {}).get("summary", ""), "suggested_tasks": len(suggested_tasks)},
   580→        usage={"prompt_tokens": prompt_tokens, "completion_tokens": completion_tokens, "total_tokens": prompt_tokens + completion_tokens},
   581→        latency_ms=latency_ms,
   582→    )
   583→
   584→    return _ok(
   585→        {
   586→            "session_id": session_id,
   587→            "assistant_message": {"role": "assistant", "a2ui": a2ui},
   588→            "suggested_tasks": suggested_tasks,
   589→            "backend": "fastapi",
   590→        }
   591→    )
   592→
   593→
   594→@router.post("/ask")
   595→def chat_ask_alias(req: ChatSendRequest, request: Request):
   596→    # Backward-compatible alias for /api/chat/send
   597→    return chat_send(req, request)
   598→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
