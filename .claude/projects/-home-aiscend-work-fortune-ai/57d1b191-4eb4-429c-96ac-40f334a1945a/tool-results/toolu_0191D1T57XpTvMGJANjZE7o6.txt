     1→from __future__ import annotations
     2→
     3→import json
     4→import os
     5→import re
     6→from string import Template
     7→from typing import Any, Dict, List, Optional, Tuple
     8→
     9→from fastapi import APIRouter, Request, Query
    10→from fastapi.responses import JSONResponse, StreamingResponse
    11→from pydantic import BaseModel, Field
    12→
    13→from api.deps import require_auth, require_csrf
    14→from common.a2ui import validate_a2ui
    15→from services import chat_service
    16→from services import glm_client
    17→from services import bazi_facts
    18→from services import kb_service
    19→from services import agent_service_client
    20→from services import soul_os
    21→from services.chat_backend_config import default_chat_backend
    22→from services.yunshi_service import yunshi_service, build_daily_yunshi_a2ui, build_annual_yunshi_a2ui
    23→from stores import fortune_db
    24→
    25→
    26→router = APIRouter(prefix="/api/chat", tags=["chat"])
    27→
    28→
    29→def _ok(data: Dict[str, Any] | None = None) -> JSONResponse:
    30→    return JSONResponse({"ok": True, "data": data or {}})
    31→
    32→
    33→def _err(status: int, code: str, message: str, detail: Optional[Dict[str, Any]] = None) -> JSONResponse:
    34→    return JSONResponse(
    35→        {"ok": False, "error": {"code": code, "message": message, "detail": detail or {}}},
    36→        status_code=status,
    37→    )
    38→
    39→
    40→class ChatSendRequest(BaseModel):
    41→    session_id: Optional[str] = Field(None)
    42→    text: str = Field(..., min_length=1)
    43→
    44→
    45→_UUID_RE = re.compile(r"^[0-9a-fA-F-]{36}$")
    46→
    47→
    48→SYSTEM_PROMPT_TEMPLATE = Template("""你是 Fortune AI 的对话 Agent，角色是积极心理学教练（Performance Coach）。
    49→
    50→【产品定位】
    51→人生导航 / 陪伴 / 提升。系统和交互保持“有效而极简”。
    52→
    53→【你的优先级（不可逆）】
    54→Coach > Teaching Assistant > Customer Support > Sales
    55→
    56→【硬性规则（必须遵守）】
    57→1) 禁止恐吓、羞辱、宿命论断言；负面信息必须紧接“你可以做什么”的行动处方。
    58→2) 禁止自行计算八字事实；只能基于提供的 facts + evidence 输出。
    59→3) 每次输出必须包含：结论(conclusion) + 依据(why) + ≤3条处方(prescriptions) + 承诺邀请(commitment_ask)。
    60→4) 时间窗口默认只给干预窗口（intervention）；forecast 只能条件句+低置信度。
    61→5) 输出必须是 A2UI JSON，且第一组件必须是 markdown_text。
    62→6) 必须给出可点击 actions（start_task / schedule_task / open_panel / opt_out）。
    63→
    64→【语言风格 persona_style】
    65→standard：清晰、中性、专业
    66→warm：共情、支持性（默认）
    67→roast：轻毒舌但不羞辱、不对人格做负面定性
    68→
    69→【输入（系统已注入）】
    70→persona_style: $persona_style
    71→user_context: $user_context_json
    72→facts: $facts_json
    73→evidence: $evidence_json
    74→
    75→【输出（必须严格仅输出 JSON）】
    76→返回 A2UI JSON，结构：
    77→- meta.summary：一句话摘要
    78→- ui_components[0]：markdown_text（必须，包含：结论要点/依据/处方/时间窗口/边界/承诺邀请）
    79→- ui_components[1]：action_buttons（必须，按钮结构：{label, action:{type, ...}}）
    80→
    81→严格按以下字段名输出（不要用 content/actions/action_type/payload 等变体；不要代码块；不要额外文本）：
    82→{
    83→  "meta": {"summary": "..."},
    84→  "ui_components": [
    85→    {"type":"markdown_text","title":"教练回复","data":"...markdown..."},
    86→    {"type":"action_buttons","title":"下一步","data":[
    87→      {"label":"开始（2-5分钟）","action":{"type":"start_task","task_id":"..."}},
    88→      {"label":"加入今日计划","action":{"type":"schedule_task","task_id":"..."}},
    89→      {"label":"打开功能区","action":{"type":"open_panel","panel":"bento"}},
    90→      {"label":"先不需要","action":{"type":"opt_out"}}
    91→    ]}
    92→  ]
    93→}
    94→""")
    95→
    96→SYSTEM_PROMPT_STREAM_TEMPLATE = Template("""你是 Fortune AI 的对话 Agent，角色是积极心理学教练（Performance Coach）。
    97→
    98→【产品定位】
    99→人生导航 / 陪伴 / 提升。系统和交互保持“有效而极简”。
   100→
   101→【你的优先级（不可逆）】
   102→Coach > Teaching Assistant > Customer Support > Sales
   103→
   104→【硬性规则（必须遵守）】
   105→1) 禁止恐吓、羞辱、宿命论断言；负面信息必须紧接“你可以做什么”的行动处方。
   106→2) 禁止自行计算八字事实；只能基于提供的 facts + evidence 输出。
   107→3) 每次输出必须包含：结论(conclusion) + 依据(why) + ≤3条处方(prescriptions) + 承诺邀请(commitment_ask)。
   108→4) 每条处方必须包含 if_then：如果____→那么____；并优先给 2–5 分钟可启动版本。
   109→5) 只输出 Markdown 文本，不要 JSON/代码块/额外说明。
   110→
   111→【语言风格 persona_style】
   112→standard：清晰、中性、专业
   113→warm：共情、支持性（默认）
   114→roast：轻毒舌但不羞辱、不对人格做负面定性
   115→
   116→【输入（系统已注入）】
   117→persona_style: $persona_style
   118→user_context: $user_context_json
   119→facts: $facts_json
   120→evidence: $evidence_json
   121→
   122→【输出（Markdown）】
   123→用如下结构输出（标题可省略但必须包含“处方”一节）：
   124→
   125→结论：...
   126→依据：...
   127→处方：
   128→1) ...（包含 if_then）
   129→2) ...（包含 if_then）
   130→3) ...（包含 if_then）
   131→承诺邀请：...（一句话，邀请用户选择一个处方开始）
   132→""")
   133→
   134→
   135→def _load_user_context(user_id: int) -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:
   136→    profile = fortune_db.fetch_one(
   137→        "SELECT name, gender, birthday_local, tz_offset_hours, location FROM fortune_user WHERE user_id=%s AND deleted_at IS NULL",
   138→        (int(user_id),),
   139→    ) or {}
   140→    prefs = fortune_db.fetch_one(
   141→        "SELECT persona_style, push_enabled, push_time, quiet_hours_start, quiet_hours_end, chat_backend FROM fortune_user_preferences WHERE user_id=%s",
   142→        (int(user_id),),
   143→    ) or {}
   144→    snap = fortune_db.fetch_one(
   145→        "SELECT facts, facts_hash, compute_version FROM fortune_bazi_snapshot WHERE user_id=%s ORDER BY created_at DESC LIMIT 1",
   146→        (int(user_id),),
   147→    ) or {}
   148→    return profile, prefs, snap
   149→
   150→
   151→def _write_agent_run_log(
   152→    user_id: int,
   153→    session_id: Optional[str],
   154→    agent_name: str,
   155→    prompt_version: str,
   156→    facts_hash: Optional[str],
   157→    input_data: Dict[str, Any],
   158→    output_data: Dict[str, Any],
   159→    tool_calls: Optional[List[Dict[str, Any]]] = None,
   160→    usage: Optional[Dict[str, Any]] = None,
   161→    latency_ms: Optional[int] = None,
   162→    error: Optional[str] = None,
   163→) -> None:
   164→    """
   165→    Write audit log to fortune_agent_run table.
   166→
   167→    REQ: REQ-AGENT-006 (Audit)
   168→    """
   169→    try:
   170→        fortune_db.execute(
   171→            """
   172→            INSERT INTO fortune_agent_run (
   173→                user_id, session_id, agent_name, prompt_version, facts_hash,
   174→                input, output, tool_calls, usage, latency_ms, error, created_at
   175→            ) VALUES (%s, %s::uuid, %s, %s, %s, %s::jsonb, %s::jsonb, %s::jsonb, %s::jsonb, %s, %s, now())
   176→            """,
   177→            [
   178→                user_id,
   179→                session_id if session_id else None,
   180→                agent_name,
   181→                prompt_version,
   182→                facts_hash,
   183→                json.dumps(input_data, ensure_ascii=False, default=str),
   184→                json.dumps(output_data, ensure_ascii=False, default=str),
   185→                json.dumps(tool_calls, ensure_ascii=False) if tool_calls else None,
   186→                json.dumps(usage, ensure_ascii=False) if usage else None,
   187→                latency_ms,
   188→                error,
   189→            ],
   190→        )
   191→    except Exception as e:
   192→        # Log but don't fail the request
   193→        import logging
   194→        logging.getLogger(__name__).warning("Failed to write agent run log: %s", str(e))
   195→
   196→
   197→def _a2ui_from_text(text: str) -> Dict[str, Any]:
   198→    raw = (text or "").strip()
   199→    obj = glm_client.extract_json_object(raw)
   200→    if isinstance(obj, dict):
   201→        norm = _normalize_a2ui(obj)
   202→        if isinstance(norm, dict):
   203→            try:
   204→                validate_a2ui(norm)
   205→                return norm
   206→            except Exception:
   207→                pass
   208→    # Fallback: wrap as markdown_text A2UI
   209→    summary = raw.splitlines()[0].strip()[:120] if raw else "输出"
   210→    return {
   211→        "meta": {"summary": summary},
   212→        "ui_components": [
   213→            {"type": "markdown_text", "title": "输出", "data": raw or "（空）"},
   214→            {"type": "action_buttons", "title": "下一步", "data": []},
   215→        ],
   216→    }
   217→
   218→
   219→def _normalize_action(action: Dict[str, Any]) -> Dict[str, Any]:
   220→    a = dict(action or {})
   221→    # common variants
   222→    if a.get("type") == "open_panel" and "panel" not in a and "panel_id" in a:
   223→        a["panel"] = a.get("panel_id")
   224→    return a
   225→
   226→
   227→def _normalize_action_button(btn: Any) -> Dict[str, Any]:
   228→    if not isinstance(btn, dict):
   229→        return {"label": str(btn), "action": {"type": "opt_out"}}
   230→    label = str(btn.get("label") or btn.get("text") or "下一步")
   231→    action = btn.get("action")
   232→    if isinstance(action, dict):
   233→        a = _normalize_action(action)
   234→        if not a.get("type"):
   235→            a["type"] = "opt_out"
   236→        return {"label": label, "action": a}
   237→    action_type = btn.get("action_type") or btn.get("type") or "opt_out"
   238→    payload = btn.get("payload") if isinstance(btn.get("payload"), dict) else {}
   239→    a = {"type": str(action_type)}
   240→    a.update(payload)
   241→    return {"label": label, "action": _normalize_action(a)}
   242→
   243→
   244→def _normalize_a2ui(obj: Dict[str, Any]) -> Optional[Dict[str, Any]]:
   245→    if not isinstance(obj, dict):
   246→        return None
   247→    meta = obj.get("meta")
   248→    if not isinstance(meta, dict):
   249→        meta = {}
   250→
   251→    comps = obj.get("ui_components")
   252→    if not isinstance(comps, list):
   253→        # tolerate alternative root key
   254→        comps = obj.get("components") if isinstance(obj.get("components"), list) else []
   255→
   256→    out_comps: List[Dict[str, Any]] = []
   257→    for c in comps:
   258→        if not isinstance(c, dict):
   259→            continue
   260→        t = str(c.get("type") or "").strip()
   261→        if not t:
   262→            continue
   263→        title = str(c.get("title") or ("教练回复" if t == "markdown_text" else "下一步" if t == "action_buttons" else "组件"))
   264→        data = c.get("data")
   265→        if data is None and "content" in c:
   266→            data = c.get("content")
   267→        if t == "action_buttons":
   268→            if data is None and "actions" in c:
   269→                data = c.get("actions")
   270→            if isinstance(data, list):
   271→                data = [_normalize_action_button(b) for b in data]
   272→            else:
   273→                data = []
   274→        if t == "markdown_text" and not isinstance(data, str):
   275→            data = "" if data is None else str(data)
   276→        out_comps.append({"type": t, "title": title, "data": data})
   277→
   278→    if not out_comps:
   279→        return None
   280→    # Ensure markdown_text is first component for renderer compatibility.
   281→    for i, c in enumerate(out_comps):
   282→        if c.get("type") == "markdown_text":
   283→            if i != 0:
   284→                out_comps.insert(0, out_comps.pop(i))
   285→            break
   286→
   287→    meta_out = dict(meta)
   288→    if not isinstance(meta_out.get("summary"), str):
   289→        meta_out["summary"] = str(meta_out.get("summary") or "")
   290→    return {"meta": meta_out, "ui_components": out_comps}
   291→
   292→
   293→_PRES_LINE_RE = re.compile(r"^\s*\d+[\.\)]\s*(.+?)\s*$")
   294→_BULLET_LINE_RE = re.compile(r"^\s*[-*]\s*(.+?)\s*$")
   295→_MD_INLINE_RE = re.compile(r"[*_`]+")
   296→
   297→# =============================================================================
   298→# Intent Detection for Yunshi
   299→# =============================================================================
   300→
   301→YUNSHI_DAILY_KEYWORDS = [
   302→    "今日运势", "今天运势", "今天的运势", "今日的运势",
   303→    "今日运气", "今天运气", "今天如何", "今天怎么样",
   304→    "今日吉凶", "今天吉凶", "运势如何", "运势怎么样",
   305→    "每日运势", "日运",
   306→]
   307→
   308→YUNSHI_ANNUAL_KEYWORDS = [
   309→    "流年", "今年运势", "年运势", "今年运气", "明年运势",
   310→    "年度运势", "今年如何", "今年怎么样", "年运",
   311→]
   312→
   313→def detect_yunshi_intent(text: str) -> Optional[str]:
   314→    """
   315→    检测用户消息是否为运势查询意图
   316→
   317→    Returns:
   318→        - "daily": 今日运势查询
   319→        - "annual": 流年运势查询
   320→        - None: 非运势查询
   321→    """
   322→    t = (text or "").strip().lower()
   323→    if not t:
   324→        return None
   325→
   326→    for kw in YUNSHI_DAILY_KEYWORDS:
   327→        if kw in t:
   328→            return "daily"
   329→
   330→    for kw in YUNSHI_ANNUAL_KEYWORDS:
   331→        if kw in t:
   332→            return "annual"
   333→
   334→    return None
   335→
   336→
   337→def handle_yunshi_intent(
   338→    user_id: int,
   339→    session_id: str,
   340→    intent: str,
   341→    user_text: str,
   342→) -> Optional[Dict[str, Any]]:
   343→    """
   344→    处理运势查询意图，返回 A2UI 格式响应
   345→
   346→    Returns:
   347→        - Dict with a2ui, suggested_tasks, backend if handled
   348→        - None if should fall through to LLM
   349→    """
   350→    try:
   351→        from datetime import date, datetime
   352→
   353→        if intent == "daily":
   354→            yunshi = yunshi_service.compute_daily_yunshi(user_id, target_date=date.today())
   355→            a2ui = build_daily_yunshi_a2ui(yunshi)
   356→
   357→            # 提取处方作为任务
   358→            suggested_tasks = []
   359→            for p in yunshi.prescriptions[:2]:
   360→                from stores import fortune_db
   361→                row = fortune_db.execute_returning_one(
   362→                    """
   363→                    INSERT INTO fortune_commitment (user_id, session_id, source, commitment_type, title, details, status, yunshi_snapshot_id)
   364→                    VALUES (%s, %s, 'yunshi', 'start_task', %s, '{}'::jsonb, 'suggested', NULL)
   365→                    RETURNING task_id
   366→                    """,
   367→                    (int(user_id), session_id, p.content[:200]),
   368→                )
   369→                if row and row.get("task_id"):
   370→                    suggested_tasks.append({"task_id": str(row["task_id"]), "title": p.content})
   371→
   372→            # 确保 action_buttons 包含任务
   373→            if a2ui.get("ui_components"):
   374→                for comp in a2ui["ui_components"]:
   375→                    if comp.get("type") == "action_buttons":
   376→                        buttons = []
   377→                        for t in suggested_tasks[:2]:
   378→                            buttons.append({
   379→                                "label": f"开始：{t['title'][:12]}...",
   380→                                "action": {"type": "start_task", "task_id": t["task_id"]},
   381→                            })
   382→                        buttons.append({"label": "查看详细运势", "action": {"type": "open_panel", "panel": "yunshi"}})
   383→                        buttons.append({"label": "先不需要", "action": {"type": "opt_out"}})
   384→                        comp["data"] = buttons
   385→                        break
   386→
   387→            md = yunshi.summary
   388→            return {
   389→                "a2ui": a2ui,
   390→                "content": md,
   391→                "suggested_tasks": suggested_tasks,
   392→                "backend": "yunshi",
   393→            }
   394→
   395→        elif intent == "annual":
   396→            target_year = datetime.now().year
   397→            yunshi = yunshi_service.compute_annual_yunshi(user_id, target_year=target_year)
   398→            a2ui = build_annual_yunshi_a2ui(yunshi)
   399→
   400→            # 流年不生成即时任务
   401→            suggested_tasks = []
   402→
   403→            md = yunshi.summary
   404→            return {
   405→                "a2ui": a2ui,
   406→                "content": md,
   407→                "suggested_tasks": suggested_tasks,
   408→                "backend": "yunshi",
   409→            }
   410→
   411→    except Exception as e:
   412→        import logging
   413→        logging.getLogger(__name__).warning(f"Yunshi intent handling failed: {e}")
   414→        return None
   415→
   416→    return None
   417→
   418→
   419→def _strip_md(text: str) -> str:
   420→    s = _MD_INLINE_RE.sub("", text or "")
   421→    s = re.sub(r"\s+", " ", s).strip()
   422→    return s
   423→
   424→
   425→def _short_label(text: str, max_len: int = 14) -> str:
   426→    s = _strip_md(text)
   427→    s = re.sub(r"^\d+[\.\)]\s*", "", s).strip()
   428→    if len(s) > max_len:
   429→        s = s[: max_len - 3].rstrip() + "..."
   430→    return s or "行动"
   431→
   432→
   433→def _extract_prescriptions(md: str) -> List[str]:
   434→    lines = [ln.strip() for ln in (md or "").splitlines() if ln.strip()]
   435→    start: Optional[int] = None
   436→    for i, ln in enumerate(lines):
   437→        if "处方" in ln and not _PRES_LINE_RE.match(ln):
   438→            start = i + 1
   439→            break
   440→    if start is None:
   441→        return []
   442→    out: List[str] = []
   443→    for ln in lines[start:]:
   444→        if ln.startswith("#"):
   445→            break
   446→        m = _PRES_LINE_RE.match(ln) or _BULLET_LINE_RE.match(ln)
   447→        if m:
   448→            item = _strip_md(m.group(1))
   449→            if item:
   450→                out.append(item)
   451→        if len(out) >= 3:
   452→            break
   453→    return out
   454→
   455→
   456→def _insert_suggested_tasks(user_id: int, session_id: str, prescriptions: List[str]) -> List[Dict[str, Any]]:
   457→    tasks: List[Dict[str, Any]] = []
   458→    for p in prescriptions[:3]:
   459→        title = _strip_md(p) or "行动"
   460→        row = fortune_db.execute_returning_one(
   461→            """
   462→            INSERT INTO fortune_commitment (user_id, session_id, source, commitment_type, title, details, status)
   463→            VALUES (%s, %s, 'chat', 'start_task', %s, '{}'::jsonb, 'suggested')
   464→            RETURNING task_id
   465→            """,
   466→            (int(user_id), session_id, title[:200]),
   467→        )
   468→        if row and row.get("task_id"):
   469→            tasks.append({"task_id": str(row["task_id"]), "title": title})
   470→    return tasks
   471→
   472→
   473→def _ensure_action_buttons(a2ui: Dict[str, Any], suggested_tasks: List[Dict[str, Any]]) -> None:
   474→    buttons = []
   475→    for t in suggested_tasks[:2]:
   476→        title = _short_label(str(t.get("title") or ""))
   477→        buttons.append({"label": f"开始：{title}", "action": {"type": "start_task", "task_id": str(t["task_id"])}})
   478→    if suggested_tasks:
   479→        buttons.append({"label": "加入今日计划", "action": {"type": "schedule_task", "task_id": str(suggested_tasks[0]["task_id"])}})
   480→    buttons.append({"label": "打开功能区", "action": {"type": "open_panel", "panel": "bento"}})
   481→    buttons.append({"label": "先不需要", "action": {"type": "opt_out"}})
   482→
   483→    comps = a2ui.get("ui_components")
   484→    if not isinstance(comps, list):
   485→        a2ui["ui_components"] = comps = []
   486→
   487→    for c in comps:
   488→        if isinstance(c, dict) and c.get("type") == "action_buttons":
   489→            c["data"] = buttons
   490→            return
   491→
   492→    comps.append({"type": "action_buttons", "title": "下一步", "data": buttons})
   493→
   494→
   495→def _assistant_markdown(a2ui: Dict[str, Any]) -> str:
   496→    comps = a2ui.get("ui_components") or []
   497→    if isinstance(comps, list) and comps:
   498→        first = comps[0] if isinstance(comps[0], dict) else None
   499→        if first and isinstance(first.get("data"), str):
   500→            return first["data"]
   501→    return json.dumps(a2ui, ensure_ascii=False)
   502→
   503→
   504→@router.post("/send")
   505→def chat_send(req: ChatSendRequest, request: Request):
   506→    """
   507→    Send a chat message and get AI response.
   508→
   509→    Supports two backends based on user preference:
   510→    - fastapi: Direct LLM call (default, current implementation)
   511→    - agent_service: Vercel AI SDK orchestration (future)
   512→
   513→    Also writes audit log to fortune_agent_run for compliance.
   514→    """
   515→    import time as time_module
   516→
   517→    auth = require_auth(request)
   518→    require_csrf(request, auth)
   519→    user_id = int(auth["user_id"])
   520→
   521→    session_id = (req.session_id or "").strip()
   522→    if session_id and not _UUID_RE.match(session_id):
   523→        return _err(400, "invalid_request", "invalid_session_id")
   524→
   525→    if not session_id:
   526→        s = chat_service.create_session(user_id, title="")
   527→        session_id = s["session_id"]
   528→    else:
   529→        if not chat_service.get_session(user_id, session_id):
   530→            return _err(404, "not_found", "session_not_found")
   531→
   532→    user_text = req.text.strip()
   533→    if not user_text:
   534→        return _err(400, "invalid_request", "empty_text")
   535→
   536→    model_name = (os.getenv("FORTUNE_AI_GLM_MODEL") or "glm-4.7").strip()
   537→
   538→    # Save user message
   539→    chat_service.append_message(session_id=session_id, user_id=user_id, role="user", content=user_text, model=model_name)
   540→    chat_service.set_title_if_empty(session_id, user_text[:24])
   541→
   542→    profile, prefs, snap = _load_user_context(user_id)
   543→    if not snap or not snap.get("facts") or not snap.get("facts_hash"):
   544→        snap = bazi_facts.ensure_snapshot_for_user(user_id)
   545→
   546→    # 使用 soul_os 获取完整上下文
   547→    full_context = soul_os.get_full_context_for_chat(user_id, user_text)
   548→
   549→    # 反依赖检查
   550→    anti_dep = full_context.get("anti_dependency", {})
   551→    if anti_dep.get("should_intervene"):
   552→        # 返回反依赖干预消息
   553→        intervention_msg = anti_dep.get("intervention_message", "")
   554→        a2ui = {
   555→            "meta": {"summary": "让我们先行动起来"},
   556→            "ui_components": [
   557→                {"type": "markdown_text", "title": "教练提醒", "data": intervention_msg},
   558→                {"type": "action_buttons", "title": "下一步", "data": [
   559→                    {"label": "好的，我去做", "action": {"type": "opt_out"}},
   560→                    {"label": "打开任务", "action": {"type": "open_panel", "panel": "tasks"}},
   561→                ]},
   562→            ],
   563→        }
   564→        chat_service.append_message(
   565→            session_id=session_id,
   566→            user_id=user_id,
   567→            role="assistant",
   568→            content=intervention_msg,
   569→            a2ui=a2ui,
   570→            model="anti_dependency",
   571→        )
   572→        return _ok({
   573→            "session_id": session_id,
   574→            "assistant_message": {"role": "assistant", "a2ui": a2ui},
   575→            "suggested_tasks": [],
   576→            "backend": "anti_dependency",
   577→            "intervention_type": anti_dep.get("intervention_type", ""),
   578→        })
   579→
   580→    # ==========================================================================
   581→    # 运势意图检测 - 优先于 LLM 调用
   582→    # ==========================================================================
   583→    yunshi_intent = detect_yunshi_intent(user_text)
   584→    if yunshi_intent:
   585→        yunshi_response = handle_yunshi_intent(user_id, session_id, yunshi_intent, user_text)
   586→        if yunshi_response:
   587→            a2ui = yunshi_response["a2ui"]
   588→            md = yunshi_response.get("content", "")
   589→            suggested_tasks = yunshi_response.get("suggested_tasks", [])
   590→
   591→            chat_service.append_message(
   592→                session_id=session_id,
   593→                user_id=user_id,
   594→                role="assistant",
   595→                content=md,
   596→                a2ui=a2ui,
   597→                model="yunshi",
   598→            )
   599→
   600→            return _ok({
   601→                "session_id": session_id,
   602→                "assistant_message": {"role": "assistant", "a2ui": a2ui},
   603→                "suggested_tasks": suggested_tasks,
   604→                "backend": "yunshi",
   605→            })
   606→
   607→    # Check chat backend preference
   608→    chat_backend = str(prefs.get("chat_backend") or default_chat_backend())
   609→    start_time = time_module.time()
   610→    persona_style = full_context.get("persona_style", "warm")
   611→    facts_obj = snap.get("facts") or {}
   612→    facts_hash = str(snap.get("facts_hash") or "")
   613→
   614→    if chat_backend == "agent_service":
   615→        # Try Agent Service backend
   616→        try:
   617→            # Build messages for agent service
   618→            hist = chat_service.get_recent_messages_for_llm(user_id, session_id, limit=10)
   619→            agent_messages = [{"role": m.get("role", "user"), "content": str(m.get("content") or "")} for m in hist]
   620→
   621→            agent_response = agent_service_client.call_agent_service_sync(
   622→                user_id=user_id,
   623→                session_id=session_id,
   624→                messages=agent_messages,
   625→                user_context={"profile": profile, "preferences": prefs},
   626→                facts=facts_obj,
   627→                persona_style=persona_style,
   628→            )
   629→
   630→            # Process response
   631→            a2ui = agent_response.a2ui or _a2ui_from_text(agent_response.text)
   632→            md = _assistant_markdown(a2ui)
   633→            prescriptions = _extract_prescriptions(md)
   634→            suggested_tasks = _insert_suggested_tasks(user_id, session_id, prescriptions)
   635→            _ensure_action_buttons(a2ui, suggested_tasks)
   636→
   637→            chat_service.append_message(
   638→                session_id=session_id,
   639→                user_id=user_id,
   640→                role="assistant",
   641→                content=md,
   642→                a2ui=a2ui,
   643→                model="agent_service",
   644→            )
   645→
   646→            # Write audit log
   647→            latency_ms = int((time_module.time() - start_time) * 1000)
   648→            _write_agent_run_log(
   649→                user_id=user_id,
   650→                session_id=session_id,
   651→                agent_name="coach",
   652→                prompt_version="agent_service",
   653→                facts_hash=facts_hash,
   654→                input_data={"user_text": user_text, "backend": "agent_service"},
   655→                output_data={"a2ui_summary": a2ui.get("meta", {}).get("summary", ""), "suggested_tasks": len(suggested_tasks)},
   656→                latency_ms=latency_ms,
   657→            )
   658→
   659→            return _ok(
   660→                {
   661→                    "session_id": session_id,
   662→                    "assistant_message": {"role": "assistant", "a2ui": a2ui},
   663→                    "suggested_tasks": suggested_tasks,
   664→                    "backend": "agent_service",
   665→                }
   666→            )
   667→
   668→        except agent_service_client.AgentServiceUnavailable:
   669→            # Fall back to FastAPI if agent service unavailable
   670→            pass
   671→        except agent_service_client.AgentServiceError as e:
   672→            # Log error and fall back
   673→            latency_ms = int((time_module.time() - start_time) * 1000)
   674→            _write_agent_run_log(
   675→                user_id=user_id,
   676→                session_id=session_id,
   677→                agent_name="coach",
   678→                prompt_version="agent_service",
   679→                facts_hash=facts_hash,
   680→                input_data={"user_text": user_text, "backend": "agent_service"},
   681→                output_data={},
   682→                latency_ms=latency_ms,
   683→                error=str(e),
   684→            )
   685→            # Fall back to FastAPI
   686→            pass
   687→
   688→    # === FastAPI Direct LLM Implementation ===
   689→    # 使用 soul_os 构建的 system prompt 和 evidence
   690→    user_context = full_context.get("user_context", {})
   691→    evidence = full_context.get("evidence", {})
   692→
   693→    user_context_json = json.dumps(user_context, ensure_ascii=False, default=str)
   694→    facts_json = json.dumps(facts_obj, ensure_ascii=False)
   695→    evidence_json = json.dumps(evidence, ensure_ascii=False)
   696→
   697→    # 使用 soul_os 生成的 system prompt
   698→    system = full_context.get("system_prompt", "")
   699→    if not system:
   700→        system = SYSTEM_PROMPT_TEMPLATE.safe_substitute(
   701→            persona_style=persona_style,
   702→            user_context_json=user_context_json,
   703→            facts_json=facts_json,
   704→            evidence_json=evidence_json,
   705→        )
   706→
   707→    # 从 evidence 中提取 kb_refs 用于审计
   708→    kb_refs = evidence.get("kb_refs", [])
   709→
   710→    # Build LLM messages from DB (last 10 turns)
   711→    hist = chat_service.get_recent_messages_for_llm(user_id, session_id, limit=10)
   712→    llm_messages = [{"role": "system", "content": system}]
   713→    for m in hist:
   714→        role = "assistant" if m.get("role") == "assistant" else "user"
   715→        llm_messages.append({"role": role, "content": str(m.get("content") or "")})
   716→
   717→    error_msg = None
   718→    try:
   719→        raw_text, raw = glm_client.call_chat_completions(messages=llm_messages)
   720→    except glm_client.GlmError as e:
   721→        error_msg = str(e)
   722→        # Write audit log for failed request
   723→        latency_ms = int((time_module.time() - start_time) * 1000)
   724→        _write_agent_run_log(
   725→            user_id=user_id,
   726→            session_id=session_id,
   727→            agent_name="coach",
   728→            prompt_version="v1.0",
   729→            facts_hash=facts_hash,
   730→            input_data={"user_text": user_text, "kb_refs": kb_refs},
   731→            output_data={},
   732→            latency_ms=latency_ms,
   733→            error=error_msg,
   734→        )
   735→        return _err(500, "glm_error", error_msg)
   736→
   737→    a2ui = _a2ui_from_text(raw_text)
   738→    md = _assistant_markdown(a2ui)
   739→    prescriptions = _extract_prescriptions(md)
   740→    suggested_tasks = _insert_suggested_tasks(user_id, session_id, prescriptions)
   741→    _ensure_action_buttons(a2ui, suggested_tasks)
   742→
   743→    usage = raw.get("usage") if isinstance(raw, dict) else {}
   744→    prompt_tokens = int(usage.get("prompt_tokens") or usage.get("input_tokens") or 0) if isinstance(usage, dict) else 0
   745→    completion_tokens = int(usage.get("completion_tokens") or usage.get("output_tokens") or 0) if isinstance(usage, dict) else 0
   746→
   747→    chat_service.append_message(
   748→        session_id=session_id,
   749→        user_id=user_id,
   750→        role="assistant",
   751→        content=md,
   752→        a2ui=a2ui,
   753→        model=str(raw.get("model") or model_name),
   754→        prompt_tokens=prompt_tokens,
   755→        completion_tokens=completion_tokens,
   756→    )
   757→
   758→    # Write audit log for successful request
   759→    latency_ms = int((time_module.time() - start_time) * 1000)
   760→    _write_agent_run_log(
   761→        user_id=user_id,
   762→        session_id=session_id,
   763→        agent_name="coach",
   764→        prompt_version="v1.0",
   765→        facts_hash=facts_hash,
   766→        input_data={"user_text": user_text, "kb_refs": kb_refs},
   767→        output_data={"a2ui_summary": a2ui.get("meta", {}).get("summary", ""), "suggested_tasks": len(suggested_tasks)},
   768→        usage={"prompt_tokens": prompt_tokens, "completion_tokens": completion_tokens, "total_tokens": prompt_tokens + completion_tokens},
   769→        latency_ms=latency_ms,
   770→    )
   771→
   772→    return _ok(
   773→        {
   774→            "session_id": session_id,
   775→            "assistant_message": {"role": "assistant", "a2ui": a2ui},
   776→            "suggested_tasks": suggested_tasks,
   777→            "backend": "fastapi",
   778→        }
   779→    )
   780→
   781→
   782→def _sse_json(obj: Any) -> str:
   783→    return f"data: {json.dumps(obj, ensure_ascii=False)}\n\n"
   784→
   785→
   786→@router.get("/stream")
   787→def chat_stream(
   788→    request: Request,
   789→    message: str = Query(..., min_length=1, description="User message"),
   790→    session_id: Optional[str] = Query(None, description="Existing session id"),
   791→):
   792→    """
   793→    Stream chat response via Server-Sent Events (SSE).
   794→
   795→    Frontend protocol:
   796→    - data: {"type":"delta","delta":"..."}  (0..n)
   797→    - data: {"type":"final","data":{...}}   (1)
   798→    - data: [DONE]
   799→    """
   800→    import time as time_module
   801→
   802→    auth = require_auth(request)
   803→    require_csrf(request, auth)
   804→    user_id = int(auth["user_id"])
   805→
   806→    sid = (session_id or "").strip()
   807→    if sid and not _UUID_RE.match(sid):
   808→        return _err(400, "invalid_request", "invalid_session_id")
   809→
   810→    if not sid:
   811→        s = chat_service.create_session(user_id, title="")
   812→        sid = s["session_id"]
   813→    else:
   814→        if not chat_service.get_session(user_id, sid):
   815→            return _err(404, "not_found", "session_not_found")
   816→
   817→    user_text = (message or "").strip()
   818→    if not user_text:
   819→        return _err(400, "invalid_request", "empty_text")
   820→
   821→    model_name = (os.getenv("FORTUNE_AI_GLM_MODEL") or "glm-4.7").strip()
   822→
   823→    # Save user message
   824→    chat_service.append_message(session_id=sid, user_id=user_id, role="user", content=user_text, model=model_name)
   825→    chat_service.set_title_if_empty(sid, user_text[:24])
   826→
   827→    profile, prefs, snap = _load_user_context(user_id)
   828→    if not snap or not snap.get("facts") or not snap.get("facts_hash"):
   829→        snap = bazi_facts.ensure_snapshot_for_user(user_id)
   830→
   831→    # Use soul_os to get context + anti-dependency check
   832→    full_context = soul_os.get_full_context_for_chat(user_id, user_text)
   833→    anti_dep = full_context.get("anti_dependency", {})
   834→
   835→    def gen():
   836→        start_time = time_module.time()
   837→        facts_obj = snap.get("facts") or {}
   838→        facts_hash = str(snap.get("facts_hash") or "")
   839→        persona_style = full_context.get("persona_style", "warm")
   840→
   841→        # Anti-dependency intervention: return immediately
   842→        if anti_dep.get("should_intervene"):
   843→            intervention_msg = str(anti_dep.get("intervention_message", "") or "")
   844→            a2ui = {
   845→                "meta": {"summary": "让我们先行动起来"},
   846→                "ui_components": [
   847→                    {"type": "markdown_text", "title": "教练提醒", "data": intervention_msg},
   848→                    {"type": "action_buttons", "title": "下一步", "data": [
   849→                        {"label": "好的，我去做", "action": {"type": "opt_out"}},
   850→                        {"label": "打开任务", "action": {"type": "open_panel", "panel": "tasks"}},
   851→                    ]},
   852→                ],
   853→            }
   854→            chat_service.append_message(
   855→                session_id=sid,
   856→                user_id=user_id,
   857→                role="assistant",
   858→                content=intervention_msg,
   859→                a2ui=a2ui,
   860→                model="anti_dependency",
   861→            )
   862→            yield _sse_json({"type": "delta", "delta": intervention_msg})
   863→            yield _sse_json(
   864→                {
   865→                    "type": "final",
   866→                    "data": {
   867→                        "session_id": sid,
   868→                        "assistant_message": {"role": "assistant", "a2ui": a2ui},
   869→                        "suggested_tasks": [],
   870→                        "backend": "anti_dependency",
   871→                        "intervention_type": str(anti_dep.get("intervention_type") or ""),
   872→                    },
   873→                }
   874→            )
   875→            yield "data: [DONE]\n\n"
   876→            return
   877→
   878→        # ==========================================================================
   879→        # 运势意图检测 - 优先于 LLM 调用
   880→        # ==========================================================================
   881→        yunshi_intent = detect_yunshi_intent(user_text)
   882→        if yunshi_intent:
   883→            yunshi_response = handle_yunshi_intent(user_id, sid, yunshi_intent, user_text)
   884→            if yunshi_response:
   885→                a2ui = yunshi_response["a2ui"]
   886→                md = yunshi_response.get("content", "")
   887→                suggested_tasks = yunshi_response.get("suggested_tasks", [])
   888→
   889→                chat_service.append_message(
   890→                    session_id=sid,
   891→                    user_id=user_id,
   892→                    role="assistant",
   893→                    content=md,
   894→                    a2ui=a2ui,
   895→                    model="yunshi",
   896→                )
   897→
   898→                yield _sse_json({"type": "delta", "delta": md})
   899→                yield _sse_json(
   900→                    {
   901→                        "type": "final",
   902→                        "data": {
   903→                            "session_id": sid,
   904→                            "assistant_message": {"role": "assistant", "a2ui": a2ui},
   905→                            "suggested_tasks": suggested_tasks,
   906→                            "backend": "yunshi",
   907→                        },
   908→                    }
   909→                )
   910→                yield "data: [DONE]\n\n"
   911→                return
   912→
   913→        # Determine backend preference
   914→        preferred_backend = str(prefs.get("chat_backend") or default_chat_backend())
   915→        yield _sse_json({"type": "meta", "session_id": sid, "backend": preferred_backend})
   916→
   917→        # Build history (last 10 turns)
   918→        hist = chat_service.get_recent_messages_for_llm(user_id, sid, limit=10)
   919→        base_messages = [{"role": m.get("role", "user"), "content": str(m.get("content") or "")} for m in hist]
   920→
   921→        # Context payload for agent service (if used)
   922→        agent_context: Dict[str, Any] = {
   923→            "user": {"profile": profile, "preferences": prefs},
   924→            "facts": facts_obj,
   925→            "persona_style": persona_style,
   926→            "output_format": "markdown",
   927→        }
   928→
   929→        full_text_parts: List[str] = []
   930→        backend_used = preferred_backend
   931→        tool_calls: Optional[List[Dict[str, Any]]] = None
   932→
   933→        # Stream from agent_service when selected and available
   934→        if preferred_backend == "agent_service":
   935→            try:
   936→                import httpx
   937→                if not agent_service_client.is_agent_service_configured():
   938→                    raise agent_service_client.AgentServiceUnavailable("Agent service not configured")
   939→
   940→                payload = {
   941→                    "user_id": user_id,
   942→                    "session_id": sid,
   943→                    "messages": [{"role": m.get("role", "user"), "content": str(m.get("content") or "")} for m in hist],
   944→                    "stream": True,
   945→                    "context": agent_context,
   946→                }
   947→
   948→                headers = {
   949→                    "Content-Type": "application/json",
   950→                    "Authorization": f"Bearer {agent_service_client.AGENT_SERVICE_TOKEN}",
   951→                }
   952→
   953→                with httpx.Client(timeout=agent_service_client.AGENT_SERVICE_TIMEOUT) as client:
   954→                    with client.stream("POST", f"{agent_service_client.AGENT_SERVICE_URL}/api/chat", json=payload, headers=headers) as resp:
   955→                        if resp.status_code == 503:
   956→                            raise agent_service_client.AgentServiceUnavailable("Agent service temporarily unavailable")
   957→                        if resp.status_code != 200:
   958→                            raise agent_service_client.AgentServiceError(f"Agent service error: {resp.status_code}")
   959→
   960→                        for raw_line in resp.iter_lines():
   961→                            line = (raw_line or "").strip()
   962→                            if not line or not line.startswith("data:"):
   963→                                continue
   964→                            data = line[5:].strip()
   965→                            if not data:
   966→                                continue
   967→                            try:
   968→                                evt = json.loads(data)
   969→                            except Exception:
   970→                                continue
   971→                            if not isinstance(evt, dict):
   972→                                continue
   973→                            if evt.get("type") == "text" and isinstance(evt.get("content"), str):
   974→                                delta = str(evt.get("content") or "")
   975→                                if delta:
   976→                                    full_text_parts.append(delta)
   977→                                    yield _sse_json({"type": "delta", "delta": delta})
   978→                            if evt.get("type") == "done":
   979→                                if isinstance(evt.get("text"), str) and evt.get("text"):
   980→                                    full_text_parts = [str(evt.get("text") or "")]
   981→                                if isinstance(evt.get("tool_calls"), list):
   982→                                    tool_calls = evt.get("tool_calls")
   983→                                break
   984→            except Exception:
   985→                backend_used = "fastapi"
   986→
   987→        # Stream from direct LLM (fastapi)
   988→        if backend_used != "agent_service":
   989→            user_context = full_context.get("user_context", {})
   990→            evidence = full_context.get("evidence", {})
   991→            kb_refs = evidence.get("kb_refs", [])
   992→
   993→            user_context_json = json.dumps(user_context, ensure_ascii=False, default=str)
   994→            facts_json = json.dumps(facts_obj, ensure_ascii=False)
   995→            evidence_json = json.dumps(evidence, ensure_ascii=False)
   996→
   997→            system_stream = SYSTEM_PROMPT_STREAM_TEMPLATE.safe_substitute(
   998→                persona_style=persona_style,
   999→                user_context_json=user_context_json,
  1000→                facts_json=facts_json,
  1001→                evidence_json=evidence_json,
  1002→            )
  1003→
  1004→            llm_messages = [{"role": "system", "content": system_stream}]
  1005→            for m in hist:
  1006→                role = "assistant" if m.get("role") == "assistant" else "user"
  1007→                llm_messages.append({"role": role, "content": str(m.get("content") or "")})
  1008→
  1009→            try:
  1010→                got_any = False
  1011→                for delta in glm_client.stream_chat_completions(messages=llm_messages):
  1012→                    if delta:
  1013→                        got_any = True
  1014→                        full_text_parts.append(delta)
  1015→                        yield _sse_json({"type": "delta", "delta": delta})
  1016→                if not got_any:
  1017→                    # Provider may not support streaming; fall back to non-streaming and simulate deltas.
  1018→                    raw_text, _raw = glm_client.call_chat_completions(messages=llm_messages)
  1019→                    raw_text = str(raw_text or "")
  1020→                    for i in range(0, len(raw_text), 48):
  1021→                        chunk = raw_text[i : i + 48]
  1022→                        if chunk:
  1023→                            full_text_parts.append(chunk)
  1024→                            yield _sse_json({"type": "delta", "delta": chunk})
  1025→            except glm_client.GlmError as e:
  1026→                # Try a non-streaming fallback before failing.
  1027→                try:
  1028→                    raw_text, _raw = glm_client.call_chat_completions(messages=llm_messages)
  1029→                    raw_text = str(raw_text or "")
  1030→                    for i in range(0, len(raw_text), 48):
  1031→                        chunk = raw_text[i : i + 48]
  1032→                        if chunk:
  1033→                            full_text_parts.append(chunk)
  1034→                            yield _sse_json({"type": "delta", "delta": chunk})
  1035→                except glm_client.GlmError:
  1036→                    err = str(e)
  1037→                    latency_ms = int((time_module.time() - start_time) * 1000)
  1038→                    _write_agent_run_log(
  1039→                        user_id=user_id,
  1040→                        session_id=sid,
  1041→                        agent_name="coach",
  1042→                        prompt_version="v1.0-stream",
  1043→                        facts_hash=facts_hash,
  1044→                        input_data={"user_text": user_text, "kb_refs": kb_refs, "backend": "fastapi"},
  1045→                        output_data={},
  1046→                        latency_ms=latency_ms,
  1047→                        error=err,
  1048→                    )
  1049→                    yield _sse_json({"type": "error", "error": err})
  1050→                    yield "data: [DONE]\n\n"
  1051→                    return
  1052→
  1053→        full_text = "".join(full_text_parts).strip()
  1054→        a2ui = _a2ui_from_text(full_text)
  1055→        md = _assistant_markdown(a2ui)
  1056→        prescriptions = _extract_prescriptions(md)
  1057→        suggested_tasks = _insert_suggested_tasks(user_id, sid, prescriptions)
  1058→        _ensure_action_buttons(a2ui, suggested_tasks)
  1059→
  1060→        chat_service.append_message(
  1061→            session_id=sid,
  1062→            user_id=user_id,
  1063→            role="assistant",
  1064→            content=md,
  1065→            a2ui=a2ui,
  1066→            model=("agent_service" if backend_used == "agent_service" else model_name),
  1067→        )
  1068→
  1069→        latency_ms = int((time_module.time() - start_time) * 1000)
  1070→        _write_agent_run_log(
  1071→            user_id=user_id,
  1072→            session_id=sid,
  1073→            agent_name="coach",
  1074→            prompt_version=("agent_service-stream" if backend_used == "agent_service" else "v1.0-stream"),
  1075→            facts_hash=facts_hash,
  1076→            input_data={"user_text": user_text, "backend": backend_used},
  1077→            output_data={"a2ui_summary": a2ui.get("meta", {}).get("summary", ""), "suggested_tasks": len(suggested_tasks)},
  1078→            tool_calls=tool_calls,
  1079→            latency_ms=latency_ms,
  1080→        )
  1081→
  1082→        yield _sse_json(
  1083→            {
  1084→                "type": "final",
  1085→                "data": {
  1086→                    "session_id": sid,
  1087→                    "assistant_message": {"role": "assistant", "a2ui": a2ui},
  1088→                    "suggested_tasks": suggested_tasks,
  1089→                    "backend": backend_used,
  1090→                },
  1091→            }
  1092→        )
  1093→        yield "data: [DONE]\n\n"
  1094→
  1095→    headers = {
  1096→        "Cache-Control": "no-cache",
  1097→        "X-Accel-Buffering": "no",
  1098→    }
  1099→    return StreamingResponse(gen(), media_type="text/event-stream", headers=headers)
  1100→
  1101→
  1102→@router.post("/ask")
  1103→def chat_ask_alias(req: ChatSendRequest, request: Request):
  1104→    # Backward-compatible alias for /api/chat/send
  1105→    return chat_send(req, request)
  1106→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
