     1→from __future__ import annotations
     2→
     3→import os
     4→import re
     5→import json
     6→import uuid
     7→import sqlite3
     8→from typing import Any, Dict, List, Optional, Tuple
     9→from datetime import datetime
    10→import subprocess
    11→from hashlib import sha256
    12→
    13→# Minimal integration layer to use cli_worker (docs at
    14→# /home/aiscend/work/cli_worker/docs/cli_worker_external_usage.md).
    15→#
    16→# Design goals:
    17→# - No access to gemini service or gemini DB.
    18→# - Submit jobs via cli_worker CLI (enqueue) and read status from cli_worker DB.
    19→# - Provide fortune_ai-compatible wrappers so upstream API无需修改。
    20→
    21→CLI_WORKER_ROOT = os.getenv("CLI_WORKER_ROOT", "/home/aiscend/work/cli_worker")
    22→DEFAULT_SQLITE_PATH = os.path.join(CLI_WORKER_ROOT, "cli_worker.db")
    23→
    24→INDEX_DIR = os.path.join("user", "cli_worker")
    25→INDEX_META = os.path.join(INDEX_DIR, "index.meta.json")
    26→INDEX_JL = os.path.join(INDEX_DIR, "index.jsonl")
    27→
    28→
    29→def _ensure_index_dirs() -> None:
    30→    os.makedirs(INDEX_DIR, exist_ok=True)
    31→    if not os.path.isfile(INDEX_META):
    32→        with open(INDEX_META, "w", encoding="utf-8") as f:
    33→            json.dump({"next_job_id": 1}, f)
    34→    if not os.path.isfile(INDEX_JL):
    35→        open(INDEX_JL, "a").close()
    36→
    37→
    38→def _load_next_job_id() -> int:
    39→    _ensure_index_dirs()
    40→    with open(INDEX_META, "r", encoding="utf-8") as f:
    41→        data = json.load(f)
    42→    return int(data.get("next_job_id", 1))
    43→
    44→
    45→def _bump_next_job_id() -> int:
    46→    nid = _load_next_job_id()
    47→    with open(INDEX_META, "w", encoding="utf-8") as f:
    48→        json.dump({"next_job_id": nid + 1}, f)
    49→    return nid + 1
    50→
    51→
    52→def _record_mapping(job_id: int, tid: str, job_name: str, payload: Dict[str, Any]) -> None:
    53→    _ensure_index_dirs()
    54→    rec = {
    55→        "job_id": job_id,
    56→        "tid": tid,
    57→        "job_name": job_name,
    58→        "payload": payload,
    59→        "created_at": datetime.utcnow().isoformat() + "Z",
    60→    }
    61→    with open(INDEX_JL, "a", encoding="utf-8") as f:
    62→        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    63→
    64→
    65→def _find_by_job_name(job_name: str) -> Optional[Dict[str, Any]]:
    66→    if not os.path.isfile(INDEX_JL):
    67→        return None
    68→    found: Optional[Dict[str, Any]] = None
    69→    with open(INDEX_JL, "r", encoding="utf-8") as f:
    70→        for line in f:
    71→            line = line.strip()
    72→            if not line:
    73→                continue
    74→            try:
    75→                rec = json.loads(line)
    76→            except Exception:
    77→                continue
    78→            if rec.get("job_name") == job_name:
    79→                found = rec
    80→    return found
    81→
    82→
    83→def _find_by_job_id(job_id: int) -> Optional[Dict[str, Any]]:
    84→    if not os.path.isfile(INDEX_JL):
    85→        return None
    86→    with open(INDEX_JL, "r", encoding="utf-8") as f:
    87→        for line in f:
    88→            line = line.strip()
    89→            if not line:
    90→                continue
    91→            try:
    92→                rec = json.loads(line)
    93→            except Exception:
    94→                continue
    95→            if int(rec.get("job_id", -1)) == int(job_id):
    96→                return rec
    97→    return None
    98→
    99→
   100→def _db_url() -> str:
   101→    # Prefer explicit CLI_WORKER_DB_URL, fallback to SQLite in cli_worker repo
   102→    url = os.getenv("CLI_WORKER_DB_URL")
   103→    if url and url.strip():
   104→        return url.strip()
   105→    return f"sqlite:///{DEFAULT_SQLITE_PATH}"
   106→
   107→
   108→def _parse_db_url(url: str) -> Tuple[str, Dict[str, str]]:
   109→    # Very small parser for sqlite/postgresql URLs
   110→    if url.startswith("sqlite:///"):
   111→        return "sqlite", {"path": url.replace("sqlite:///", "", 1)}
   112→    if url.startswith("postgresql://") or url.startswith("postgres://"):
   113→        m = re.match(r"^postgres(?:ql)?://([^:]+):([^@]+)@([^:/]+):(\d+)/(\S+)$", url)
   114→        if not m:
   115→            raise ValueError(f"Unsupported DB_URL: {url}")
   116→        user, pwd, host, port, db = m.groups()
   117→        return "postgres", {"user": user, "password": pwd, "host": host, "port": port, "database": db}
   118→    raise ValueError(f"Unsupported DB_URL: {url}")
   119→
   120→
   121→def _pg_connect(params: Dict[str, str]):
   122→    import psycopg2
   123→    from psycopg2.extras import RealDictCursor
   124→    conn = psycopg2.connect(
   125→        host=params["host"],
   126→        port=int(params["port"]),
   127→        user=params["user"],
   128→        password=params["password"],
   129→        dbname=params["database"],
   130→        cursor_factory=RealDictCursor,
   131→    )
   132→    return conn
   133→
   134→
   135→def _fetch_task_record(tid: str) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
   136→    url = _db_url()
   137→    kind, params = _parse_db_url(url)
   138→    task = last_run = first_file = None
   139→    if kind == "sqlite":
   140→        conn = sqlite3.connect(params["path"])  # type: ignore[index]
   141→        conn.row_factory = sqlite3.Row
   142→        try:
   143→            task = conn.execute("SELECT * FROM tasks WHERE id=?", (tid,)).fetchone()
   144→            if task:
   145→                task = dict(task)
   146→                last_run = conn.execute(
   147→                    "SELECT * FROM task_runs WHERE task_id=? ORDER BY started_at DESC LIMIT 1",
   148→                    (tid,),
   149→                ).fetchone()
   150→                if last_run:
   151→                    last_run = dict(last_run)
   152→                # created_at is not guaranteed in SQLite schema; fetch first output file
   153→                first_file = conn.execute(
   154→                    "SELECT * FROM task_files WHERE task_id=? AND is_input=0 LIMIT 1",
   155→                    (tid,),
   156→                ).fetchone()
   157→                if first_file:
   158→                    first_file = dict(first_file)
   159→        finally:
   160→            conn.close()
   161→    else:
   162→        conn = _pg_connect(params)
   163→        try:
   164→            with conn.cursor() as cur:
   165→                cur.execute("SELECT * FROM tasks WHERE id=%s", (tid,))
   166→                row = cur.fetchone()
   167→                if row:
   168→                    task = dict(row)
   169→                cur.execute(
   170→                    "SELECT * FROM task_runs WHERE task_id=%s ORDER BY started_at DESC LIMIT 1",
   171→                    (tid,),
   172→                )
   173→                row = cur.fetchone()
   174→                if row:
   175→                    last_run = dict(row)
   176→                cur.execute(
   177→                    "SELECT * FROM task_files WHERE task_id=%s AND is_input=false LIMIT 1",
   178→                    (tid,),
   179→                )
   180→                row = cur.fetchone()
   181→                if row:
   182→                    first_file = dict(row)
   183→        finally:
   184→            conn.close()
   185→    return task, last_run, first_file
   186→
   187→
   188→def _status_to_progress(status: str) -> int:
   189→    s = (status or "").lower()
   190→    if s in ("queued", "leased"):
   191→        return 1
   192→    if s in ("running",):
   193→        return 10
   194→    if s in ("succeeded",):
   195→        return 99
   196→    if s in ("failed", "canceled", "deadletter"):
   197→        return -1
   198→    return 0
   199→
   200→
   201→def _run_enqueue(prompt: str, key: Optional[str] = None) -> str:
   202→    # Execute cli_worker enqueue and capture returned task UUID from stdout
   203→    env = os.environ.copy()
   204→    # Force CLI import path to repo src
   205→    env["PYTHONPATH"] = "src"
   206→    env.setdefault("DB_URL", _db_url())
   207→    cmd = [
   208→        "python3",
   209→        "-m",
   210→        "cli_worker.cli",
   211→        "enqueue",
   212→        "--tool",
   213→        "gemini",
   214→        "--prompt",
   215→        prompt,
   216→        "--output-spec",
   217→        "out.txt",
   218→    ]
   219→    if key:
   220→        cmd += ["--key", key]
   221→    # Allow model mapping later if needed
   222→    try:
   223→        proc = subprocess.run(
   224→            cmd,
   225→            cwd=CLI_WORKER_ROOT,
   226→            env=env,
   227→            stdout=subprocess.PIPE,
   228→            stderr=subprocess.PIPE,
   229→            text=True,
   230→            check=True,
   231→        )
   232→    except subprocess.CalledProcessError as e:
   233→        raise RuntimeError(f"enqueue failed: {e.stderr.strip() or e.stdout.strip()}")
   234→    out = (proc.stdout or "").strip()
   235→    # stdout is expected to be the task UUID; accept the last UUID-like token
   236→    m = re.findall(r"[0-9a-fA-F-]{32,36}", out)
   237→    if not m:
   238→        # fallback a whole line if it's UUID only
   239→        if re.fullmatch(r"[0-9a-fA-F-]{32,36}", out):
   240→            tid = out
   241→        else:
   242→            raise RuntimeError(f"enqueue returned unexpected output: {out}")
   243→    else:
   244→        tid = m[-1]
   245→    # normalize UUID
   246→    try:
   247→        tid = str(uuid.UUID(tid))
   248→    except Exception:
   249→        pass
   250→    return tid
   251→
   252→
   253→# === Fortune-compatible API wrappers ===
   254→
   255→def create_gemini_job(job_name: str, tasks: List[Dict], user_email: Optional[str] = None) -> Tuple[Optional[str], Optional[int], List[int]]:
   256→    # We only support single-task submission for now; pick the first
   257→    if not tasks:
   258→        return "no_tasks", None, []
   259→    t0 = tasks[0] or {}
   260→    prompt = str(t0.get("prompt") or "").strip()
   261→    if not prompt:
   262→        # 明确拒绝空提示，避免返回占位文本影响体验
   263→        return "empty_prompt", None, []
   264→    # Compute idempotency key from job_name + prompt for de-dup
   265→    key = sha256((job_name + "\n" + prompt).encode("utf-8")).hexdigest()
   266→    try:
   267→        tid = _run_enqueue(prompt, key=key)
   268→    except Exception as e:
   269→        return str(e), None, []
   270→
   271→    # Allocate local integer job_id and index mapping
   272→    jid = _load_next_job_id()
   273→    _record_mapping(jid, tid, job_name, {"task": t0})
   274→    _bump_next_job_id()
   275→    return None, jid, [1]
   276→
   277→
   278→def get_tasks_by_job_id(job_id: int) -> List[Dict]:
   279→    rec = _find_by_job_id(int(job_id))
   280→    if not rec:
   281→        return []
   282→    tid = rec.get("tid")
   283→    if not tid:
   284→        return []
   285→    task, last_run, first_file = _fetch_task_record(str(tid))
   286→    if not task:
   287→        # Not found yet; treat as queued
   288→        return [{"id": 1, "status": 0}]
   289→    status = _status_to_progress(task.get("status") or "")
   290→    output_text = None
   291→    output_url = None
   292→    output_code = None
   293→    output_file_path = None
   294→    # Prefer file content (if any) over stdout; many executors write main output to file
   295→    if first_file:
   296→        c = first_file.get("content")
   297→        if c is not None:
   298→            try:
   299→                if isinstance(c, (bytes, bytearray)):
   300→                    output_text = c.decode("utf-8", errors="ignore")[:200000]
   301→                elif isinstance(c, memoryview):  # type: ignore[name-defined]
   302→                    output_text = bytes(c).decode("utf-8", errors="ignore")[:200000]
   303→                else:
   304→                    output_text = str(c)[:200000]
   305→            except Exception:
   306→                output_text = None
   307→        output_file_path = first_file.get("path")
   308→    if not output_text and last_run:
   309→        # Fallback to stdout when file content is absent
   310→        out = (last_run.get("stdout") or "")
   311→        if isinstance(out, bytes):
   312→            try:
   313→                out = out.decode("utf-8", errors="ignore")
   314→            except Exception:
   315→                out = ""
   316→        output_text = str(out)[:200000] if out else None
   317→    return [
   318→        {
   319→            "id": 1,
   320→            "status": status,
   321→            "output_text": output_text,
   322→            "output_url": output_url,
   323→            "output_code": output_code,
   324→            "output_file_path": output_file_path,
   325→            "gemini_url": None,
   326→        }
   327→    ]
   328→
   329→
   330→def _fetch_task_files(tid: str) -> list[dict]:
   331→    """Return non-input files for a task id from cli_worker DB."""
   332→    url = _db_url()
   333→    kind, params = _parse_db_url(url)
   334→    rows: list[dict] = []
   335→    if kind == "sqlite":
   336→        conn = sqlite3.connect(params["path"])  # type: ignore[index]
   337→        conn.row_factory = sqlite3.Row
   338→        try:
   339→            for r in conn.execute(
   340→                "SELECT path, content FROM task_files WHERE task_id=? AND is_input=0",
   341→                (tid,),
   342→            ).fetchall():
   343→                d = dict(r)
   344→                p = d.get("path")
   345→                fname = os.path.basename(p) if isinstance(p, str) else None
   346→                rows.append({
   347→                    "filename": fname,
   348→                    "path": p,
   349→                    "has_content": d.get("content") is not None,
   350→                    "content": d.get("content"),
   351→                })
   352→        finally:
   353→            conn.close()
   354→    else:
   355→        conn = _pg_connect(params)
   356→        try:
   357→            with conn.cursor() as cur:
   358→                cur.execute(
   359→                    "SELECT path, content FROM task_files WHERE task_id=%s AND is_input=false",
   360→                    (tid,),
   361→                )
   362→                for r in cur.fetchall():
   363→                    d = dict(r)
   364→                    p = d.get("path")
   365→                    fname = os.path.basename(p) if isinstance(p, str) else None
   366→                    rows.append({
   367→                        "filename": fname,
   368→                        "path": p,
   369→                        "has_content": d.get("content") is not None,
   370→                        "content": d.get("content"),
   371→                    })
   372→        finally:
   373→            conn.close()
   374→    return rows
   375→
   376→
   377→def list_output_files_by_job_id(job_id: int) -> list[dict]:
   378→    """List output files metadata for a mapped job id.
   379→
   380→    Returns list of {filename, path, has_content}.
   381→    """
   382→    rec = _find_by_job_id(int(job_id))
   383→    if not rec:
   384→        return []
   385→    tid = rec.get("tid")
   386→    if not tid:
   387→        return []
   388→    files = _fetch_task_files(str(tid))
   389→    return [{"filename": f.get("filename"), "path": f.get("path"), "has_content": bool(f.get("has_content"))} for f in files]
   390→
   391→
   392→def fetch_output_file_content(job_id: int, filename: Optional[str] = None) -> Tuple[Optional[str], Optional[bytes]]:
   393→    """Fetch inline content bytes for the specified output file.
   394→
   395→    If filename is None, choose the first available file with inline content.
   396→    Returns (filename, content_bytes) or (None, None) when not available.
   397→    """
   398→    rec = _find_by_job_id(int(job_id))
   399→    if not rec:
   400→        return None, None
   401→    tid = rec.get("tid")
   402→    if not tid:
   403→        return None, None
   404→    files = _fetch_task_files(str(tid))
   405→    def _to_bytes(c):
   406→        if c is None:
   407→            return None
   408→        if isinstance(c, (bytes, bytearray)):
   409→            return bytes(c)
   410→        # psycopg2 may return memoryview for BYTEA
   411→        try:
   412→            import builtins
   413→            if isinstance(c, memoryview):  # type: ignore[name-defined]
   414→                return c.tobytes()
   415→        except Exception:
   416→            pass
   417→        return str(c).encode("utf-8")
   418→
   419→    if filename:
   420→        for f in files:
   421→            if str(f.get("filename")) == filename:
   422→                c = f.get("content")
   423→                return str(f.get("filename")), _to_bytes(c)
   424→        return None, None
   425→    # pick the first with content
   426→    for f in files:
   427→        c = f.get("content")
   428→        if c is not None:
   429→            return str(f.get("filename")), _to_bytes(c)
   430→    return None, None
   431→
   432→
   433→def get_gemini_job_by_job_name(job_name: str) -> Optional[Dict]:
   434→    rec = _find_by_job_name(job_name)
   435→    if not rec:
   436→        return None
   437→    return {"id": int(rec.get("job_id"))}
   438→
   439→
   440→def get_pending_gemini_jobs() -> List[Dict]:
   441→    # Optional: enumerate queued/running tasks for observability
   442→    url = _db_url()
   443→    kind, params = _parse_db_url(url)
   444→    tids: List[Dict[str, Any]] = []
   445→    if kind == "sqlite":
   446→        conn = sqlite3.connect(params["path"])  # type: ignore[index]
   447→        conn.row_factory = sqlite3.Row
   448→        try:
   449→            rows = conn.execute(
   450→                "SELECT id,status FROM tasks WHERE status IN ('queued','leased','running') ORDER BY created_at ASC LIMIT 50"
   451→            ).fetchall()
   452→            tids = [dict(r) for r in rows]
   453→        finally:
   454→            conn.close()
   455→    else:
   456→        conn = _pg_connect(params)
   457→        try:
   458→            with conn.cursor() as cur:
   459→                cur.execute(
   460→                    "SELECT id,status FROM tasks WHERE status IN ('queued','leased','running') ORDER BY created_at ASC LIMIT 50"
   461→                )
   462→                tids = [dict(row) for row in cur.fetchall()]
   463→        finally:
   464→            conn.close()
   465→    # Map to fortune-like job list by consulting local index
   466→    results: List[Dict[str, Any]] = []
   467→    # Build tid->job_id map from index quickly
   468→    tid2jid: Dict[str, int] = {}
   469→    if os.path.isfile(INDEX_JL):
   470→        with open(INDEX_JL, "r", encoding="utf-8") as f:
   471→            for line in f:
   472→                try:
   473→                    rec = json.loads(line)
   474→                except Exception:
   475→                    continue
   476→                tid2jid[str(rec.get("tid"))] = int(rec.get("job_id"))
   477→    for r in tids:
   478→        jid = tid2jid.get(str(r.get("id")))
   479→        if not jid:
   480→            continue
   481→        results.append({"id": jid, "status": r.get("status")})
   482→    return results
   483→
   484→
   485→def update_gemini_task_status(*args, **kwargs):  # no-op in CLI backend
   486→    raise PermissionError("CLI backend: direct task updates are not supported")
   487→
   488→
   489→def update_gemini_job_status(*args, **kwargs):  # no-op in CLI backend
   490→    raise PermissionError("CLI backend: direct job updates are not supported")
   491→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
