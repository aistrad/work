     1→"""
     2→LLM Service - Multi-model LLM support with Zhipu GLM-4 as primary
     3→Based on: vibelife spec v3.0
     4→
     5→Supported providers:
     6→- Zhipu GLM-4-Plus (primary) - Chinese optimized
     7→- Claude (backup) - Complex reasoning
     8→- Gemini (backup) - Multimodal
     9→"""
    10→import os
    11→import json
    12→from typing import Optional, List, Dict, Any, AsyncGenerator, Union
    13→from dataclasses import dataclass, field
    14→from enum import Enum
    15→import httpx
    16→import logging
    17→
    18→logger = logging.getLogger(__name__)
    19→
    20→
    21→class LLMProvider(str, Enum):
    22→    """Supported LLM providers"""
    23→    ZHIPU = "zhipu"
    24→    CLAUDE = "claude"
    25→    GEMINI = "gemini"
    26→
    27→
    28→@dataclass
    29→class LLMMessage:
    30→    """Standard message format for all LLM providers"""
    31→    role: str  # 'system', 'user', 'assistant'
    32→    content: Union[str, List[Dict[str, Any]]]  # str or multimodal content
    33→
    34→
    35→@dataclass
    36→class LLMResponse:
    37→    """Standard response format"""
    38→    content: str
    39→    model: str
    40→    provider: LLMProvider
    41→    usage: Optional[Dict[str, int]] = None
    42→    finish_reason: Optional[str] = None
    43→    raw_response: Optional[Dict] = None
    44→
    45→
    46→@dataclass
    47→class LLMConfig:
    48→    """LLM configuration"""
    49→    # Zhipu (Primary)
    50→    zhipu_api_key: str = field(default_factory=lambda: os.getenv("ZHIPU_API_KEY", ""))
    51→    zhipu_base_url: str = "https://open.bigmodel.cn/api/paas/v4"
    52→    zhipu_chat_model: str = field(default_factory=lambda: os.getenv("ZHIPU_CHAT_MODEL", "glm-4-plus"))
    53→    zhipu_vision_model: str = "glm-4v-plus"
    54→
    55→    # Claude (Backup)
    56→    claude_api_key: str = field(default_factory=lambda: os.getenv("CLAUDE_API_KEY", ""))
    57→    claude_model: str = "claude-3-5-sonnet-20241022"
    58→
    59→    # Gemini (Backup)
    60→    gemini_api_key: str = field(default_factory=lambda: os.getenv("GEMINI_API_KEY", ""))
    61→    gemini_model: str = "gemini-1.5-pro"
    62→
    63→    # Default settings
    64→    default_provider: LLMProvider = field(default_factory=lambda: LLMProvider(
    65→        os.getenv("DEFAULT_LLM_PROVIDER", "zhipu")
    66→    ))
    67→    default_temperature: float = 0.7
    68→    default_max_tokens: int = 4096
    69→    timeout: float = 120.0
    70→
    71→
    72→class LLMService:
    73→    """
    74→    Unified LLM service with multi-provider support.
    75→
    76→    Features:
    77→    - Zhipu GLM-4-Plus as primary (Chinese optimized)
    78→    - Automatic fallback on errors
    79→    - Streaming support
    80→    - Vision/multimodal support
    81→    """
    82→
    83→    def __init__(self, config: Optional[LLMConfig] = None):
    84→        self.config = config or LLMConfig()
    85→
    86→    # ═══════════════════════════════════════════════════════════════════
    87→    # Zhipu GLM-4 Methods
    88→    # ═══════════════════════════════════════════════════════════════════
    89→
    90→    async def chat_zhipu(
    91→        self,
    92→        messages: List[LLMMessage],
    93→        model: Optional[str] = None,
    94→        temperature: float = 0.7,
    95→        max_tokens: int = 4096,
    96→        **kwargs
    97→    ) -> LLMResponse:
    98→        """Chat with Zhipu GLM-4 (non-streaming)"""
    99→        model = model or self.config.zhipu_chat_model
   100→
   101→        formatted_messages = self._format_messages_for_zhipu(messages)
   102→
   103→        payload = {
   104→            "model": model,
   105→            "messages": formatted_messages,
   106→            "temperature": temperature,
   107→            "max_tokens": max_tokens,
   108→            **kwargs
   109→        }
   110→
   111→        headers = {
   112→            "Authorization": f"Bearer {self.config.zhipu_api_key}",
   113→            "Content-Type": "application/json"
   114→        }
   115→
   116→        async with httpx.AsyncClient(timeout=self.config.timeout) as client:
   117→            response = await client.post(
   118→                f"{self.config.zhipu_base_url}/chat/completions",
   119→                json=payload,
   120→                headers=headers
   121→            )
   122→            response.raise_for_status()
   123→            data = response.json()
   124→
   125→        return LLMResponse(
   126→            content=data["choices"][0]["message"]["content"],
   127→            model=model,
   128→            provider=LLMProvider.ZHIPU,
   129→            usage=data.get("usage"),
   130→            finish_reason=data["choices"][0].get("finish_reason"),
   131→            raw_response=data
   132→        )
   133→
   134→    async def stream_zhipu(
   135→        self,
   136→        messages: List[LLMMessage],
   137→        model: Optional[str] = None,
   138→        temperature: float = 0.7,
   139→        max_tokens: int = 4096,
   140→        **kwargs
   141→    ) -> AsyncGenerator[str, None]:
   142→        """Stream chat with Zhipu GLM-4"""
   143→        model = model or self.config.zhipu_chat_model
   144→
   145→        formatted_messages = self._format_messages_for_zhipu(messages)
   146→
   147→        payload = {
   148→            "model": model,
   149→            "messages": formatted_messages,
   150→            "temperature": temperature,
   151→            "max_tokens": max_tokens,
   152→            "stream": True,
   153→            **kwargs
   154→        }
   155→
   156→        headers = {
   157→            "Authorization": f"Bearer {self.config.zhipu_api_key}",
   158→            "Content-Type": "application/json"
   159→        }
   160→
   161→        async with httpx.AsyncClient(timeout=self.config.timeout) as client:
   162→            async with client.stream(
   163→                "POST",
   164→                f"{self.config.zhipu_base_url}/chat/completions",
   165→                json=payload,
   166→                headers=headers
   167→            ) as response:
   168→                response.raise_for_status()
   169→                async for line in response.aiter_lines():
   170→                    if line.startswith("data: "):
   171→                        data_str = line[6:]
   172→                        if data_str == "[DONE]":
   173→                            break
   174→                        try:
   175→                            data = json.loads(data_str)
   176→                            delta = data["choices"][0].get("delta", {})
   177→                            content = delta.get("content", "")
   178→                            if content:
   179→                                yield content
   180→                        except json.JSONDecodeError:
   181→                            continue
   182→
   183→    async def vision_zhipu(
   184→        self,
   185→        image_base64: str,
   186→        prompt: str,
   187→        model: Optional[str] = None
   188→    ) -> LLMResponse:
   189→        """Analyze image with Zhipu vision model"""
   190→        model = model or self.config.zhipu_vision_model
   191→
   192→        messages = [{
   193→            "role": "user",
   194→            "content": [
   195→                {"type": "text", "text": prompt},
   196→                {
   197→                    "type": "image_url",
   198→                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
   199→                }
   200→            ]
   201→        }]
   202→
   203→        payload = {"model": model, "messages": messages}
   204→
   205→        headers = {
   206→            "Authorization": f"Bearer {self.config.zhipu_api_key}",
   207→            "Content-Type": "application/json"
   208→        }
   209→
   210→        async with httpx.AsyncClient(timeout=self.config.timeout) as client:
   211→            response = await client.post(
   212→                f"{self.config.zhipu_base_url}/chat/completions",
   213→                json=payload,
   214→                headers=headers
   215→            )
   216→            response.raise_for_status()
   217→            data = response.json()
   218→
   219→        return LLMResponse(
   220→            content=data["choices"][0]["message"]["content"],
   221→            model=model,
   222→            provider=LLMProvider.ZHIPU,
   223→            usage=data.get("usage"),
   224→            raw_response=data
   225→        )
   226→
   227→    def _format_messages_for_zhipu(self, messages: List[LLMMessage]) -> List[Dict]:
   228→        """Format messages for Zhipu API"""
   229→        return [
   230→            {"role": m.role, "content": m.content}
   231→            for m in messages
   232→        ]
   233→
   234→    # ═══════════════════════════════════════════════════════════════════
   235→    # Claude Methods (Backup)
   236→    # ═══════════════════════════════════════════════════════════════════
   237→
   238→    async def chat_claude(
   239→        self,
   240→        messages: List[LLMMessage],
   241→        model: Optional[str] = None,
   242→        temperature: float = 0.7,
   243→        max_tokens: int = 4096,
   244→        **kwargs
   245→    ) -> LLMResponse:
   246→        """Chat with Claude (non-streaming)"""
   247→        if not self.config.claude_api_key:
   248→            raise ValueError("CLAUDE_API_KEY not configured")
   249→
   250→        model = model or self.config.claude_model
   251→
   252→        # Extract system message
   253→        system_content = None
   254→        formatted_messages = []
   255→        for m in messages:
   256→            if m.role == "system":
   257→                system_content = m.content if isinstance(m.content, str) else str(m.content)
   258→            else:
   259→                formatted_messages.append({
   260→                    "role": m.role,
   261→                    "content": m.content
   262→                })
   263→
   264→        payload = {
   265→            "model": model,
   266→            "messages": formatted_messages,
   267→            "max_tokens": max_tokens,
   268→            **kwargs
   269→        }
   270→        if system_content:
   271→            payload["system"] = system_content
   272→
   273→        headers = {
   274→            "x-api-key": self.config.claude_api_key,
   275→            "anthropic-version": "2023-06-01",
   276→            "Content-Type": "application/json"
   277→        }
   278→
   279→        async with httpx.AsyncClient(timeout=self.config.timeout) as client:
   280→            response = await client.post(
   281→                "https://api.anthropic.com/v1/messages",
   282→                json=payload,
   283→                headers=headers
   284→            )
   285→            response.raise_for_status()
   286→            data = response.json()
   287→
   288→        return LLMResponse(
   289→            content=data["content"][0]["text"],
   290→            model=model,
   291→            provider=LLMProvider.CLAUDE,
   292→            usage={
   293→                "input_tokens": data["usage"]["input_tokens"],
   294→                "output_tokens": data["usage"]["output_tokens"]
   295→            },
   296→            finish_reason=data.get("stop_reason"),
   297→            raw_response=data
   298→        )
   299→
   300→    # ═══════════════════════════════════════════════════════════════════
   301→    # Unified Interface
   302→    # ═══════════════════════════════════════════════════════════════════
   303→
   304→    async def chat(
   305→        self,
   306→        messages: List[LLMMessage],
   307→        provider: Optional[LLMProvider] = None,
   308→        model: Optional[str] = None,
   309→        temperature: Optional[float] = None,
   310→        max_tokens: Optional[int] = None,
   311→        fallback: bool = True,
   312→        **kwargs
   313→    ) -> LLMResponse:
   314→        """
   315→        Unified chat interface with automatic fallback.
   316→
   317→        Args:
   318→            messages: List of messages
   319→            provider: LLM provider (default: config.default_provider)
   320→            model: Model name (default: provider's default)
   321→            temperature: Sampling temperature
   322→            max_tokens: Max tokens to generate
   323→            fallback: Whether to fallback on errors
   324→            **kwargs: Additional provider-specific args
   325→
   326→        Returns:
   327→            LLMResponse with generated content
   328→        """
   329→        provider = provider or self.config.default_provider
   330→        temperature = temperature or self.config.default_temperature
   331→        max_tokens = max_tokens or self.config.default_max_tokens
   332→
   333→        try:
   334→            if provider == LLMProvider.ZHIPU:
   335→                return await self.chat_zhipu(
   336→                    messages, model, temperature, max_tokens, **kwargs
   337→                )
   338→            elif provider == LLMProvider.CLAUDE:
   339→                return await self.chat_claude(
   340→                    messages, model, temperature, max_tokens, **kwargs
   341→                )
   342→            else:
   343→                raise ValueError(f"Unsupported provider: {provider}")
   344→
   345→        except Exception as e:
   346→            logger.warning(f"{provider} failed: {e}")
   347→
   348→            if fallback:
   349→                # Try fallback providers
   350→                fallback_order = [
   351→                    p for p in [LLMProvider.ZHIPU, LLMProvider.CLAUDE]
   352→                    if p != provider
   353→                ]
   354→
   355→                for fallback_provider in fallback_order:
   356→                    try:
   357→                        logger.info(f"Falling back to {fallback_provider}")
   358→                        if fallback_provider == LLMProvider.ZHIPU:
   359→                            return await self.chat_zhipu(
   360→                                messages, None, temperature, max_tokens, **kwargs
   361→                            )
   362→                        elif fallback_provider == LLMProvider.CLAUDE:
   363→                            return await self.chat_claude(
   364→                                messages, None, temperature, max_tokens, **kwargs
   365→                            )
   366→                    except Exception as fallback_error:
   367→                        logger.warning(f"Fallback to {fallback_provider} failed: {fallback_error}")
   368→                        continue
   369→
   370→            raise
   371→
   372→    async def stream(
   373→        self,
   374→        messages: List[LLMMessage],
   375→        provider: Optional[LLMProvider] = None,
   376→        model: Optional[str] = None,
   377→        temperature: Optional[float] = None,
   378→        max_tokens: Optional[int] = None,
   379→        **kwargs
   380→    ) -> AsyncGenerator[str, None]:
   381→        """
   382→        Unified streaming interface.
   383→
   384→        Currently only Zhipu supports streaming.
   385→        For other providers, yields entire response at once.
   386→        """
   387→        provider = provider or self.config.default_provider
   388→        temperature = temperature or self.config.default_temperature
   389→        max_tokens = max_tokens or self.config.default_max_tokens
   390→
   391→        if provider == LLMProvider.ZHIPU:
   392→            async for chunk in self.stream_zhipu(
   393→                messages, model, temperature, max_tokens, **kwargs
   394→            ):
   395→                yield chunk
   396→        else:
   397→            # Fallback to non-streaming
   398→            response = await self.chat(
   399→                messages, provider, model, temperature, max_tokens, **kwargs
   400→            )
   401→            yield response.content
   402→
   403→    async def vision(
   404→        self,
   405→        image_base64: str,
   406→        prompt: str,
   407→        provider: Optional[LLMProvider] = None,
   408→        model: Optional[str] = None
   409→    ) -> LLMResponse:
   410→        """
   411→        Unified vision/multimodal interface.
   412→        """
   413→        provider = provider or LLMProvider.ZHIPU
   414→
   415→        if provider == LLMProvider.ZHIPU:
   416→            return await self.vision_zhipu(image_base64, prompt, model)
   417→        else:
   418→            raise ValueError(f"Vision not supported for provider: {provider}")
   419→
   420→
   421→# ═══════════════════════════════════════════════════════════════════════════
   422→# Global Instance
   423→# ═══════════════════════════════════════════════════════════════════════════
   424→
   425→_llm_service: Optional[LLMService] = None
   426→
   427→
   428→def get_llm_service() -> LLMService:
   429→    """Get or create global LLM service instance"""
   430→    global _llm_service
   431→    if _llm_service is None:
   432→        _llm_service = LLMService()
   433→    return _llm_service
   434→
   435→
   436→# ═══════════════════════════════════════════════════════════════════════════
   437→# Helper Functions
   438→# ═══════════════════════════════════════════════════════════════════════════
   439→
   440→def create_message(role: str, content: str) -> LLMMessage:
   441→    """Create a simple text message"""
   442→    return LLMMessage(role=role, content=content)
   443→
   444→
   445→def create_system_message(content: str) -> LLMMessage:
   446→    """Create a system message"""
   447→    return LLMMessage(role="system", content=content)
   448→
   449→
   450→def create_user_message(content: str) -> LLMMessage:
   451→    """Create a user message"""
   452→    return LLMMessage(role="user", content=content)
   453→
   454→
   455→def create_assistant_message(content: str) -> LLMMessage:
   456→    """Create an assistant message"""
   457→    return LLMMessage(role="assistant", content=content)
   458→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
