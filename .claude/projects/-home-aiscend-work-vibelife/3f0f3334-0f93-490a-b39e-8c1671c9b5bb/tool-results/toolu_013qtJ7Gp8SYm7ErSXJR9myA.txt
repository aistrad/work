     1→"""
     2→LLM Client - Unified LLM interface with simplified routing
     3→V5 Architecture: Single entry point for all LLM calls
     4→"""
     5→import os
     6→import json
     7→import logging
     8→from typing import Optional, List, Dict, Any, AsyncGenerator
     9→from dataclasses import dataclass
    10→import httpx
    11→
    12→from .config import LLMConfig, ModelConfig, ModelSelection
    13→
    14→logger = logging.getLogger(__name__)
    15→
    16→
    17→@dataclass
    18→class LLMMessage:
    19→    """Standard message format"""
    20→    role: str
    21→    content: str
    22→    tool_call_id: Optional[str] = None  # For tool result messages
    23→    tool_calls: Optional[List[Dict[str, Any]]] = None  # For assistant messages with tool calls
    24→
    25→
    26→@dataclass
    27→class ToolCall:
    28→    """Tool call from LLM response"""
    29→    id: str
    30→    name: str
    31→    arguments: str  # JSON string
    32→
    33→
    34→@dataclass
    35→class LLMResponse:
    36→    """Standard LLM response"""
    37→    content: str
    38→    model: str
    39→    provider: str
    40→    tool_calls: Optional[List[ToolCall]] = None
    41→    usage: Optional[Dict[str, int]] = None
    42→
    43→
    44→class LLMClient:
    45→    """
    46→    Unified LLM client with automatic routing and fallback.
    47→
    48→    Features:
    49→    - Automatic model selection based on tier
    50→    - Single fallback chain
    51→    - Streaming support with tool calling
    52→    """
    53→
    54→    def __init__(self, timeout: float = 120.0):
    55→        self.timeout = timeout
    56→        self._usage = {"calls": 0, "tokens": 0}
    57→
    58→    @property
    59→    def usage(self) -> Dict[str, int]:
    60→        """Get accumulated usage for this session"""
    61→        return self._usage
    62→
    63→    async def chat(
    64→        self,
    65→        messages: List[LLMMessage],
    66→        tools: Optional[List[Dict[str, Any]]] = None,
    67→        user_tier: str = "free",
    68→        task: str = "chat",
    69→        model: Optional[str] = None,
    70→        temperature: float = 0.7,
    71→        max_tokens: int = 4096,
    72→    ) -> LLMResponse:
    73→        """
    74→        Chat with LLM (non-streaming).
    75→
    76→        Args:
    77→            messages: Conversation messages
    78→            tools: Tool definitions (OpenAI format)
    79→            user_tier: User tier for model selection
    80→            task: Task type for model selection
    81→            model: Override model (optional)
    82→            temperature: Sampling temperature
    83→            max_tokens: Max tokens to generate
    84→        """
    85→        selection = ModelConfig.resolve(user_tier, task)
    86→        model = model or selection.model
    87→        provider = selection.provider
    88→
    89→        # Try primary, then fallback chain
    90→        providers_to_try = [provider] + [
    91→            ModelConfig.load().get("models", {}).get(m, {}).get("provider", "glm")
    92→            for m in selection.fallback_chain
    93→        ]
    94→        models_to_try = [model] + [
    95→            ModelConfig.load().get("models", {}).get(m, {}).get("model_name", m)
    96→            for m in selection.fallback_chain
    97→        ]
    98→
    99→        last_error = None
   100→        for p, m in zip(providers_to_try, models_to_try):
   101→            try:
   102→                response = await self._call_provider(
   103→                    provider=p,
   104→                    model=m,
   105→                    messages=messages,
   106→                    tools=tools,
   107→                    temperature=temperature,
   108→                    max_tokens=max_tokens,
   109→                    stream=False
   110→                )
   111→                self._record_usage(response)
   112→                return response
   113→            except Exception as e:
   114→                logger.warning(f"Provider {p} failed: {e}")
   115→                last_error = e
   116→                continue
   117→
   118→        raise last_error or Exception("All providers failed")
   119→
   120→    async def stream(
   121→        self,
   122→        messages: List[LLMMessage],
   123→        tools: Optional[List[Dict[str, Any]]] = None,
   124→        tool_choice: Optional[str] = None,  # "auto", "required", or {"type": "function", "function": {"name": "xxx"}}
   125→        user_tier: str = "free",
   126→        task: str = "chat",
   127→        model: Optional[str] = None,
   128→        temperature: float = 0.7,
   129→        max_tokens: int = 4096,
   130→    ) -> AsyncGenerator[Dict[str, Any], None]:
   131→        """
   132→        Stream chat with LLM.
   133→
   134→        Yields:
   135→            dict with type: "content" | "tool_call"
   136→        """
   137→        selection = ModelConfig.resolve(user_tier, task)
   138→        model = model or selection.model
   139→        provider = selection.provider
   140→
   141→        async for chunk in self._stream_provider(
   142→            provider=provider,
   143→            model=model,
   144→            messages=messages,
   145→            tools=tools,
   146→            tool_choice=tool_choice,
   147→            temperature=temperature,
   148→            max_tokens=max_tokens
   149→        ):
   150→            yield chunk
   151→
   152→    async def _call_provider(
   153→        self,
   154→        provider: str,
   155→        model: str,
   156→        messages: List[LLMMessage],
   157→        tools: Optional[List[Dict[str, Any]]],
   158→        temperature: float,
   159→        max_tokens: int,
   160→        stream: bool = False
   161→    ) -> LLMResponse:
   162→        """Call specific provider"""
   163→        # Use Claude-specific method for anthropic-compatible providers
   164→        if provider in ("claude", "anthropic", "deepseek", "glm", "zhipu"):
   165→            return await self._call_claude(model, messages, tools, temperature, max_tokens, provider=provider)
   166→
   167→        api_key = self._get_api_key(provider)
   168→        base_url = self._get_base_url(provider)
   169→
   170→        # Format messages for OpenAI-compatible API
   171→        formatted_messages = []
   172→        for m in messages:
   173→            msg = {"role": m.role, "content": m.content}
   174→            if m.tool_call_id:
   175→                msg["tool_call_id"] = m.tool_call_id
   176→            if m.tool_calls:
   177→                msg["tool_calls"] = m.tool_calls
   178→            formatted_messages.append(msg)
   179→
   180→        payload = {
   181→            "model": model,
   182→            "messages": formatted_messages,
   183→            "temperature": temperature,
   184→            "max_tokens": max_tokens,
   185→            "stream": stream
   186→        }
   187→        if tools:
   188→            payload["tools"] = tools
   189→
   190→        headers = {
   191→            "Authorization": f"Bearer {api_key}",
   192→            "Content-Type": "application/json"
   193→        }
   194→
   195→        async with httpx.AsyncClient(timeout=self.timeout) as client:
   196→            response = await client.post(
   197→                f"{base_url}/chat/completions",
   198→                json=payload,
   199→                headers=headers
   200→            )
   201→            response.raise_for_status()
   202→            data = response.json()
   203→
   204→        message = data["choices"][0]["message"]
   205→        content = message.get("content", "") or ""
   206→
   207→        # Parse tool calls
   208→        tool_calls = None
   209→        if "tool_calls" in message and message["tool_calls"]:
   210→            tool_calls = [
   211→                ToolCall(
   212→                    id=tc["id"],
   213→                    name=tc["function"]["name"],
   214→                    arguments=tc["function"]["arguments"]
   215→                )
   216→                for tc in message["tool_calls"]
   217→            ]
   218→
   219→        return LLMResponse(
   220→            content=content,
   221→            model=model,
   222→            provider=provider,
   223→            tool_calls=tool_calls,
   224→            usage=data.get("usage")
   225→        )
   226→
   227→    async def _call_claude(
   228→        self,
   229→        model: str,
   230→        messages: List[LLMMessage],
   231→        tools: Optional[List[Dict[str, Any]]],
   232→        temperature: float,
   233→        max_tokens: int,
   234→        provider: str = "claude"
   235→    ) -> LLMResponse:
   236→        """Call Claude API (Anthropic format)"""
   237→        api_key = self._get_api_key(provider)
   238→        base_url = self._get_base_url(provider)
   239→
   240→        # Extract system message and convert to Anthropic format
   241→        system_content = ""
   242→        anthropic_messages = []
   243→
   244→        for m in messages:
   245→            if m.role == "system":
   246→                system_content = m.content
   247→            elif m.role == "tool":
   248→                # Convert tool result to Anthropic format
   249→                # Tool results must follow an assistant message with tool_use
   250→                anthropic_messages.append({
   251→                    "role": "user",
   252→                    "content": [{
   253→                        "type": "tool_result",
   254→                        "tool_use_id": m.tool_call_id,
   255→                        "content": m.content
   256→                    }]
   257→                })
   258→            elif m.role == "assistant" and m.tool_calls:
   259→                # Assistant message with tool calls - convert to Anthropic format
   260→                content_blocks = []
   261→                if m.content:
   262→                    content_blocks.append({"type": "text", "text": m.content})
   263→                for tc in m.tool_calls:
   264→                    content_blocks.append({
   265→                        "type": "tool_use",
   266→                        "id": tc.get("id", ""),
   267→                        "name": tc.get("function", {}).get("name", ""),
   268→                        "input": json.loads(tc.get("function", {}).get("arguments", "{}"))
   269→                    })
   270→                anthropic_messages.append({
   271→                    "role": "assistant",
   272→                    "content": content_blocks
   273→                })
   274→            else:
   275→                anthropic_messages.append({
   276→                    "role": m.role,
   277→                    "content": m.content
   278→                })
   279→
   280→        # Convert OpenAI tools format to Anthropic format
   281→        anthropic_tools = None
   282→        if tools:
   283→            anthropic_tools = []
   284→            for t in tools:
   285→                if t.get("type") == "function":
   286→                    func = t["function"]
   287→                    anthropic_tools.append({
   288→                        "name": func["name"],
   289→                        "description": func.get("description", ""),
   290→                        "input_schema": func.get("parameters", {"type": "object", "properties": {}})
   291→                    })
   292→
   293→        payload = {
   294→            "model": model,
   295→            "max_tokens": max_tokens,
   296→            "messages": anthropic_messages,
   297→        }
   298→        if system_content:
   299→            payload["system"] = system_content
   300→        if anthropic_tools:
   301→            payload["tools"] = anthropic_tools
   302→
   303→        headers = {
   304→            "x-api-key": api_key,
   305→            "anthropic-version": "2023-06-01",
   306→            "Content-Type": "application/json"
   307→        }
   308→
   309→        async with httpx.AsyncClient(timeout=self.timeout) as client:
   310→            response = await client.post(
   311→                f"{base_url}/messages",
   312→                json=payload,
   313→                headers=headers
   314→            )
   315→            response.raise_for_status()
   316→            data = response.json()
   317→
   318→        # Parse Claude response
   319→        content = ""
   320→        tool_calls = []
   321→
   322→        for block in data.get("content", []):
   323→            if block.get("type") == "text":
   324→                content += block.get("text", "")
   325→            elif block.get("type") == "tool_use":
   326→                tool_calls.append(ToolCall(
   327→                    id=block.get("id", ""),
   328→                    name=block.get("name", ""),
   329→                    arguments=json.dumps(block.get("input", {}))
   330→                ))
   331→
   332→        return LLMResponse(
   333→            content=content,
   334→            model=model,
   335→            provider=provider,
   336→            tool_calls=tool_calls if tool_calls else None,
   337→            usage=data.get("usage")
   338→        )
   339→
   340→    async def _stream_provider(
   341→        self,
   342→        provider: str,
   343→        model: str,
   344→        messages: List[LLMMessage],
   345→        tools: Optional[List[Dict[str, Any]]],
   346→        tool_choice: Optional[str] = None,
   347→        temperature: float = 0.7,
   348→        max_tokens: int = 4096
   349→    ) -> AsyncGenerator[Dict[str, Any], None]:
   350→        """Stream from specific provider"""
   351→        logger.info(f"_stream_provider called with provider={provider}, model={model}")
   352→
   353→        # Use Claude-specific streaming for anthropic/claude/deepseek/glm providers
   354→        if provider in ("claude", "anthropic", "deepseek", "glm", "zhipu"):
   355→            logger.info(f"Using Claude/Anthropic streaming for provider={provider}")
   356→            async for chunk in self._stream_claude(model, messages, tools, max_tokens, provider=provider, tool_choice=tool_choice):
   357→                yield chunk
   358→            return
   359→
   360→        logger.info(f"Using OpenAI-compatible streaming for provider={provider}")
   361→
   362→        api_key = self._get_api_key(provider)
   363→        base_url = self._get_base_url(provider)
   364→
   365→        # Debug: log messages
   366→        for i, m in enumerate(messages):
   367→            logger.debug(f"Message {i}: role={m.role}, content={m.content[:100] if m.content else None}..., tool_call_id={m.tool_call_id}, tool_calls={m.tool_calls}")
   368→
   369→        # Format messages for OpenAI-compatible API
   370→        formatted_messages = []
   371→        for m in messages:
   372→            msg = {"role": m.role, "content": m.content}
   373→            if m.tool_call_id:
   374→                msg["tool_call_id"] = m.tool_call_id
   375→            if m.tool_calls:
   376→                msg["tool_calls"] = m.tool_calls
   377→            formatted_messages.append(msg)
   378→
   379→        payload = {
   380→            "model": model,
   381→            "messages": formatted_messages,
   382→            "temperature": temperature,
   383→            "max_tokens": max_tokens,
   384→            "stream": True
   385→        }
   386→        if tools:
   387→            payload["tools"] = tools
   388→
   389→        headers = {
   390→            "Authorization": f"Bearer {api_key}",
   391→            "Content-Type": "application/json"
   392→        }
   393→
   394→        tool_call_chunks = {}
   395→
   396→        async with httpx.AsyncClient(timeout=self.timeout) as client:
   397→            async with client.stream(
   398→                "POST",
   399→                f"{base_url}/chat/completions",
   400→                json=payload,
   401→                headers=headers
   402→            ) as response:
   403→                response.raise_for_status()
   404→                async for line in response.aiter_lines():
   405→                    if line.startswith("data: "):
   406→                        data_str = line[6:]
   407→                        if data_str == "[DONE]":
   408→                            break
   409→                        try:
   410→                            data = json.loads(data_str)
   411→                            delta = data["choices"][0].get("delta", {})
   412→
   413→                            # Content chunks
   414→                            content = delta.get("content", "")
   415→                            if content:
   416→                                yield {"type": "content", "content": content}
   417→
   418→                            # Tool call chunks
   419→                            if "tool_calls" in delta:
   420→                                for tc_delta in delta["tool_calls"]:
   421→                                    index = tc_delta.get("index", 0)
   422→                                    if index not in tool_call_chunks:
   423→                                        tool_call_chunks[index] = {
   424→                                            "id": "",
   425→                                            "name": "",
   426→                                            "arguments": ""
   427→                                        }
   428→                                    if "id" in tc_delta:
   429→                                        tool_call_chunks[index]["id"] = tc_delta["id"]
   430→                                    if "function" in tc_delta:
   431→                                        func = tc_delta["function"]
   432→                                        if "name" in func:
   433→                                            tool_call_chunks[index]["name"] += func["name"]
   434→                                        if "arguments" in func:
   435→                                            tool_call_chunks[index]["arguments"] += func["arguments"]
   436→                        except json.JSONDecodeError:
   437→                            continue
   438→
   439→        # Emit complete tool calls
   440→        for tc in tool_call_chunks.values():
   441→            if tc["name"]:
   442→                yield {
   443→                    "type": "tool_call",
   444→                    "tool_call_id": tc["id"],
   445→                    "tool_name": tc["name"],
   446→                    "tool_args": tc["arguments"]
   447→                }
   448→
   449→    async def _stream_claude(
   450→        self,
   451→        model: str,
   452→        messages: List[LLMMessage],
   453→        tools: Optional[List[Dict[str, Any]]],
   454→        max_tokens: int,
   455→        provider: str = "claude",
   456→        tool_choice: Optional[str] = None
   457→    ) -> AsyncGenerator[Dict[str, Any], None]:
   458→        """Stream from Claude/Anthropic-compatible API"""
   459→        api_key = self._get_api_key(provider)
   460→        base_url = self._get_base_url(provider)
   461→
   462→        # Extract system message and convert to Anthropic format
   463→        system_content = ""
   464→        anthropic_messages = []
   465→
   466→        for m in messages:
   467→            if m.role == "system":
   468→                system_content = m.content
   469→            elif m.role == "tool":
   470→                anthropic_messages.append({
   471→                    "role": "user",
   472→                    "content": [{
   473→                        "type": "tool_result",
   474→                        "tool_use_id": m.tool_call_id,
   475→                        "content": m.content
   476→                    }]
   477→                })
   478→            elif m.role == "assistant" and m.tool_calls:
   479→                # Assistant message with tool calls - convert to Anthropic format
   480→                content_blocks = []
   481→                if m.content:
   482→                    content_blocks.append({"type": "text", "text": m.content})
   483→                for tc in m.tool_calls:
   484→                    content_blocks.append({
   485→                        "type": "tool_use",
   486→                        "id": tc.get("id", ""),
   487→                        "name": tc.get("function", {}).get("name", ""),
   488→                        "input": json.loads(tc.get("function", {}).get("arguments", "{}"))
   489→                    })
   490→                anthropic_messages.append({
   491→                    "role": "assistant",
   492→                    "content": content_blocks
   493→                })
   494→            else:
   495→                anthropic_messages.append({
   496→                    "role": m.role,
   497→                    "content": m.content
   498→                })
   499→
   500→        # Convert OpenAI tools format to Anthropic format
   501→        anthropic_tools = None
   502→        if tools:
   503→            anthropic_tools = []
   504→            for t in tools:
   505→                if t.get("type") == "function":
   506→                    func = t["function"]
   507→                    anthropic_tools.append({
   508→                        "name": func["name"],
   509→                        "description": func.get("description", ""),
   510→                        "input_schema": func.get("parameters", {"type": "object", "properties": {}})
   511→                    })
   512→
   513→        payload = {
   514→            "model": model,
   515→            "max_tokens": max_tokens,
   516→            "messages": anthropic_messages,
   517→            "stream": True
   518→        }
   519→        if system_content:
   520→            payload["system"] = system_content
   521→        if anthropic_tools:
   522→            payload["tools"] = anthropic_tools
   523→            # Add tool_choice if specified (Anthropic format)
   524→            if tool_choice:
   525→                if tool_choice == "required":
   526→                    payload["tool_choice"] = {"type": "any"}
   527→                elif tool_choice == "auto":
   528→                    payload["tool_choice"] = {"type": "auto"}
   529→                elif isinstance(tool_choice, dict):
   530→                    # {"type": "function", "function": {"name": "xxx"}} -> {"type": "tool", "name": "xxx"}
   531→                    if tool_choice.get("type") == "function":
   532→                        payload["tool_choice"] = {"type": "tool", "name": tool_choice["function"]["name"]}
   533→                    else:
   534→                        payload["tool_choice"] = tool_choice
   535→
   536→        headers = {
   537→            "x-api-key": api_key,
   538→            "anthropic-version": "2023-06-01",
   539→            "Content-Type": "application/json"
   540→        }
   541→
   542→        # Track tool use blocks being built
   543→        current_tool_use = {}
   544→
   545→        async with httpx.AsyncClient(timeout=self.timeout) as client:
   546→            async with client.stream(
   547→                "POST",
   548→                f"{base_url}/messages",
   549→                json=payload,
   550→                headers=headers
   551→            ) as response:
   552→                response.raise_for_status()
   553→                async for line in response.aiter_lines():
   554→                    if line.startswith("data: "):
   555→                        data_str = line[6:]
   556→                        try:
   557→                            data = json.loads(data_str)
   558→                            event_type = data.get("type", "")
   559→
   560→                            if event_type == "content_block_start":
   561→                                block = data.get("content_block", {})
   562→                                if block.get("type") == "tool_use":
   563→                                    current_tool_use = {
   564→                                        "id": block.get("id", ""),
   565→                                        "name": block.get("name", ""),
   566→                                        "input_json": ""
   567→                                    }
   568→
   569→                            elif event_type == "content_block_delta":
   570→                                delta = data.get("delta", {})
   571→                                if delta.get("type") == "text_delta":
   572→                                    text = delta.get("text", "")
   573→                                    if text:
   574→                                        yield {"type": "content", "content": text}
   575→                                elif delta.get("type") == "input_json_delta":
   576→                                    if current_tool_use:
   577→                                        current_tool_use["input_json"] += delta.get("partial_json", "")
   578→
   579→                            elif event_type == "content_block_stop":
   580→                                if current_tool_use and current_tool_use.get("name"):
   581→                                    yield {
   582→                                        "type": "tool_call",
   583→                                        "tool_call_id": current_tool_use["id"],
   584→                                        "tool_name": current_tool_use["name"],
   585→                                        "tool_args": current_tool_use["input_json"]
   586→                                    }
   587→                                    current_tool_use = {}
   588→
   589→                            elif event_type == "message_stop":
   590→                                break
   591→
   592→                        except json.JSONDecodeError:
   593→                            continue
   594→
   595→    def _get_api_key(self, provider: str) -> str:
   596→        """Get API key for provider - 从 models.yaml 配置读取"""
   597→        return LLMConfig.get_api_key(provider)
   598→
   599→    def _get_base_url(self, provider: str) -> str:
   600→        """Get base URL for provider - 从 models.yaml 配置读取"""
   601→        return LLMConfig.get_base_url(provider)
   602→
   603→    def _record_usage(self, response: LLMResponse):
   604→        """Record usage for quota tracking"""
   605→        self._usage["calls"] += 1
   606→        if response.usage:
   607→            tokens = (
   608→                response.usage.get("total_tokens", 0) or
   609→                response.usage.get("prompt_tokens", 0) + response.usage.get("completion_tokens", 0)
   610→            )
   611→            self._usage["tokens"] += tokens
   612→
   613→
   614→# Global instance
   615→_llm_client: Optional[LLMClient] = None
   616→
   617→
   618→def get_llm_client() -> LLMClient:
   619→    """Get or create global LLM client"""
   620→    global _llm_client
   621→    if _llm_client is None:
   622→        _llm_client = LLMClient()
   623→    return _llm_client
   624→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
