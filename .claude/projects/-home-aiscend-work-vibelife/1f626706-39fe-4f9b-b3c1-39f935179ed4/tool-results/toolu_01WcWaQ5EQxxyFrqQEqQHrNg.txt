     1→"""
     2→Retrieval Service V2 - Hybrid search with Jieba preprocessing
     3→"""
     4→from typing import Optional, List, Dict, Any
     5→
     6→import jieba
     7→
     8→from stores import KnowledgeRepository
     9→from .embedding import EmbeddingService
    10→
    11→
    12→class RetrievalService:
    13→    """
    14→    Knowledge retrieval with hybrid search (Vector + FTS + RRF)
    15→
    16→    V2 improvements:
    17→    - Application-layer Jieba tokenization
    18→    - Uses PostgreSQL SQL functions for efficient RRF
    19→    - Supports section path information
    20→    """
    21→
    22→    @classmethod
    23→    def preprocess_query(cls, query: str) -> str:
    24→        """
    25→        Preprocess query with Jieba tokenization
    26→
    27→        Input:  "比肩代表独立能力strong independence"
    28→        Output: "比肩 代表 独立 能力 strong independence"
    29→        """
    30→        tokens = jieba.cut_for_search(query)
    31→        return " ".join(tokens)
    32→
    33→    @classmethod
    34→    async def search(
    35→        cls,
    36→        query: str,
    37→        skill_id: str,
    38→        top_k: int = 5,
    39→        use_hybrid: bool = True,
    40→        vector_weight: float = 0.7,
    41→        text_weight: float = 0.3
    42→    ) -> List[Dict[str, Any]]:
    43→        """
    44→        Search knowledge base with hybrid retrieval.
    45→
    46→        Args:
    47→            query: User's search query
    48→            skill_id: Skill to search in
    49→            top_k: Number of results to return
    50→            use_hybrid: Whether to use hybrid search (default True)
    51→            vector_weight: Weight for vector similarity
    52→            text_weight: Weight for text match
    53→
    54→        Returns:
    55→            List of matching chunks with content, score, and metadata
    56→        """
    57→        # Generate query embedding
    58→        query_embedding = await EmbeddingService.embed_query(query)
    59→
    60→        if use_hybrid:
    61→            # Preprocess query for FTS
    62→            query_preprocessed = cls.preprocess_query(query)
    63→
    64→            # Hybrid search using SQL function
    65→            results = await KnowledgeRepository.hybrid_search(
    66→                query_preprocessed=query_preprocessed,
    67→                embedding=query_embedding,
    68→                skill_id=skill_id,
    69→                top_k=top_k,
    70→                vector_weight=vector_weight,
    71→                text_weight=text_weight
    72→            )
    73→        else:
    74→            # Vector-only search
    75→            results = await KnowledgeRepository.vector_search(
    76→                embedding=query_embedding,
    77→                skill_id=skill_id,
    78→                top_k=top_k
    79→            )
    80→
    81→        return results
    82→
    83→    @classmethod
    84→    async def text_only_search(
    85→        cls,
    86→        query: str,
    87→        skill_id: str,
    88→        top_k: int = 5
    89→    ) -> List[Dict[str, Any]]:
    90→        """Search using full-text search only (no embeddings)"""
    91→        query_preprocessed = cls.preprocess_query(query)
    92→
    93→        results = await KnowledgeRepository.text_search(
    94→            query_preprocessed=query_preprocessed,
    95→            skill_id=skill_id,
    96→            top_k=top_k
    97→        )
    98→
    99→        return results
   100→
   101→    @classmethod
   102→    async def get_context_for_query(
   103→        cls,
   104→        query: str,
   105→        skill_id: str,
   106→        max_chunks: int = 3,
   107→        max_chars: int = 3000
   108→    ) -> str:
   109→        """
   110→        Get formatted context string for LLM from knowledge base.
   111→
   112→        Args:
   113→            query: User's query
   114→            skill_id: Skill to search in
   115→            max_chunks: Maximum number of chunks to include
   116→            max_chars: Maximum total characters
   117→
   118→        Returns:
   119→            Formatted context string for LLM prompt
   120→        """
   121→        chunks = await cls.search(query, skill_id, top_k=max_chunks)
   122→
   123→        if not chunks:
   124→            return ""
   125→
   126→        context_parts = []
   127→        total_chars = 0
   128→
   129→        for chunk in chunks:
   130→            content = chunk["content"]
   131→            section_title = chunk.get("section_title", "")
   132→
   133→            # Check character limit
   134→            if total_chars + len(content) > max_chars:
   135→                # Truncate to fit
   136→                remaining = max_chars - total_chars
   137→                if remaining > 100:
   138→                    content = content[:remaining] + "..."
   139→                else:
   140→                    break
   141→
   142→            # Format with section info
   143→            if section_title:
   144→                context_parts.append(f"### {section_title}\n{content}")
   145→            else:
   146→                context_parts.append(content)
   147→
   148→            total_chars += len(content)
   149→
   150→        return "\n\n".join(context_parts)
   151→
   152→    @classmethod
   153→    async def get_related_sections(
   154→        cls,
   155→        query: str,
   156→        skill_id: str,
   157→        top_k: int = 5
   158→    ) -> List[Dict[str, Any]]:
   159→        """
   160→        Get related sections with their hierarchy paths.
   161→
   162→        Returns simplified results for UI display.
   163→        """
   164→        results = await cls.search(query, skill_id, top_k=top_k)
   165→
   166→        sections = []
   167→        for r in results:
   168→            sections.append({
   169→                "id": str(r["id"]),
   170→                "title": r.get("section_title") or "Untitled",
   171→                "path": r.get("section_path") or [],
   172→                "preview": r["content"][:150] + "..." if len(r["content"]) > 150 else r["content"],
   173→                "score": r.get("score", 0),
   174→                "match_type": r.get("match_type", "unknown")
   175→            })
   176→
   177→        return sections
   178→
   179→    @classmethod
   180→    async def index_content(
   181→        cls,
   182→        skill_id: str,
   183→        content: str,
   184→        content_type: str = "knowledge",
   185→        section_path: Optional[List[str]] = None,
   186→        section_title: Optional[str] = None,
   187→        metadata: Optional[Dict[str, Any]] = None
   188→    ) -> Dict[str, Any]:
   189→        """
   190→        Index new content into knowledge base.
   191→
   192→        For bulk indexing, use the IngestionWorker instead.
   193→        """
   194→        # Generate embedding
   195→        embedding = await EmbeddingService.embed_text(content)
   196→
   197→        # Preprocess for search
   198→        search_text = cls.preprocess_query(content)
   199→
   200→        # Create chunk
   201→        chunk = await KnowledgeRepository.create_chunk(
   202→            skill_id=skill_id,
   203→            content=content,
   204→            search_text_preprocessed=search_text,
   205→            embedding=embedding,
   206→            content_type=content_type,
   207→            section_path=section_path,
   208→            section_title=section_title,
   209→            metadata=metadata
   210→        )
   211→
   212→        return chunk
   213→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
