     1→"""
     2→Chat Routes - Conversation endpoints for VibeLife v3.0
     3→Based on: vibelife spec v3.0
     4→
     5→Features:
     6→- SSE streaming responses
     7→- Voice mode toggle (warm / sarcastic)
     8→- Skill-based conversations (bazi / zodiac)
     9→- Guest mode for demo
    10→- AI Interview endpoints
    11→- File upload + extraction
    12→"""
    13→import json
    14→import logging
    15→from typing import Optional, List
    16→from uuid import UUID, uuid4
    17→
    18→from fastapi import APIRouter, HTTPException, Depends, Query, UploadFile, File
    19→from fastapi.responses import StreamingResponse
    20→from pydantic import BaseModel, Field
    21→from sse_starlette.sse import EventSourceResponse
    22→
    23→from services.identity import get_optional_user, CurrentUser
    24→from services.vibe_engine import (
    25→    get_llm_service,
    26→    get_context_builder,
    27→    get_portrait_service,
    28→    get_tools_for_skill,
    29→    VoiceMode,
    30→    Skill,
    31→    create_user_message,
    32→)
    33→from services.interview import get_interview_service, InterviewSession
    34→from services.extraction import get_extraction_service
    35→from services.knowledge.retrieval import RetrievalService
    36→from stores import conversation_repo, message_repo, profile_repo
    37→
    38→router = APIRouter(prefix="/chat", tags=["Chat"])
    39→
    40→logger = logging.getLogger(__name__)
    41→
    42→
    43→# ═══════════════════════════════════════════════════════════════════════════
    44→# Request/Response Models
    45→# ═══════════════════════════════════════════════════════════════════════════
    46→
    47→class ChatRequest(BaseModel):
    48→    """Chat request model"""
    49→    message: str = Field(..., description="User message")
    50→    skill: str = Field(..., description="Skill: bazi or zodiac")
    51→    conversation_id: Optional[UUID] = Field(None, description="Existing conversation ID")
    52→    voice_mode: Optional[str] = Field("warm", description="Voice mode: warm or sarcastic")
    53→
    54→
    55→class ChatResponse(BaseModel):
    56→    """Chat response model (for non-streaming)"""
    57→    content: str
    58→    conversation_id: UUID
    59→    skill: str
    60→    voice_mode: str
    61→    metadata: Optional[dict] = None
    62→
    63→
    64→class GuestChatRequest(BaseModel):
    65→    """Guest chat request (no auth required)"""
    66→    message: str
    67→    skill: str = "bazi"
    68→
    69→
    70→# ═══════════════════════════════════════════════════════════════════════════
    71→# Helper Functions
    72→# ═══════════════════════════════════════════════════════════════════════════
    73→
    74→def parse_skill(skill_str: str) -> Skill:
    75→    """Parse skill string to enum"""
    76→    skill_lower = skill_str.lower()
    77→    if skill_lower in ("bazi", "八字"):
    78→        return Skill.BAZI
    79→    elif skill_lower in ("zodiac", "星座"):
    80→        return Skill.ZODIAC
    81→    else:
    82→        raise HTTPException(status_code=400, detail=f"Unknown skill: {skill_str}")
    83→
    84→
    85→def parse_voice_mode(mode_str: str) -> VoiceMode:
    86→    """Parse voice mode string to enum"""
    87→    mode_lower = mode_str.lower()
    88→    if mode_lower in ("warm", "温暖"):
    89→        return VoiceMode.WARM
    90→    elif mode_lower in ("sarcastic", "吐槽"):
    91→        return VoiceMode.SARCASTIC
    92→    else:
    93→        return VoiceMode.WARM  # Default
    94→
    95→
    96→async def get_user_profile(user_id: Optional[UUID]) -> dict:
    97→    """Get user profile from database"""
    98→    if user_id:
    99→        try:
   100→            profile_data = await profile_repo.get_profile_data(user_id)
   101→            return profile_data
   102→        except Exception as e:
   103→            logger.error(f"Failed to get user profile for {user_id}: {e}")
   104→            # Fall through to return default profile
   105→
   106→    # Return default profile for guests or on error
   107→    from services.vibe_engine import DEFAULT_PROFILE
   108→    return DEFAULT_PROFILE.copy()
   109→
   110→
   111→async def get_conversation_history(
   112→    conversation_id: Optional[UUID],
   113→    limit: int = 20
   114→) -> List[dict]:
   115→    """Get conversation history from database"""
   116→    if not conversation_id:
   117→        return []
   118→
   119→    try:
   120→        messages = await message_repo.get_messages_for_context(conversation_id, limit)
   121→        return messages
   122→    except Exception as e:
   123→        logger.error(f"Failed to get conversation history for {conversation_id}: {e}")
   124→        return []
   125→
   126→
   127→async def save_message(
   128→    conversation_id: UUID,
   129→    role: str,
   130→    content: str,
   131→    metadata: Optional[dict] = None
   132→) -> None:
   133→    """Save message to database"""
   134→    try:
   135→        await message_repo.create_message(
   136→            conversation_id=conversation_id,
   137→            role=role,
   138→            content=content,
   139→            metadata=metadata
   140→        )
   141→    except Exception as e:
   142→        # Log but don't fail the request
   143→        logger.warning(f"Failed to save message: {e}")
   144→
   145→
   146→# ═══════════════════════════════════════════════════════════════════════════
   147→# Endpoints
   148→# ═══════════════════════════════════════════════════════════════════════════
   149→
   150→@router.post("/stream")
   151→async def chat_stream(
   152→    request: ChatRequest,
   153→    current_user: Optional[CurrentUser] = Depends(get_optional_user)
   154→):
   155→    """
   156→    Send a message and get streaming response (SSE).
   157→
   158→    Returns Server-Sent Events with chunks of the response.
   159→    Final event contains [DONE] marker.
   160→    """
   161→    skill = parse_skill(request.skill)
   162→    voice_mode = parse_voice_mode(request.voice_mode or "warm")
   163→
   164→    # Get or create conversation
   165→    conversation_id = request.conversation_id or uuid4()
   166→
   167→    # Get user ID if authenticated
   168→    user_id = current_user.user_id if current_user else None
   169→
   170→    # Get services
   171→    llm = get_llm_service()
   172→    context_builder = get_context_builder()
   173→
   174→    async def generate():
   175→        try:
   176→            # Get user profile (for logged-in users)
   177→            profile = await get_user_profile(user_id)
   178→
   179→            # Get conversation history
   180→            history = await get_conversation_history(conversation_id)
   181→
   182→            # Retrieve relevant knowledge chunks (RAG)
   183→            knowledge_chunks = []
   184→            try:
   185→                knowledge_chunks = await RetrievalService.search(
   186→                    query=request.message,
   187→                    skill_id=skill.value,
   188→                    top_k=3
   189→                )
   190→            except Exception as e:
   191→                import logging
   192→                logging.warning(f"Knowledge retrieval failed: {e}")
   193→
   194→            # Build context with knowledge
   195→            system_prompt, messages = await context_builder.build(
   196→                skill=skill,
   197→                voice_mode=voice_mode,
   198→                current_message=request.message,
   199→                profile=profile,
   200→                history=history,
   201→                knowledge_chunks=knowledge_chunks
   202→            )
   203→
   204→            # Save user message
   205→            await save_message(conversation_id, "user", request.message)
   206→
   207→            # Get tools for the current skill
   208→            tools = get_tools_for_skill(skill.value)
   209→
   210→            # Stream response with tool support
   211→            full_content = ""
   212→            tool_calls = []
   213→
   214→            async for chunk in llm.stream(messages, tools=tools):
   215→                chunk_type = chunk.get("type")
   216→
   217→                if chunk_type == "content":
   218→                    # Text content chunk
   219→                    content = chunk.get("content", "")
   220→                    full_content += content
   221→                    yield {
   222→                        "event": "message",
   223→                        "data": json.dumps({
   224→                            "type": "chunk",
   225→                            "content": content
   226→                        })
   227→                    }
   228→
   229→                elif chunk_type == "tool_call":
   230→                    # Tool call event - emit to frontend
   231→                    tool_call_id = chunk.get("tool_call_id")
   232→                    tool_name = chunk.get("tool_name")
   233→                    tool_args_str = chunk.get("tool_args", "{}")
   234→
   235→                    # Parse args
   236→                    try:
   237→                        tool_args = json.loads(tool_args_str)
   238→                    except json.JSONDecodeError:
   239→                        tool_args = {}
   240→
   241→                    # Store for later
   242→                    tool_calls.append({
   243→                        "id": tool_call_id,
   244→                        "name": tool_name,
   245→                        "args": tool_args
   246→                    })
   247→
   248→                    # Emit tool_call event (AI SDK 6 format)
   249→                    yield {
   250→                        "event": "message",
   251→                        "data": json.dumps({
   252→                            "type": "tool_call",
   253→                            "tool_name": tool_name,
   254→                            "tool_call_id": tool_call_id,
   255→                            "args": tool_args
   256→                        })
   257→                    }
   258→
   259→            # Save assistant message
   260→            message_metadata = {}
   261→            if tool_calls:
   262→                message_metadata["tool_calls"] = tool_calls
   263→
   264→            await save_message(
   265→                conversation_id,
   266→                "assistant",
   267→                full_content,
   268→                metadata=message_metadata
   269→            )
   270→
   271→            # Send completion event
   272→            yield {
   273→                "event": "message",
   274→                "data": json.dumps({
   275→                    "type": "done",
   276→                    "conversation_id": str(conversation_id),
   277→                    "skill": skill.value,
   278→                    "voice_mode": voice_mode.value
   279→                })
   280→            }
   281→
   282→        except Exception as e:
   283→            logger.error(f"Chat stream error: {e}", exc_info=True)
   284→            yield {
   285→                "event": "error",
   286→                "data": json.dumps({
   287→                    "type": "error",
   288→                    "message": str(e)
   289→                })
   290→            }
   291→
   292→    return EventSourceResponse(generate())
   293→
   294→
   295→@router.post("/", response_model=ChatResponse)
   296→async def chat(
   297→    request: ChatRequest,
   298→    current_user: Optional[CurrentUser] = Depends(get_optional_user)
   299→):
   300→    """
   301→    Send a message and get non-streaming response.
   302→    For clients that don't support SSE.
   303→    """
   304→    skill = parse_skill(request.skill)
   305→    voice_mode = parse_voice_mode(request.voice_mode or "warm")
   306→
   307→    # Get or create conversation
   308→    conversation_id = request.conversation_id or uuid4()
   309→
   310→    # Get user ID if authenticated
   311→    user_id = current_user.user_id if current_user else None
   312→
   313→    # Get services
   314→    llm = get_llm_service()
   315→    context_builder = get_context_builder()
   316→
   317→    try:
   318→        # Get user profile
   319→        profile = await get_user_profile(user_id)
   320→
   321→        # Get conversation history
   322→        history = await get_conversation_history(conversation_id)
   323→
   324→        # Retrieve relevant knowledge chunks (RAG)
   325→        knowledge_chunks = []
   326→        try:
   327→            knowledge_chunks = await RetrievalService.search(
   328→                query=request.message,
   329→                skill_id=skill.value,
   330→                top_k=3
   331→            )
   332→        except Exception as e:
   333→            import logging
   334→            logging.warning(f"Knowledge retrieval failed: {e}")
   335→
   336→        # Build context with knowledge
   337→        system_prompt, messages = await context_builder.build(
   338→            skill=skill,
   339→            voice_mode=voice_mode,
   340→            current_message=request.message,
   341→            profile=profile,
   342→            history=history,
   343→            knowledge_chunks=knowledge_chunks
   344→        )
   345→
   346→        # Save user message
   347→        await save_message(conversation_id, "user", request.message)
   348→
   349→        # Get response
   350→        response = await llm.chat(messages)
   351→
   352→        # Save assistant message
   353→        await save_message(conversation_id, "assistant", response.content)
   354→
   355→        return ChatResponse(
   356→            content=response.content,
   357→            conversation_id=conversation_id,
   358→            skill=skill.value,
   359→            voice_mode=voice_mode.value,
   360→            metadata={
   361→                "model": response.model,
   362→                "usage": response.usage
   363→            }
   364→        )
   365→
   366→    except Exception as e:
   367→        raise HTTPException(status_code=500, detail=str(e))
   368→
   369→
   370→@router.post("/guest")
   371→async def chat_guest(request: GuestChatRequest):
   372→    """
   373→    Guest chat endpoint (no authentication required).
   374→    For landing page demo with limited functionality.
   375→    """
   376→    skill = parse_skill(request.skill)
   377→
   378→    llm = get_llm_service()
   379→    context_builder = get_context_builder()
   380→
   381→    try:
   382→        # Build simple context (no profile, no history)
   383→        system_prompt, messages = await context_builder.build(
   384→            skill=skill,
   385→            voice_mode=VoiceMode.WARM,
   386→            current_message=request.message,
   387→            profile=None,
   388→            history=None
   389→        )
   390→
   391→        # Get response (limited tokens for guest)
   392→        response = await llm.chat(messages, max_tokens=1024)
   393→
   394→        return {
   395→            "content": response.content,
   396→            "is_guest": True,
   397→            "skill": skill.value,
   398→            "suggestion": "注册后可以获得完整的个性化分析和对话历史保存"
   399→        }
   400→
   401→    except Exception as e:
   402→        raise HTTPException(status_code=500, detail=str(e))
   403→
   404→
   405→@router.post("/guest/stream")
   406→async def chat_guest_stream(request: GuestChatRequest):
   407→    """
   408→    Guest streaming chat endpoint.
   409→    For landing page demo with SSE.
   410→    """
   411→    skill = parse_skill(request.skill)
   412→
   413→    llm = get_llm_service()
   414→    context_builder = get_context_builder()
   415→
   416→    async def generate():
   417→        try:
   418→            # Build simple context
   419→            system_prompt, messages = await context_builder.build(
   420→                skill=skill,
   421→                voice_mode=VoiceMode.WARM,
   422→                current_message=request.message,
   423→                profile=None,
   424→                history=None
   425→            )
   426→
   427→            # Get tools for the current skill
   428→            tools = get_tools_for_skill(skill.value)
   429→
   430→            # Stream response with tool support
   431→            async for chunk in llm.stream(messages, max_tokens=1024, tools=tools):
   432→                chunk_type = chunk.get("type")
   433→
   434→                if chunk_type == "content":
   435→                    # Text content chunk
   436→                    content = chunk.get("content", "")
   437→                    yield {
   438→                        "event": "message",
   439→                        "data": json.dumps({
   440→                            "type": "chunk",
   441→                            "content": content
   442→                        })
   443→                    }
   444→
   445→                elif chunk_type == "tool_call":
   446→                    # Tool call event - emit to frontend
   447→                    tool_call_id = chunk.get("tool_call_id")
   448→                    tool_name = chunk.get("tool_name")
   449→                    tool_args_str = chunk.get("tool_args", "{}")
   450→
   451→                    # Parse args
   452→                    try:
   453→                        tool_args = json.loads(tool_args_str)
   454→                    except json.JSONDecodeError:
   455→                        tool_args = {}
   456→
   457→                    # Emit tool_call event
   458→                    yield {
   459→                        "event": "message",
   460→                        "data": json.dumps({
   461→                            "type": "tool_call",
   462→                            "tool_name": tool_name,
   463→                            "tool_call_id": tool_call_id,
   464→                            "args": tool_args
   465→                        })
   466→                    }
   467→
   468→            yield {
   469→                "event": "message",
   470→                "data": json.dumps({
   471→                    "type": "done",
   472→                    "is_guest": True,
   473→                    "skill": skill.value
   474→                })
   475→            }
   476→
   477→        except Exception as e:
   478→            logger.error(f"Guest chat stream error: {e}", exc_info=True)
   479→            yield {
   480→                "event": "error",
   481→                "data": json.dumps({
   482→                    "type": "error",
   483→                    "message": str(e)
   484→                })
   485→            }
   486→
   487→    return EventSourceResponse(generate())
   488→
   489→
   490→# ═══════════════════════════════════════════════════════════════════════════
   491→# Voice Mode Toggle
   492→# ═══════════════════════════════════════════════════════════════════════════
   493→
   494→class VoiceModeRequest(BaseModel):
   495→    """Voice mode toggle request"""
   496→    conversation_id: UUID
   497→    voice_mode: str  # warm | sarcastic
   498→
   499→
   500→@router.post("/voice-mode")
   501→async def toggle_voice_mode(request: VoiceModeRequest):
   502→    """
   503→    Toggle voice mode for a conversation.
   504→    Changes Vibe's personality between warm and sarcastic.
   505→    """
   506→    voice_mode = parse_voice_mode(request.voice_mode)
   507→
   508→    # TODO: Update conversation voice_mode in database
   509→
   510→    return {
   511→        "success": True,
   512→        "conversation_id": str(request.conversation_id),
   513→        "voice_mode": voice_mode.value,
   514→        "message": "语气已切换" if voice_mode == VoiceMode.SARCASTIC else "已切换到温暖模式"
   515→    }
   516→
   517→
   518→# ═══════════════════════════════════════════════════════════════════════════
   519→# Conversation Management
   520→# ═══════════════════════════════════════════════════════════════════════════
   521→
   522→@router.get("/conversations")
   523→async def list_conversations(
   524→    skill: Optional[str] = None,
   525→    limit: int = Query(20, ge=1, le=100)
   526→):
   527→    """
   528→    List user's conversations.
   529→    TODO: Implement with authentication.
   530→    """
   531→    # TODO: Implement with actual database query
   532→    return {
   533→        "conversations": [],
   534→        "total": 0
   535→    }
   536→
   537→
   538→@router.get("/conversations/{conversation_id}")
   539→async def get_conversation(conversation_id: UUID):
   540→    """
   541→    Get conversation details with messages.
   542→    """
   543→    # TODO: Implement with actual database query
   544→    return {
   545→        "id": str(conversation_id),
   546→        "messages": [],
   547→        "skill": None,
   548→        "voice_mode": "warm",
   549→        "created_at": None
   550→    }
   551→
   552→
   553→@router.delete("/conversations/{conversation_id}")
   554→async def delete_conversation(conversation_id: UUID):
   555→    """
   556→    Delete a conversation.
   557→    """
   558→    try:
   559→        await conversation_repo.delete_conversation(conversation_id)
   560→        return {"success": True, "message": "Conversation deleted"}
   561→    except Exception as e:
   562→        raise HTTPException(status_code=500, detail=str(e))
   563→
   564→
   565→# ═══════════════════════════════════════════════════════════════════════════
   566→# Interview Endpoints
   567→# ═══════════════════════════════════════════════════════════════════════════
   568→
   569→class InterviewStartRequest(BaseModel):
   570→    """Start interview request"""
   571→    skill: str = "bazi"
   572→    user_id: Optional[UUID] = None
   573→
   574→
   575→class InterviewAnswerRequest(BaseModel):
   576→    """Submit answer request"""
   577→    session_id: UUID
   578→    answer: str
   579→
   580→
   581→@router.post("/interview/start")
   582→async def start_interview(request: InterviewStartRequest):
   583→    """
   584→    Start an AI interview session.
   585→    Required before generating reports.
   586→    """
   587→    interview_service = get_interview_service()
   588→
   589→    session = await interview_service.start_session(
   590→        skill=request.skill,
   591→        user_id=request.user_id
   592→    )
   593→
   594→    current_question = interview_service.get_current_question(session)
   595→
   596→    return {
   597→        "session": session.to_dict(),
   598→        "intro": interview_service.get_intro(),
   599→        "current_question": {
   600→            "id": current_question.id,
   601→            "text": current_question.question_text,
   602→            "type": current_question.question_type
   603→        } if current_question else None,
   604→        "progress": interview_service.get_progress(session)
   605→    }
   606→
   607→
   608→@router.post("/interview/answer")
   609→async def submit_interview_answer(request: InterviewAnswerRequest):
   610→    """
   611→    Submit an answer to the current interview question.
   612→    """
   613→    interview_service = get_interview_service()
   614→
   615→    session = await interview_service.get_session(request.session_id)
   616→    if not session:
   617→        raise HTTPException(status_code=404, detail="Session not found")
   618→
   619→    next_question, is_complete = await interview_service.submit_answer(
   620→        session,
   621→        request.answer
   622→    )
   623→
   624→    response = {
   625→        "is_complete": is_complete,
   626→        "next_question": None,
   627→        "progress": interview_service.get_progress(session),
   628→        "result": None
   629→    }
   630→
   631→    if is_complete:
   632→        result = interview_service.get_result(session)
   633→        response["result"] = {
   634→            "success": result.success,
   635→            "extracted_profile": result.extracted_profile,
   636→            "summary": result.summary,
   637→            "questions_answered": result.questions_answered,
   638→            "total_questions": result.total_questions
   639→        }
   640→    elif next_question:
   641→        response["next_question"] = {
   642→            "id": next_question.id,
   643→            "text": next_question.question_text,
   644→            "type": next_question.question_type
   645→        }
   646→
   647→    return response
   648→
   649→
   650→@router.post("/interview/skip")
   651→async def skip_interview(session_id: UUID):
   652→    """
   653→    Skip the interview (user choice).
   654→    """
   655→    interview_service = get_interview_service()
   656→
   657→    session = await interview_service.get_session(session_id)
   658→    if not session:
   659→        raise HTTPException(status_code=404, detail="Session not found")
   660→
   661→    warning = await interview_service.skip_session(session)
   662→    result = interview_service.get_result(session)
   663→
   664→    return {
   665→        "warning": warning,
   666→        "result": {
   667→            "success": result.success,
   668→            "extracted_profile": result.extracted_profile,
   669→            "summary": result.summary,
   670→            "questions_answered": result.questions_answered,
   671→            "total_questions": result.total_questions
   672→        }
   673→    }
   674→
   675→
   676→@router.get("/interview/{session_id}")
   677→async def get_interview_session(session_id: UUID):
   678→    """
   679→    Get interview session status.
   680→    """
   681→    interview_service = get_interview_service()
   682→
   683→    session = await interview_service.get_session(session_id)
   684→    if not session:
   685→        raise HTTPException(status_code=404, detail="Session not found")
   686→
   687→    current_question = interview_service.get_current_question(session)
   688→
   689→    return {
   690→        "session": session.to_dict(),
   691→        "current_question": {
   692→            "id": current_question.id,
   693→            "text": current_question.question_text,
   694→            "type": current_question.question_type
   695→        } if current_question else None,
   696→        "progress": interview_service.get_progress(session)
   697→    }
   698→
   699→
   700→# ═══════════════════════════════════════════════════════════════════════════
   701→# File Upload Endpoints
   702→# ═══════════════════════════════════════════════════════════════════════════
   703→
   704→@router.post("/upload")
   705→async def upload_file(
   706→    file: UploadFile = File(...),
   707→    context_hint: Optional[str] = None
   708→):
   709→    """
   710→    Upload a file for AI extraction.
   711→
   712→    Supported formats:
   713→    - Images (JPG, PNG): Screenshots from other apps, chat records
   714→    - Documents (PDF, DOCX, TXT): Resumes, diaries
   715→
   716→    Returns extracted information that can be merged into user profile.
   717→    """
   718→    # Read file data
   719→    file_data = await file.read()
   720→
   721→    # Check file size (10MB limit)
   722→    if len(file_data) > 10 * 1024 * 1024:
   723→        raise HTTPException(status_code=413, detail="文件过大，最大支持 10MB")
   724→
   725→    # Extract information
   726→    extraction_service = get_extraction_service()
   727→
   728→    result = await extraction_service.extract_from_file(
   729→        file_data=file_data,
   730→        filename=file.filename or "unknown",
   731→        mime_type=file.content_type,
   732→        context_hint=context_hint
   733→    )
   734→
   735→    return {
   736→        "success": result.success,
   737→        "file_type": result.file_type,
   738→        "content_type": result.content_type,
   739→        "summary": result.summary,
   740→        "confidence": result.confidence,
   741→        "extracted_data": result.extracted_data,
   742→        "profile_updates": result.profile_updates,
   743→        "error": result.error
   744→    }
   745→
   746→
   747→@router.post("/upload/batch")
   748→async def upload_files_batch(
   749→    files: List[UploadFile] = File(...)
   750→):
   751→    """
   752→    Upload multiple files for batch extraction.
   753→    """
   754→    if len(files) > 5:
   755→        raise HTTPException(status_code=400, detail="最多同时上传 5 个文件")
   756→
   757→    extraction_service = get_extraction_service()
   758→    results = []
   759→
   760→    for file in files:
   761→        file_data = await file.read()
   762→
   763→        if len(file_data) > 10 * 1024 * 1024:
   764→            results.append({
   765→                "filename": file.filename,
   766→                "success": False,
   767→                "error": "文件过大"
   768→            })
   769→            continue
   770→
   771→        result = await extraction_service.extract_from_file(
   772→            file_data=file_data,
   773→            filename=file.filename or "unknown",
   774→            mime_type=file.content_type
   775→        )
   776→
   777→        results.append({
   778→            "filename": file.filename,
   779→            "success": result.success,
   780→            "content_type": result.content_type,
   781→            "summary": result.summary,
   782→            "error": result.error
   783→        })
   784→
   785→    return {
   786→        "total": len(files),
   787→        "successful": sum(1 for r in results if r["success"]),
   788→        "results": results
   789→    }
   790→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
