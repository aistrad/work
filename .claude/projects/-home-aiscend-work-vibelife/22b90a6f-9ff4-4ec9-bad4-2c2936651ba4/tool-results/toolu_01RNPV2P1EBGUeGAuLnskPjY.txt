     1→#!/usr/bin/env python3
     2→"""
     3→Knowledge Builder Pipeline - Unified CLI Entry Point
     4→
     5→Usage:
     6→    python scripts/build_knowledge.py --skill bazi --stages all
     7→    python scripts/build_knowledge.py --skill bazi --stages 0,1,2,3
     8→    python scripts/build_knowledge.py --skill bazi --stages 4a,4b
     9→    python scripts/build_knowledge.py --skill bazi --stages 5 --review
    10→    python scripts/build_knowledge.py --skill bazi --stages 6 --report
    11→"""
    12→import asyncio
    13→import argparse
    14→import logging
    15→import sys
    16→import os
    17→from pathlib import Path
    18→from datetime import datetime
    19→
    20→# Add project root to path
    21→# parents[0]=scripts, [1]=api, [2]=apps, [3]=vibelife
    22→project_root = Path(__file__).resolve().parents[3]
    23→api_path = project_root / "apps" / "api"
    24→if str(api_path) not in sys.path:
    25→    sys.path.insert(0, str(api_path))
    26→
    27→from dotenv import load_dotenv
    28→load_dotenv(project_root / ".env")
    29→
    30→logging.basicConfig(
    31→    level=logging.INFO,
    32→    format="%(asctime)s [%(levelname)s] %(message)s"
    33→)
    34→logger = logging.getLogger("knowledge_builder")
    35→
    36→# Data directories
    37→DATA_DIR = Path("/data/vibelife")
    38→SKILLS_DIR = api_path / "skills"
    39→
    40→
    41→class KnowledgeBuilder:
    42→    """
    43→    Unified knowledge building pipeline.
    44→
    45→    Stages:
    46→        0: Format conversion (source → converted MD)
    47→        1: Parse & chunk
    48→        2: Vectorize
    49→        3: Store in database
    50→        4a: Extract cases
    51→        4b: Generate scenario candidates
    52→        5: Review & publish scenarios
    53→        6: Quality check & report
    54→    """
    55→
    56→    def __init__(self, skill_id: str):
    57→        self.skill_id = skill_id
    58→        self.source_dir = DATA_DIR / "knowledge" / skill_id / "source"
    59→        self.converted_dir = DATA_DIR / "knowledge" / skill_id / "converted"
    60→        self.skill_dir = SKILLS_DIR / skill_id
    61→
    62→    async def run_stage_0(self):
    63→        """Stage 0: Format conversion (supports subdirectories)"""
    64→        logger.info("=" * 60)
    65→        logger.info("Stage 0: Format Conversion")
    66→        logger.info("=" * 60)
    67→
    68→        from workers.converters import DocumentConverter
    69→
    70→        converter = DocumentConverter()
    71→        self.source_dir.mkdir(parents=True, exist_ok=True)
    72→        self.converted_dir.mkdir(parents=True, exist_ok=True)
    73→
    74→        # Recursively find all files in source directory
    75→        source_files = list(self.source_dir.rglob("*"))
    76→        converted_count = 0
    77→
    78→        for source_file in source_files:
    79→            if source_file.is_file() and converter.can_convert(str(source_file)):
    80→                # Preserve subdirectory structure in converted dir
    81→                rel_path = source_file.relative_to(self.source_dir)
    82→                converted_subdir = self.converted_dir / rel_path.parent
    83→                converted_subdir.mkdir(parents=True, exist_ok=True)
    84→                converted_path = converted_subdir / f"{source_file.stem}.converted.md"
    85→
    86→                if converted_path.exists():
    87→                    logger.info(f"  Skip (exists): {rel_path}")
    88→                    continue
    89→
    90→                try:
    91→                    md_content = converter.convert(str(source_file))
    92→                    converted_path.write_text(md_content, encoding="utf-8")
    93→                    converted_count += 1
    94→                    logger.info(f"  Converted: {rel_path} → {converted_path.name}")
    95→                except Exception as e:
    96→                    logger.error(f"  Failed: {rel_path} - {e}")
    97→
    98→        logger.info(f"Stage 0 complete: {converted_count} files converted")
    99→        return converted_count
   100→
   101→    async def run_stages_1_2_3(self):
   102→        """Stages 1-3: Chunk, vectorize, store (supports subdirectories)"""
   103→        logger.info("=" * 60)
   104→        logger.info("Stages 1-3: Chunk → Vectorize → Store")
   105→        logger.info("=" * 60)
   106→
   107→        from workers.ingestion import IngestionWorker
   108→
   109→        worker = IngestionWorker()
   110→        # Recursively find all MD files
   111→        md_files = list(self.converted_dir.rglob("*.md"))
   112→
   113→        total_chunks = 0
   114→        for md_file in md_files:
   115→            try:
   116→                result = await worker.process_file(str(md_file), self.skill_id)
   117→                chunks = result.get("chunk_count", 0)
   118→                total_chunks += chunks
   119→                logger.info(f"  Processed: {md_file.name} ({chunks} chunks)")
   120→            except Exception as e:
   121→                logger.error(f"  Failed: {md_file.name} - {e}")
   122→
   123→        logger.info(f"Stages 1-3 complete: {total_chunks} total chunks")
   124→        return total_chunks
   125→
   126→    async def run_stage_4a(self, limit: int = 100):
   127→        """Stage 4a: Extract cases"""
   128→        logger.info("=" * 60)
   129→        logger.info("Stage 4a: Case Extraction")
   130→        logger.info("=" * 60)
   131→
   132→        from workers.case_extractor import CaseExtractor
   133→        from stores.db import get_connection
   134→
   135→        # Fetch chunks
   136→        async with get_connection() as conn:
   137→            rows = await conn.fetch(
   138→                """
   139→                SELECT id, chunk_text as content
   140→                FROM knowledge_chunks
   141→                WHERE skill_id = $1
   142→                LIMIT $2
   143→                """,
   144→                self.skill_id,
   145→                limit,
   146→            )
   147→            chunks = [dict(row) for row in rows]
   148→
   149→        if not chunks:
   150→            logger.warning("No chunks found for case extraction")
   151→            return 0
   152→
   153→        extractor = CaseExtractor()
   154→        cases = await extractor.extract_from_chunks(chunks, self.skill_id)
   155→        saved = await extractor.save_cases(cases)
   156→
   157→        logger.info(f"Stage 4a complete: {len(cases)} cases extracted, {saved} saved")
   158→        return saved
   159→
   160→    async def run_stage_4b(self, limit: int = 50):
   161→        """Stage 4b: Generate scenario candidates"""
   162→        logger.info("=" * 60)
   163→        logger.info("Stage 4b: Scenario Generation")
   164→        logger.info("=" * 60)
   165→
   166→        from workers.scenario_generator import ScenarioGenerator
   167→        from stores.db import get_connection
   168→
   169→        # Fetch chunks
   170→        async with get_connection() as conn:
   171→            rows = await conn.fetch(
   172→                """
   173→                SELECT id, chunk_text as content, chunk_type
   174→                FROM knowledge_chunks
   175→                WHERE skill_id = $1
   176→                ORDER BY chunk_type DESC
   177→                LIMIT $2
   178→                """,
   179→                self.skill_id,
   180→                limit,
   181→            )
   182→            chunks = [dict(row) for row in rows]
   183→
   184→        if not chunks:
   185→            logger.warning("No chunks found for scenario generation")
   186→            return 0
   187→
   188→        generator = ScenarioGenerator()
   189→        scenarios = await generator.generate_from_chunks(chunks, self.skill_id)
   190→        saved = await generator.save_candidates(scenarios)
   191→
   192→        logger.info(f"Stage 4b complete: {len(scenarios)} scenarios generated, {saved} saved")
   193→        return saved
   194→
   195→    async def run_stage_5(self, interactive: bool = True):
   196→        """Stage 5: Review & publish scenarios"""
   197→        logger.info("=" * 60)
   198→        logger.info("Stage 5: Scenario Review & Publish")
   199→        logger.info("=" * 60)
   200→
   201→        from workers.scenario_reviewer import ScenarioReviewer, review_cli
   202→
   203→        if interactive:
   204→            await review_cli()
   205→        else:
   206→            # Auto-approve all pending
   207→            reviewer = ScenarioReviewer()
   208→            pending = await reviewer.list_pending(self.skill_id)
   209→
   210→            for s in pending:
   211→                await reviewer.approve(s["skill_id"], s["scenario_id"])
   212→                logger.info(f"  Auto-approved: {s['scenario_id']}")
   213→
   214→            logger.info(f"Stage 5 complete: {len(pending)} scenarios approved")
   215→            return len(pending)
   216→
   217→    async def run_stage_6(self, save_report: bool = True):
   218→        """Stage 6: Quality check"""
   219→        logger.info("=" * 60)
   220→        logger.info("Stage 6: Quality Check")
   221→        logger.info("=" * 60)
   222→
   223→        from workers.quality_checker import QualityChecker
   224→
   225→        checker = QualityChecker()
   226→        report = await checker.generate_report(self.skill_id)
   227→
   228→        print(checker.format_report(report))
   229→
   230→        if save_report:
   231→            report_id = await checker.save_report(report)
   232→            logger.info(f"Report saved with ID: {report_id}")
   233→
   234→        return report.overall_score
   235→
   236→    async def run_all(self):
   237→        """Run complete pipeline"""
   238→        logger.info("=" * 60)
   239→        logger.info(f"Knowledge Builder Pipeline - {self.skill_id}")
   240→        logger.info(f"Started: {datetime.now().isoformat()}")
   241→        logger.info("=" * 60)
   242→
   243→        results = {}
   244→
   245→        # Stage 0
   246→        results["stage_0"] = await self.run_stage_0()
   247→
   248→        # Stages 1-3
   249→        results["stages_1_3"] = await self.run_stages_1_2_3()
   250→
   251→        # Stage 4a
   252→        results["stage_4a"] = await self.run_stage_4a()
   253→
   254→        # Stage 4b
   255→        results["stage_4b"] = await self.run_stage_4b()
   256→
   257→        # Stage 5 (non-interactive for full pipeline)
   258→        results["stage_5"] = await self.run_stage_5(interactive=False)
   259→
   260→        # Stage 6
   261→        results["stage_6"] = await self.run_stage_6()
   262→
   263→        logger.info("=" * 60)
   264→        logger.info("Pipeline Complete!")
   265→        logger.info(f"Results: {results}")
   266→        logger.info("=" * 60)
   267→
   268→        return results
   269→
   270→
   271→async def main():
   272→    parser = argparse.ArgumentParser(
   273→        description="Knowledge Builder Pipeline",
   274→        formatter_class=argparse.RawDescriptionHelpFormatter,
   275→        epilog="""
   276→Examples:
   277→  # Run all stages
   278→  python build_knowledge.py --skill bazi --stages all
   279→
   280→  # Run specific stages
   281→  python build_knowledge.py --skill bazi --stages 0,1,2,3
   282→  python build_knowledge.py --skill bazi --stages 4a,4b
   283→
   284→  # Interactive review
   285→  python build_knowledge.py --skill bazi --stages 5 --review
   286→
   287→  # Quality report only
   288→  python build_knowledge.py --skill bazi --stages 6
   289→        """
   290→    )
   291→
   292→    parser.add_argument(
   293→        "--skill", "-s",
   294→        required=True,
   295→        help="Skill ID (bazi, zodiac, tarot, career)"
   296→    )
   297→    parser.add_argument(
   298→        "--stages",
   299→        default="all",
   300→        help="Stages to run: all, 0, 1-3, 4a, 4b, 5, 6 (comma-separated)"
   301→    )
   302→    parser.add_argument(
   303→        "--review",
   304→        action="store_true",
   305→        help="Interactive review mode for stage 5"
   306→    )
   307→    parser.add_argument(
   308→        "--limit",
   309→        type=int,
   310→        default=100,
   311→        help="Limit for chunk processing (default: 100)"
   312→    )
   313→
   314→    args = parser.parse_args()
   315→
   316→    builder = KnowledgeBuilder(args.skill)
   317→
   318→    if args.stages == "all":
   319→        await builder.run_all()
   320→    else:
   321→        stages = [s.strip() for s in args.stages.split(",")]
   322→
   323→        for stage in stages:
   324→            if stage == "0":
   325→                await builder.run_stage_0()
   326→            elif stage in ("1", "2", "3", "1-3"):
   327→                await builder.run_stages_1_2_3()
   328→            elif stage == "4a":
   329→                await builder.run_stage_4a(args.limit)
   330→            elif stage == "4b":
   331→                await builder.run_stage_4b(args.limit)
   332→            elif stage == "5":
   333→                await builder.run_stage_5(interactive=args.review)
   334→            elif stage == "6":
   335→                await builder.run_stage_6()
   336→            else:
   337→                logger.warning(f"Unknown stage: {stage}")
   338→
   339→
   340→if __name__ == "__main__":
   341→    asyncio.run(main())
   342→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
