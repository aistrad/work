     1→"""
     2→Unified Extractor - Stage 4 of Knowledge Building Pipeline
     3→
     4→Extracts both Cases and Scenarios from MD files in a single LLM call.
     5→Supports incremental processing via MD5 hash tracking.
     6→
     7→Features:
     8→- Reads directly from MD source files (not DB chunks)
     9→- Single LLM call extracts both Case and Scenario
    10→- Incremental processing via extraction_log.json
    11→- Large files split by sections (< 30K tokens per call)
    12→- Output: Cases → DB, Scenarios → files
    13→"""
    14→import asyncio
    15→import hashlib
    16→import json
    17→import logging
    18→import re
    19→import uuid
    20→from dataclasses import dataclass, asdict
    21→from datetime import datetime
    22→from pathlib import Path
    23→from typing import Optional
    24→
    25→logger = logging.getLogger(__name__)
    26→
    27→# Base directories
    28→DATA_DIR = Path("/data/vibelife")
    29→
    30→# Skill-specific configurations
    31→SKILL_CONFIGS = {
    32→    "bazi": {
    33→        "thinking_frameworks": """
    34→主体思维架构：身强/身弱判定
    35→客体思维架构：十神分析
    36→五行思维架构：五行分布特征
    37→柱位思维架构：四柱位置含义
    38→冲合思维架构：冲合关系解读
    39→时运思维架构：大运流年分析
    40→正偏思维架构：正偏十神转换
    41→""",
    42→        "core_data_schema": """
    43→- year: 年柱 (如: 甲子)
    44→- month: 月柱
    45→- day: 日柱
    46→- hour: 时柱
    47→- gender: 性别 (男/女)
    48→""",
    49→        "features_schema": """
    50→- daymaster: 日主 (如: 甲木)
    51→- strength: 身强/身弱
    52→- pattern: 格局 (如: 正官格)
    53→- key_gods: 关键十神 (数组)
    54→""",
    55→    },
    56→    "zodiac": {
    57→        "thinking_frameworks": """
    58→元素思维架构：火/土/风/水元素分析
    59→模式思维架构：基本/固定/变动模式分析
    60→相位思维架构：行星相位解读
    61→宫位思维架构：十二宫位分析
    62→行运思维架构：行运推进分析
    63→""",
    64→        "core_data_schema": """
    65→- sun_sign: 太阳星座
    66→- moon_sign: 月亮星座
    67→- rising_sign: 上升星座
    68→- birth_chart: 星盘配置摘要
    69→""",
    70→        "features_schema": """
    71→- dominant_element: 主导元素
    72→- dominant_modality: 主导模式
    73→- key_aspects: 关键相位
    74→- stellium: 星群 (如有)
    75→""",
    76→    },
    77→    "tarot": {
    78→        "thinking_frameworks": """
    79→大小牌思维架构：大牌vs小牌分析
    80→花色思维架构：权杖/圣杯/宝剑/钱币
    81→数字思维架构：牌号数字含义
    82→叙事思维架构：牌阵故事整合
    83→元素尊严架构：元素强弱分析
    84→""",
    85→        "core_data_schema": """
    86→- spread_type: 牌阵类型
    87→- cards: 抽到的牌 (数组)
    88→- question: 问题类型
    89→""",
    90→        "features_schema": """
    91→- major_count: 大牌数量
    92→- dominant_suit: 主导花色
    93→- reversed_count: 逆位数量
    94→- key_cards: 关键牌
    95→""",
    96→    },
    97→    "career": {
    98→        "thinking_frameworks": """
    99→职业锚思维架构：核心职业动机分析
   100→霍兰德思维架构：职业兴趣类型分析
   101→MBTI思维架构：性格类型分析
   102→技能矩阵架构：技能组合分析
   103→发展阶段架构：职业周期分析
   104→""",
   105→        "core_data_schema": """
   106→- background: 职业背景
   107→- current_role: 当前职位
   108→- challenge: 面临挑战
   109→""",
   110→        "features_schema": """
   111→- industry: 行业
   112→- career_stage: 职业阶段
   113→- key_skills: 核心技能
   114→- goals: 职业目标
   115→""",
   116→    },
   117→}
   118→
   119→UNIFIED_EXTRACTION_PROMPT = """你是一个专业的知识抽取专家。请从以下文本中同时识别：
   120→1. **案例 (Case)**：具体的分析实例，包含完整的推理过程
   121→2. **场景 (Scenario)**：可服务用户的服务流程定义
   122→
   123→## Skill 类型
   124→{skill_id}
   125→
   126→## 该 Skill 的思维架构体系
   127→{thinking_frameworks}
   128→
   129→## 源文件信息
   130→- 文件: {source_file}
   131→- 章节: {source_section}
   132→
   133→## 案例数据结构
   134→
   135→### core_data (核心数据)
   136→{core_data_schema}
   137→
   138→### features (特征数据)
   139→{features_schema}
   140→
   141→## 待分析文本
   142→
   143→{text}
   144→
   145→## 输出格式
   146→
   147→以 JSON 格式输出，结构如下：
   148→
   149→{{
   150→  "cases": [
   151→    {{
   152→      "name": "案例名称",
   153→      "core_data": {{}},
   154→      "features": {{}},
   155→      "thinking_frameworks_used": ["架构1"],
   156→      "reasoning_chain": [
   157→        {{"step": 1, "framework": "...", "observation": "...", "analysis": "...", "conclusion": "..."}}
   158→      ],
   159→      "guidance_patterns": [
   160→        {{"pattern_name": "...", "condition": "...", "advice": "...", "source": "..."}}
   161→      ],
   162→      "tags": [],
   163→      "scenario_ids": []
   164→    }}
   165→  ],
   166→  "scenarios": [
   167→    {{
   168→      "scenario_id": "英文ID",
   169→      "name": "中文名称",
   170→      "level": "entry|standard|professional",
   171→      "billing": "free|basic|premium",
   172→      "description": "服务描述",
   173→      "primary_triggers": ["触发词1"],
   174→      "secondary_triggers": [],
   175→      "sop_phases": [
   176→        {{"phase": 1, "name": "阶段名", "type": "required", "description": "...", "tools": [], "knowledge_queries": []}}
   177→      ]
   178→    }}
   179→  ],
   180→  "extraction_notes": ""
   181→}}
   182→
   183→如果文本不包含案例，cases 返回空数组 []。
   184→如果文本不包含场景定义，scenarios 返回空数组 []。
   185→请直接输出 JSON，不要添加其他说明。
   186→"""
   187→
   188→
   189→@dataclass
   190→class ExtractedCase:
   191→    """Extracted case data structure"""
   192→    name: str
   193→    skill_id: str
   194→    core_data: dict
   195→    features: dict
   196→    thinking_frameworks_used: list[str]
   197→    reasoning_chain: list[dict]
   198→    guidance_patterns: list[dict]
   199→    tags: list[str]
   200→    scenario_ids: list[str]
   201→    source_file: str
   202→    source_section: str
   203→    quality_score: float = 0.0
   204→    status: str = "pending"
   205→
   206→
   207→@dataclass
   208→class ExtractedScenario:
   209→    """Extracted scenario data structure"""
   210→    scenario_id: str
   211→    skill_id: str
   212→    name: str
   213→    level: str
   214→    billing: str
   215→    description: str
   216→    primary_triggers: list[str]
   217→    secondary_triggers: list[str]
   218→    sop_phases: list[dict]
   219→    source_file: str
   220→
   221→
   222→class UnifiedExtractor:
   223→    """
   224→    Stage 4: Unified Case & Scenario Extraction from MD Files
   225→
   226→    Features:
   227→    - Reads directly from MD source files (not DB chunks)
   228→    - Single LLM call extracts both Case and Scenario
   229→    - Incremental processing via extraction_log.json
   230→    - Large files split by sections (< 30K tokens per call)
   231→    - Output: Cases → DB, Scenarios → files
   232→    """
   233→
   234→    # Approximate tokens per character (conservative estimate)
   235→    CHARS_PER_TOKEN = 2.5
   236→    MAX_TOKENS = 30000
   237→
   238→    def __init__(self, skill_id: str):
   239→        self.skill_id = skill_id
   240→        self.converted_dir = DATA_DIR / "knowledge" / skill_id / "converted"
   241→        self.extracted_dir = DATA_DIR / "knowledge" / skill_id / "extracted"
   242→        self.log_path = self.extracted_dir / "extraction_log.json"
   243→        self.config = SKILL_CONFIGS.get(skill_id, SKILL_CONFIGS["bazi"])
   244→
   245→    def _ensure_directories(self):
   246→        """Create necessary directories if they don't exist"""
   247→        (self.extracted_dir / "cases").mkdir(parents=True, exist_ok=True)
   248→        (self.extracted_dir / "scenarios").mkdir(parents=True, exist_ok=True)
   249→
   250→    def _load_extraction_log(self) -> dict:
   251→        """Load or initialize extraction log"""
   252→        if self.log_path.exists():
   253→            try:
   254→                return json.loads(self.log_path.read_text())
   255→            except json.JSONDecodeError:
   256→                logger.warning("Corrupted extraction log, reinitializing")
   257→
   258→        return {
   259→            "skill_id": self.skill_id,
   260→            "prompt_version": 1,
   261→            "last_run": None,
   262→            "files": {},
   263→        }
   264→
   265→    def _save_extraction_log(self, log: dict):
   266→        """Save extraction log to file"""
   267→        self._ensure_directories()
   268→        log["last_run"] = datetime.now().isoformat()
   269→        self.log_path.write_text(json.dumps(log, indent=2, ensure_ascii=False))
   270→
   271→    def _compute_md5(self, file_path: Path) -> str:
   272→        """Compute MD5 hash of file content"""
   273→        content = file_path.read_bytes()
   274→        return hashlib.md5(content).hexdigest()
   275→
   276→    def _estimate_tokens(self, text: str) -> int:
   277→        """Estimate token count from text"""
   278→        return int(len(text) / self.CHARS_PER_TOKEN)
   279→
   280→    def _split_by_sections(self, content: str, max_tokens: int = None) -> list[dict]:
   281→        """
   282→        Split large MD file by sections (headers).
   283→
   284→        Returns list of dicts with 'name' and 'content' keys.
   285→        """
   286→        max_tokens = max_tokens or self.MAX_TOKENS
   287→
   288→        # Find all headers (## or ### level)
   289→        header_pattern = r'^(#{1,3})\s+(.+)$'
   290→        lines = content.split('\n')
   291→
   292→        sections = []
   293→        current_section = {"name": "开头", "content": [], "start_line": 0}
   294→
   295→        for i, line in enumerate(lines):
   296→            match = re.match(header_pattern, line)
   297→            if match:
   298→                # Save previous section if it has content
   299→                if current_section["content"]:
   300→                    current_section["content"] = '\n'.join(current_section["content"])
   301→                    sections.append(current_section)
   302→
   303→                # Start new section
   304→                current_section = {
   305→                    "name": match.group(2).strip(),
   306→                    "content": [line],
   307→                    "start_line": i,
   308→                }
   309→            else:
   310→                current_section["content"].append(line)
   311→
   312→        # Don't forget the last section
   313→        if current_section["content"]:
   314→            current_section["content"] = '\n'.join(current_section["content"])
   315→            sections.append(current_section)
   316→
   317→        # Merge small sections and split large ones
   318→        result = []
   319→        buffer = {"name": "", "content": ""}
   320→
   321→        for section in sections:
   322→            section_tokens = self._estimate_tokens(section["content"])
   323→
   324→            if section_tokens > max_tokens:
   325→                # Section too large, need to split further
   326→                # Save buffer first
   327→                if buffer["content"]:
   328→                    result.append(buffer)
   329→                    buffer = {"name": "", "content": ""}
   330→
   331→                # Split large section by paragraphs
   332→                paragraphs = section["content"].split('\n\n')
   333→                chunk = {"name": section["name"], "content": ""}
   334→
   335→                for para in paragraphs:
   336→                    if self._estimate_tokens(chunk["content"] + para) > max_tokens:
   337→                        if chunk["content"]:
   338→                            result.append(chunk)
   339→                        chunk = {"name": f"{section['name']} (续)", "content": para}
   340→                    else:
   341→                        chunk["content"] += ('\n\n' if chunk["content"] else '') + para
   342→
   343→                if chunk["content"]:
   344→                    result.append(chunk)
   345→
   346→            elif self._estimate_tokens(buffer["content"] + section["content"]) > max_tokens:
   347→                # Buffer would be too large, flush it
   348→                if buffer["content"]:
   349→                    result.append(buffer)
   350→                buffer = section
   351→
   352→            else:
   353→                # Merge with buffer
   354→                if buffer["content"]:
   355→                    buffer["name"] = f"{buffer['name']} + {section['name']}"
   356→                    buffer["content"] += '\n\n' + section["content"]
   357→                else:
   358→                    buffer = section
   359→
   360→        # Don't forget the buffer
   361→        if buffer["content"]:
   362→            result.append(buffer)
   363→
   364→        return result
   365→
   366→    async def _call_llm(self, text: str, source_file: str, source_section: str) -> dict:
   367→        """Call LLM with unified extraction prompt"""
   368→        from services.model_router import chat
   369→
   370→        prompt = UNIFIED_EXTRACTION_PROMPT.format(
   371→            skill_id=self.skill_id,
   372→            thinking_frameworks=self.config["thinking_frameworks"],
   373→            core_data_schema=self.config["core_data_schema"],
   374→            features_schema=self.config["features_schema"],
   375→            source_file=source_file,
   376→            source_section=source_section,
   377→            text=text,
   378→        )
   379→
   380→        try:
   381→            response = await chat(
   382→                messages=[{"role": "user", "content": prompt}],
   383→                capability="analysis",
   384→            )
   385→
   386→            content = response.content if hasattr(response, 'content') else str(response)
   387→
   388→            # Extract JSON from response
   389→            if "```json" in content:
   390→                content = content.split("```json")[1].split("```")[0]
   391→            elif "```" in content:
   392→                content = content.split("```")[1].split("```")[0]
   393→
   394→            data = json.loads(content.strip())
   395→            return data
   396→
   397→        except json.JSONDecodeError as e:
   398→            logger.warning(f"Failed to parse LLM response: {e}")
   399→            return {"cases": [], "scenarios": [], "extraction_notes": f"Parse error: {e}"}
   400→        except Exception as e:
   401→            logger.error(f"LLM call failed: {e}")
   402→            return {"cases": [], "scenarios": [], "extraction_notes": f"LLM error: {e}"}
   403→
   404→    def _calculate_quality_score(self, case: dict) -> float:
   405→        """
   406→        Calculate quality score for a case (0-1).
   407→
   408→        Scoring criteria:
   409→        - Has reasoning_chain with 2+ steps: 30%
   410→        - Has thinking_frameworks_used: 20%
   411→        - Has guidance_patterns: 25%
   412→        - Has complete core_data/features: 15%
   413→        - Has tags: 10%
   414→        """
   415→        score = 0.0
   416→
   417→        # Reasoning chain (30%)
   418→        reasoning = case.get("reasoning_chain", [])
   419→        if len(reasoning) >= 3:
   420→            score += 0.30
   421→        elif len(reasoning) >= 2:
   422→            score += 0.20
   423→        elif len(reasoning) >= 1:
   424→            score += 0.10
   425→
   426→        # Thinking frameworks (20%)
   427→        frameworks = case.get("thinking_frameworks_used", [])
   428→        if len(frameworks) >= 2:
   429→            score += 0.20
   430→        elif len(frameworks) >= 1:
   431→            score += 0.10
   432→
   433→        # Guidance patterns (25%)
   434→        patterns = case.get("guidance_patterns", [])
   435→        if len(patterns) >= 2:
   436→            score += 0.25
   437→        elif len(patterns) >= 1:
   438→            score += 0.15
   439→
   440→        # Core data / features (15%)
   441→        core_data = case.get("core_data", {})
   442→        features = case.get("features", {})
   443→        if core_data and features:
   444→            score += 0.15
   445→        elif core_data or features:
   446→            score += 0.07
   447→
   448→        # Tags (10%)
   449→        tags = case.get("tags", [])
   450→        if tags:
   451→            score += 0.10
   452→
   453→        return min(score, 1.0)
   454→
   455→    async def extract_from_file(self, file_path: Path) -> dict:
   456→        """
   457→        Extract Cases and Scenarios from a single MD file.
   458→        For large files, split by sections.
   459→
   460→        Returns:
   461→            dict with 'cases' and 'scenarios' lists
   462→        """
   463→        content = file_path.read_text(encoding='utf-8')
   464→        rel_path = str(file_path.relative_to(self.converted_dir))
   465→
   466→        token_estimate = self._estimate_tokens(content)
   467→        logger.info(f"Processing {rel_path} (~{token_estimate} tokens)")
   468→
   469→        results = {"cases": [], "scenarios": []}
   470→
   471→        if token_estimate > self.MAX_TOKENS:
   472→            # Large file - split by sections
   473→            sections = self._split_by_sections(content)
   474→            logger.info(f"  Split into {len(sections)} sections")
   475→
   476→            for i, section in enumerate(sections):
   477→                logger.info(f"  Processing section {i+1}/{len(sections)}: {section['name']}")
   478→                section_result = await self._call_llm(
   479→                    section["content"],
   480→                    rel_path,
   481→                    section["name"],
   482→                )
   483→
   484→                # Add source info to cases
   485→                for case in section_result.get("cases", []):
   486→                    case["source_file"] = rel_path
   487→                    case["source_section"] = section["name"]
   488→                    case["quality_score"] = self._calculate_quality_score(case)
   489→                    results["cases"].append(case)
   490→
   491→                # Add source info to scenarios
   492→                for scenario in section_result.get("scenarios", []):
   493→                    scenario["source_file"] = rel_path
   494→                    results["scenarios"].append(scenario)
   495→        else:
   496→            # Small file - process as whole
   497→            extraction = await self._call_llm(content, rel_path, "全文")
   498→
   499→            for case in extraction.get("cases", []):
   500→                case["source_file"] = rel_path
   501→                case["source_section"] = "全文"
   502→                case["quality_score"] = self._calculate_quality_score(case)
   503→                results["cases"].append(case)
   504→
   505→            for scenario in extraction.get("scenarios", []):
   506→                scenario["source_file"] = rel_path
   507→                results["scenarios"].append(scenario)
   508→
   509→        logger.info(f"  Extracted {len(results['cases'])} cases, {len(results['scenarios'])} scenarios")
   510→        return results
   511→
   512→    async def process_pending_files(self, force_reextract: bool = False) -> dict:
   513→        """
   514→        Incremental processing:
   515→        - Check MD5 hash for file changes
   516→        - Only process changed/new files
   517→        - Update extraction_log.json
   518→
   519→        Args:
   520→            force_reextract: If True, reprocess all files regardless of status
   521→
   522→        Returns:
   523→            dict with counts: cases_count, scenarios_count, files_processed
   524→        """
   525→        self._ensure_directories()
   526→        log = self._load_extraction_log()
   527→        md_files = list(self.converted_dir.rglob("*.md"))
   528→
   529→        results = {
   530→            "cases_count": 0,
   531→            "scenarios_count": 0,
   532→            "files_processed": 0,
   533→            "files_skipped": 0,
   534→        }
   535→
   536→        for md_file in md_files:
   537→            rel_path = str(md_file.relative_to(self.converted_dir))
   538→            current_md5 = self._compute_md5(md_file)
   539→
   540→            # Check if file needs processing
   541→            if not force_reextract and rel_path in log["files"]:
   542→                file_entry = log["files"][rel_path]
   543→                if file_entry.get("md5") == current_md5:
   544→                    logger.debug(f"Skipping {rel_path} (unchanged)")
   545→                    results["files_skipped"] += 1
   546→                    continue
   547→
   548→            logger.info(f"Processing: {rel_path}")
   549→
   550→            try:
   551→                # Extract
   552→                extraction = await self.extract_from_file(md_file)
   553→
   554→                # Save Cases to DB
   555→                saved_cases = await self._save_cases_to_db(extraction["cases"])
   556→
   557→                # Save Scenarios to files
   558→                saved_scenarios = await self._save_scenarios_to_files(extraction["scenarios"])
   559→
   560→                # Also save to YAML for review
   561→                await self._save_cases_to_yaml(extraction["cases"], rel_path)
   562→
   563→                # Update log
   564→                log["files"][rel_path] = {
   565→                    "status": "extracted",
   566→                    "md5": current_md5,
   567→                    "extracted_at": datetime.now().isoformat(),
   568→                    "cases_count": len(extraction["cases"]),
   569→                    "scenarios_count": len(extraction["scenarios"]),
   570→                }
   571→
   572→                results["cases_count"] += saved_cases
   573→                results["scenarios_count"] += saved_scenarios
   574→                results["files_processed"] += 1
   575→
   576→            except Exception as e:
   577→                logger.error(f"Failed to process {rel_path}: {e}")
   578→                log["files"][rel_path] = {
   579→                    "status": "error",
   580→                    "md5": current_md5,
   581→                    "error": str(e),
   582→                    "extracted_at": datetime.now().isoformat(),
   583→                }
   584→
   585→        self._save_extraction_log(log)
   586→
   587→        logger.info(
   588→            f"Extraction complete: {results['files_processed']} files processed, "
   589→            f"{results['files_skipped']} skipped, "
   590→            f"{results['cases_count']} cases, {results['scenarios_count']} scenarios"
   591→        )
   592→
   593→        return results
   594→
   595→    async def _save_cases_to_db(self, cases: list[dict]) -> int:
   596→        """
   597→        Insert cases to PostgreSQL with status='pending'.
   598→
   599→        Returns number of cases saved.
   600→        """
   601→        if not cases:
   602→            return 0
   603→
   604→        from stores.db import get_connection
   605→
   606→        saved = 0
   607→        async with get_connection() as conn:
   608→            for case in cases:
   609→                case_id = f"CASE_{self.skill_id}_{uuid.uuid4().hex[:8]}"
   610→
   611→                try:
   612→                    await conn.execute(
   613→                        """
   614→                        INSERT INTO cases (
   615→                            id, skill_id, scenario_ids, name,
   616→                            core_data, features, tags,
   617→                            thinking_frameworks_used,
   618→                            reasoning_chain, guidance_patterns,
   619→                            source_file, source_section,
   620→                            quality_score, status,
   621→                            analysis, conclusion, authority
   622→                        ) VALUES (
   623→                            $1, $2, $3, $4, $5, $6, $7, $8, $9, $10,
   624→                            $11, $12, $13, $14, $15, $16, $17
   625→                        )
   626→                        ON CONFLICT (id) DO UPDATE SET
   627→                            core_data = EXCLUDED.core_data,
   628→                            features = EXCLUDED.features,
   629→                            reasoning_chain = EXCLUDED.reasoning_chain,
   630→                            guidance_patterns = EXCLUDED.guidance_patterns,
   631→                            quality_score = EXCLUDED.quality_score
   632→                        """,
   633→                        case_id,
   634→                        self.skill_id,
   635→                        case.get("scenario_ids", ["basic_reading"]),
   636→                        case.get("name", "未命名案例"),
   637→                        json.dumps(case.get("core_data", {})),
   638→                        json.dumps(case.get("features", {})),
   639→                        case.get("tags", []),
   640→                        case.get("thinking_frameworks_used", []),
   641→                        json.dumps(case.get("reasoning_chain", [])),
   642→                        json.dumps(case.get("guidance_patterns", [])),
   643→                        case.get("source_file", ""),
   644→                        case.get("source_section", ""),
   645→                        case.get("quality_score", 0.0),
   646→                        "pending",
   647→                        json.dumps({"key_points": []}),  # Legacy field
   648→                        json.dumps({"summary": "", "advice": ""}),  # Legacy field
   649→                        "medium",
   650→                    )
   651→                    saved += 1
   652→                except Exception as e:
   653→                    logger.error(f"Failed to save case {case.get('name')}: {e}")
   654→
   655→        return saved
   656→
   657→    async def _save_cases_to_yaml(self, cases: list[dict], source_file: str):
   658→        """
   659→        Save cases to YAML file for human review.
   660→        """
   661→        if not cases:
   662→            return
   663→
   664→        import yaml
   665→
   666→        # Create filename from source file
   667→        file_stem = Path(source_file).stem.replace('.converted', '')
   668→        output_path = self.extracted_dir / "cases" / f"{file_stem}.cases.yaml"
   669→
   670→        yaml_content = yaml.dump(
   671→            {"source_file": source_file, "cases": cases},
   672→            allow_unicode=True,
   673→            default_flow_style=False,
   674→            sort_keys=False,
   675→        )
   676→
   677→        output_path.write_text(yaml_content, encoding='utf-8')
   678→        logger.debug(f"Saved cases YAML to {output_path}")
   679→
   680→    async def _save_scenarios_to_files(self, scenarios: list[dict]) -> int:
   681→        """
   682→        Save scenarios to extracted/scenarios/ directory as .md files.
   683→        These are candidates for human review.
   684→
   685→        Returns number of scenarios saved.
   686→        """
   687→        if not scenarios:
   688→            return 0
   689→
   690→        saved = 0
   691→        for scenario in scenarios:
   692→            scenario_id = scenario.get("scenario_id", f"scenario_{uuid.uuid4().hex[:8]}")
   693→            output_path = self.extracted_dir / "scenarios" / f"{scenario_id}.md"
   694→
   695→            md_content = self._render_scenario_md(scenario)
   696→            output_path.write_text(md_content, encoding='utf-8')
   697→            saved += 1
   698→            logger.debug(f"Saved scenario to {output_path}")
   699→
   700→        return saved
   701→
   702→    def _render_scenario_md(self, scenario: dict) -> str:
   703→        """Render scenario as Markdown file"""
   704→        sop_phases = scenario.get("sop_phases", [])
   705→        phases_md = ""
   706→
   707→        for phase in sop_phases:
   708→            tools_str = ", ".join(phase.get("tools", [])) or "无"
   709→            queries_str = ", ".join(phase.get("knowledge_queries", [])) or "无"
   710→
   711→            phases_md += f"""
   712→### Phase {phase.get('phase', '?')}: {phase.get('name', '未命名')}
   713→
   714→**类型**: {phase.get('type', 'required')}
   715→
   716→{phase.get('description', '')}
   717→
   718→**工具**: {tools_str}
   719→**知识查询**: {queries_str}
   720→"""
   721→
   722→        return f"""# {scenario.get('name', '未命名场景')}
   723→
   724→> 自动生成的场景候选，需人工审核后发布。
   725→> 来源文件: {scenario.get('source_file', '未知')}
   726→
   727→## 元数据
   728→
   729→| 属性 | 值 |
   730→|------|-----|
   731→| ID | `{scenario.get('scenario_id', '')}` |
   732→| Skill | {self.skill_id} |
   733→| 级别 | {scenario.get('level', 'entry')} |
   734→| 计费 | {scenario.get('billing', 'free')} |
   735→
   736→## 描述
   737→
   738→{scenario.get('description', '')}
   739→
   740→## 触发条件
   741→
   742→**主要触发词**: {', '.join(scenario.get('primary_triggers', []))}
   743→
   744→**次要触发词**: {', '.join(scenario.get('secondary_triggers', []))}
   745→
   746→## SOP 流程
   747→
   748→{phases_md}
   749→
   750→---
   751→
   752→*生成时间: {datetime.now().isoformat()}*
   753→"""
   754→
   755→
   756→# CLI entry point
   757→async def unified_extract_cli(skill_id: str, force: bool = False):
   758→    """CLI entry point for unified extraction"""
   759→    extractor = UnifiedExtractor(skill_id)
   760→    results = await extractor.process_pending_files(force_reextract=force)
   761→
   762→    print(f"\n{'='*60}")
   763→    print(f"Unified Extraction Complete - {skill_id}")
   764→    print(f"{'='*60}")
   765→    print(f"Files processed: {results['files_processed']}")
   766→    print(f"Files skipped:   {results['files_skipped']}")
   767→    print(f"Cases extracted: {results['cases_count']}")
   768→    print(f"Scenarios extracted: {results['scenarios_count']}")
   769→    print(f"{'='*60}")
   770→
   771→
   772→if __name__ == "__main__":
   773→    import sys
   774→
   775→    skill = sys.argv[1] if len(sys.argv) > 1 else "bazi"
   776→    force = "--force" in sys.argv or "-f" in sys.argv
   777→
   778→    asyncio.run(unified_extract_cli(skill, force))
   779→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
