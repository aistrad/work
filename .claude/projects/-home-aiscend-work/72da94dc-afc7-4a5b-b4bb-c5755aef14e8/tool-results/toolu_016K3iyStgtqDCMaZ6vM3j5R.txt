     1→"""
     2→LLM Orchestrator - Multi-model LLM support with streaming
     3→"""
     4→import os
     5→import json
     6→from typing import Optional, List, Dict, Any, AsyncGenerator
     7→from dataclasses import dataclass
     8→
     9→import httpx
    10→
    11→
    12→@dataclass
    13→class LLMMessage:
    14→    """LLM message format"""
    15→    role: str  # 'system', 'user', 'assistant'
    16→    content: str
    17→
    18→
    19→@dataclass
    20→class LLMResponse:
    21→    """LLM response"""
    22→    content: str
    23→    model: str
    24→    usage: Optional[Dict[str, int]] = None
    25→    finish_reason: Optional[str] = None
    26→
    27→
    28→class LLMOrchestrator:
    29→    """
    30→    Multi-model LLM orchestrator with streaming support.
    31→    Supports: GLM (primary), Claude (backup)
    32→    """
    33→
    34→    # ─────────────────────────────────────────────────────────────────
    35→    # Configuration
    36→    # ─────────────────────────────────────────────────────────────────
    37→
    38→    GLM_API_KEY = os.getenv("GLM_API_KEY", "")
    39→    GLM_BASE_URL = os.getenv("GLM_BASE_URL", "https://open.bigmodel.cn/api/paas/v4")
    40→    GLM_CHAT_MODEL = os.getenv("GLM_CHAT_MODEL", "glm-4.7")
    41→    GLM_VISION_MODEL = os.getenv("GLM_VISION_MODEL", "glm-4.6v")
    42→
    43→    CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY", "")
    44→    CLAUDE_MODEL = os.getenv("CLAUDE_MODEL", "claude-3-5-sonnet-20241022")
    45→
    46→    DEFAULT_PROVIDER = os.getenv("DEFAULT_LLM_PROVIDER", "glm")
    47→
    48→    # ─────────────────────────────────────────────────────────────────
    49→    # GLM Methods
    50→    # ─────────────────────────────────────────────────────────────────
    51→
    52→    @classmethod
    53→    async def chat_glm(
    54→        cls,
    55→        messages: List[LLMMessage],
    56→        model: Optional[str] = None,
    57→        temperature: float = 0.7,
    58→        max_tokens: int = 2048,
    59→        **kwargs
    60→    ) -> LLMResponse:
    61→        """Chat with GLM model (non-streaming)"""
    62→        model = model or cls.GLM_CHAT_MODEL
    63→
    64→        formatted_messages = [
    65→            {"role": m.role, "content": m.content}
    66→            for m in messages
    67→        ]
    68→
    69→        payload = {
    70→            "model": model,
    71→            "messages": formatted_messages,
    72→            "temperature": temperature,
    73→            "max_tokens": max_tokens,
    74→            **kwargs
    75→        }
    76→
    77→        headers = {
    78→            "Authorization": f"Bearer {cls.GLM_API_KEY}",
    79→            "Content-Type": "application/json"
    80→        }
    81→
    82→        async with httpx.AsyncClient(timeout=120.0) as client:
    83→            response = await client.post(
    84→                f"{cls.GLM_BASE_URL}/chat/completions",
    85→                json=payload,
    86→                headers=headers
    87→            )
    88→            response.raise_for_status()
    89→            data = response.json()
    90→
    91→        return LLMResponse(
    92→            content=data["choices"][0]["message"]["content"],
    93→            model=model,
    94→            usage=data.get("usage"),
    95→            finish_reason=data["choices"][0].get("finish_reason")
    96→        )
    97→
    98→    @classmethod
    99→    async def stream_glm(
   100→        cls,
   101→        messages: List[LLMMessage],
   102→        model: Optional[str] = None,
   103→        temperature: float = 0.7,
   104→        max_tokens: int = 2048,
   105→        **kwargs
   106→    ) -> AsyncGenerator[str, None]:
   107→        """Stream chat with GLM model"""
   108→        model = model or cls.GLM_CHAT_MODEL
   109→
   110→        formatted_messages = [
   111→            {"role": m.role, "content": m.content}
   112→            for m in messages
   113→        ]
   114→
   115→        payload = {
   116→            "model": model,
   117→            "messages": formatted_messages,
   118→            "temperature": temperature,
   119→            "max_tokens": max_tokens,
   120→            "stream": True,
   121→            **kwargs
   122→        }
   123→
   124→        headers = {
   125→            "Authorization": f"Bearer {cls.GLM_API_KEY}",
   126→            "Content-Type": "application/json"
   127→        }
   128→
   129→        async with httpx.AsyncClient(timeout=120.0) as client:
   130→            async with client.stream(
   131→                "POST",
   132→                f"{cls.GLM_BASE_URL}/chat/completions",
   133→                json=payload,
   134→                headers=headers
   135→            ) as response:
   136→                response.raise_for_status()
   137→                async for line in response.aiter_lines():
   138→                    if line.startswith("data: "):
   139→                        data_str = line[6:]
   140→                        if data_str == "[DONE]":
   141→                            break
   142→                        try:
   143→                            data = json.loads(data_str)
   144→                            delta = data["choices"][0].get("delta", {})
   145→                            content = delta.get("content", "")
   146→                            if content:
   147→                                yield content
   148→                        except json.JSONDecodeError:
   149→                            continue
   150→
   151→    # ─────────────────────────────────────────────────────────────────
   152→    # Claude Methods (backup)
   153→    # ─────────────────────────────────────────────────────────────────
   154→
   155→    @classmethod
   156→    async def chat_claude(
   157→        cls,
   158→        messages: List[LLMMessage],
   159→        model: Optional[str] = None,
   160→        temperature: float = 0.7,
   161→        max_tokens: int = 2048,
   162→        system: Optional[str] = None,
   163→        **kwargs
   164→    ) -> LLMResponse:
   165→        """Chat with Claude model (non-streaming)"""
   166→        if not cls.CLAUDE_API_KEY:
   167→            raise ValueError("CLAUDE_API_KEY not configured")
   168→
   169→        model = model or cls.CLAUDE_MODEL
   170→
   171→        # Extract system message
   172→        formatted_messages = []
   173→        for m in messages:
   174→            if m.role == "system":
   175→                system = m.content
   176→            else:
   177→                formatted_messages.append({
   178→                    "role": m.role,
   179→                    "content": m.content
   180→                })
   181→
   182→        payload = {
   183→            "model": model,
   184→            "messages": formatted_messages,
   185→            "max_tokens": max_tokens,
   186→            **kwargs
   187→        }
   188→        if system:
   189→            payload["system"] = system
   190→
   191→        headers = {
   192→            "x-api-key": cls.CLAUDE_API_KEY,
   193→            "anthropic-version": "2023-06-01",
   194→            "Content-Type": "application/json"
   195→        }
   196→
   197→        async with httpx.AsyncClient(timeout=120.0) as client:
   198→            response = await client.post(
   199→                "https://api.anthropic.com/v1/messages",
   200→                json=payload,
   201→                headers=headers
   202→            )
   203→            response.raise_for_status()
   204→            data = response.json()
   205→
   206→        return LLMResponse(
   207→            content=data["content"][0]["text"],
   208→            model=model,
   209→            usage={
   210→                "input_tokens": data["usage"]["input_tokens"],
   211→                "output_tokens": data["usage"]["output_tokens"]
   212→            },
   213→            finish_reason=data.get("stop_reason")
   214→        )
   215→
   216→    # ─────────────────────────────────────────────────────────────────
   217→    # Unified Interface
   218→    # ─────────────────────────────────────────────────────────────────
   219→
   220→    @classmethod
   221→    async def chat(
   222→        cls,
   223→        messages: List[LLMMessage],
   224→        provider: Optional[str] = None,
   225→        model: Optional[str] = None,
   226→        temperature: float = 0.7,
   227→        max_tokens: int = 2048,
   228→        **kwargs
   229→    ) -> LLMResponse:
   230→        """
   231→        Unified chat interface.
   232→        Automatically falls back to alternative provider on error.
   233→        """
   234→        provider = provider or cls.DEFAULT_PROVIDER
   235→
   236→        try:
   237→            if provider == "glm":
   238→                return await cls.chat_glm(
   239→                    messages, model, temperature, max_tokens, **kwargs
   240→                )
   241→            elif provider == "claude":
   242→                return await cls.chat_claude(
   243→                    messages, model, temperature, max_tokens, **kwargs
   244→                )
   245→            else:
   246→                raise ValueError(f"Unknown provider: {provider}")
   247→
   248→        except Exception as e:
   249→            # Fallback to alternative provider
   250→            if provider == "glm" and cls.CLAUDE_API_KEY:
   251→                print(f"GLM failed ({e}), falling back to Claude")
   252→                return await cls.chat_claude(
   253→                    messages, model, temperature, max_tokens, **kwargs
   254→                )
   255→            raise
   256→
   257→    @classmethod
   258→    async def stream(
   259→        cls,
   260→        messages: List[LLMMessage],
   261→        provider: Optional[str] = None,
   262→        model: Optional[str] = None,
   263→        temperature: float = 0.7,
   264→        max_tokens: int = 2048,
   265→        **kwargs
   266→    ) -> AsyncGenerator[str, None]:
   267→        """Unified streaming interface"""
   268→        provider = provider or cls.DEFAULT_PROVIDER
   269→
   270→        if provider == "glm":
   271→            async for chunk in cls.stream_glm(
   272→                messages, model, temperature, max_tokens, **kwargs
   273→            ):
   274→                yield chunk
   275→        else:
   276→            # Claude streaming would go here
   277→            response = await cls.chat_claude(
   278→                messages, model, temperature, max_tokens, **kwargs
   279→            )
   280→            yield response.content
   281→
   282→    # ─────────────────────────────────────────────────────────────────
   283→    # Vision Methods
   284→    # ─────────────────────────────────────────────────────────────────
   285→
   286→    @classmethod
   287→    async def vision(
   288→        cls,
   289→        image_base64: str,
   290→        prompt: str,
   291→        model: Optional[str] = None
   292→    ) -> LLMResponse:
   293→        """Analyze image with vision model"""
   294→        model = model or cls.GLM_VISION_MODEL
   295→
   296→        messages = [{
   297→            "role": "user",
   298→            "content": [
   299→                {"type": "text", "text": prompt},
   300→                {
   301→                    "type": "image_url",
   302→                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
   303→                }
   304→            ]
   305→        }]
   306→
   307→        payload = {
   308→            "model": model,
   309→            "messages": messages
   310→        }
   311→
   312→        headers = {
   313→            "Authorization": f"Bearer {cls.GLM_API_KEY}",
   314→            "Content-Type": "application/json"
   315→        }
   316→
   317→        async with httpx.AsyncClient(timeout=120.0) as client:
   318→            response = await client.post(
   319→                f"{cls.GLM_BASE_URL}/chat/completions",
   320→                json=payload,
   321→                headers=headers
   322→            )
   323→            response.raise_for_status()
   324→            data = response.json()
   325→
   326→        return LLMResponse(
   327→            content=data["choices"][0]["message"]["content"],
   328→            model=model,
   329→            usage=data.get("usage")
   330→        )
   331→
   332→    # ─────────────────────────────────────────────────────────────────
   333→    # Prompt Building Helpers
   334→    # ─────────────────────────────────────────────────────────────────
   335→
   336→    @staticmethod
   337→    def build_system_prompt(
   338→        persona: str,
   339→        skill_context: Optional[str] = None,
   340→        user_context: Optional[str] = None,
   341→        guidelines: Optional[str] = None
   342→    ) -> str:
   343→        """Build system prompt from components"""
   344→        parts = [persona]
   345→
   346→        if skill_context:
   347→            parts.append(f"\n\n## Skill Context\n{skill_context}")
   348→
   349→        if user_context:
   350→            parts.append(f"\n\n## User Context\n{user_context}")
   351→
   352→        if guidelines:
   353→            parts.append(f"\n\n## Response Guidelines\n{guidelines}")
   354→
   355→        return "\n".join(parts)
   356→
   357→    @staticmethod
   358→    def build_messages(
   359→        system_prompt: str,
   360→        history: List[Dict[str, str]],
   361→        user_message: str
   362→    ) -> List[LLMMessage]:
   363→        """Build message list from components"""
   364→        messages = [LLMMessage(role="system", content=system_prompt)]
   365→
   366→        for msg in history:
   367→            messages.append(LLMMessage(
   368→                role=msg["role"],
   369→                content=msg["content"]
   370→            ))
   371→
   372→        messages.append(LLMMessage(role="user", content=user_message))
   373→
   374→        return messages
   375→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
