     1→"""
     2→Chat Routes - Skill conversation endpoints
     3→"""
     4→from typing import Optional, List
     5→from uuid import UUID
     6→
     7→from fastapi import APIRouter, HTTPException, Depends
     8→from fastapi.responses import StreamingResponse
     9→from pydantic import BaseModel
    10→
    11→from services.identity import get_current_user, get_optional_user, CurrentUser
    12→from services.agent import AgentRuntime
    13→
    14→
    15→router = APIRouter(prefix="/chat", tags=["Chat"])
    16→
    17→
    18→# ─────────────────────────────────────────────────────────────────
    19→# Request/Response Models
    20→# ─────────────────────────────────────────────────────────────────
    21→
    22→class ChatRequest(BaseModel):
    23→    message: str
    24→    skill_id: str
    25→    conversation_id: Optional[UUID] = None
    26→
    27→
    28→class ChatResponse(BaseModel):
    29→    content: str
    30→    conversation_id: UUID
    31→    intent: Optional[str] = None
    32→    tools_used: Optional[List[str]] = None
    33→    knowledge_used: bool = False
    34→    insight: Optional[dict] = None
    35→    suggestions: Optional[List[str]] = None
    36→
    37→
    38→# ─────────────────────────────────────────────────────────────────
    39→# Endpoints
    40→# ─────────────────────────────────────────────────────────────────
    41→
    42→@router.post("/", response_model=ChatResponse)
    43→async def chat(
    44→    request: ChatRequest,
    45→    current_user: CurrentUser = Depends(get_current_user)
    46→):
    47→    """Send a message and get response"""
    48→    try:
    49→        response = await AgentRuntime.process_message(
    50→            user_id=current_user.user_id,
    51→            skill_id=request.skill_id,
    52→            message=request.message,
    53→            conversation_id=request.conversation_id
    54→        )
    55→
    56→        return ChatResponse(
    57→            content=response.content,
    58→            conversation_id=response.conversation_id,
    59→            intent=response.intent.value if response.intent else None,
    60→            tools_used=response.tools_used,
    61→            knowledge_used=response.knowledge_used,
    62→            insight=response.insight,
    63→            suggestions=response.suggestions
    64→        )
    65→    except Exception as e:
    66→        raise HTTPException(status_code=500, detail=str(e))
    67→
    68→
    69→@router.post("/stream")
    70→async def chat_stream(
    71→    request: ChatRequest,
    72→    current_user: CurrentUser = Depends(get_current_user)
    73→):
    74→    """Send a message and get streaming response (SSE)"""
    75→
    76→    async def generate():
    77→        try:
    78→            async for chunk in AgentRuntime.stream_message(
    79→                user_id=current_user.user_id,
    80→                skill_id=request.skill_id,
    81→                message=request.message,
    82→                conversation_id=request.conversation_id
    83→            ):
    84→                yield f"data: {chunk}\n\n"
    85→            yield "data: [DONE]\n\n"
    86→        except Exception as e:
    87→            yield f"data: [ERROR] {str(e)}\n\n"
    88→
    89→    return StreamingResponse(
    90→        generate(),
    91→        media_type="text/event-stream",
    92→        headers={
    93→            "Cache-Control": "no-cache",
    94→            "Connection": "keep-alive"
    95→        }
    96→    )
    97→
    98→
    99→@router.post("/guest")
   100→async def chat_guest(request: ChatRequest):
   101→    """
   102→    Guest chat (limited functionality, no persistence).
   103→    For landing page demo.
   104→    """
   105→    # For guest users, use a simple direct LLM call
   106→    from services.vibe_engine import LLMOrchestrator, LLMMessage
   107→    from services.agent import PersonaManager
   108→
   109→    persona = PersonaManager.get_persona(request.skill_id)
   110→
   111→    messages = [
   112→        LLMMessage(role="system", content=persona),
   113→        LLMMessage(role="user", content=request.message)
   114→    ]
   115→
   116→    try:
   117→        # Note: some GLM models may spend a large portion of tokens on reasoning
   118→        # before producing final `content`. Use a safer default budget for guest mode.
   119→        response = await LLMOrchestrator.chat(messages, max_tokens=1024)
   120→
   121→        return {
   122→            "content": response.content or "你好，我在这里。你想从哪里开始聊起？",
   123→            "is_guest": True,
   124→            "suggestion": "注册后可以获得完整的个性化分析"
   125→        }
   126→    except Exception as e:
   127→        raise HTTPException(status_code=500, detail=str(e))
   128→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
