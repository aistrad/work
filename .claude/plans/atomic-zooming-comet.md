# VibeLife Knowledge System Optimization Plan

## ç›®æ ‡
åŸºäº PostgreSQL + pgvectorï¼Œè®¾è®¡**æç®€é«˜æ•ˆ**çš„çŸ¥è¯†åº“ç³»ç»Ÿï¼š
- æ”¯æŒå¤šæ ¼å¼æ–‡æ¡£ï¼šPDF, MD, TXT, EPUB, DOCX, HTML
- æ¯ä¸ª skill module æŒ‡å®šçŸ¥è¯†æºæ–‡ä»¶å¤¹ï¼Œæ¯æ—¥å‡Œæ™¨ 4:00 è‡ªåŠ¨åŒæ­¥
- ä¸­è‹±æ–‡æ··åˆæŸ¥è¯¢ä¼˜åŒ– (70% ä¸­æ–‡ / 30% è‹±æ–‡)
- æç®€æ¶æ„ï¼šæ—  Redisã€Celeryã€ESï¼Œçº¯ PostgreSQL Native

---

## ç”¨æˆ·ç¡®è®¤çš„è®¾è®¡å†³ç­–

| å†³ç­–é¡¹ | é€‰æ‹© |
|--------|------|
| å‘é‡ç»´åº¦ | **ä¿æŒ Gemini 3072ç»´** |
| æ–‡ä»¶åˆ é™¤ç­–ç•¥ | **è½¯åˆ é™¤/å½’æ¡£** (æ ‡è®° archived, ä¿ç•™æ•°æ®) |
| QAç”Ÿæˆ | **ä¸éœ€è¦** (å½“å‰é˜¶æ®µä¸è‡ªåŠ¨ç”Ÿæˆ QA å¯¹) |
| æ‰«æé¢‘ç‡ | **æ¯æ—¥å‡Œæ™¨ 4:00** |
| User Insight | **ä¸åœ¨æœ¬è®¡åˆ’èŒƒå›´** (ç‹¬ç«‹æ¨¡å—) |
| Source-Insight | **ä¸å¼•å…¥** (ä¿æŒæç®€) |

---

## è®¾è®¡å†³ç­–è¯¦è§£

### 1. æ–‡ä»¶ç›®å½•ç®€åŒ–
æ¯ä¸ª skill ä¸€ä¸ªçŸ¥è¯†æºæ–‡ä»¶å¤¹ï¼š

```
/home/aiscend/work/vibelife/knowledge/
â”œâ”€â”€ bazi/           # å…«å­—æŠ€èƒ½çŸ¥è¯†åº“
â”‚   â”œâ”€â”€ å¤©å¹²åœ°æ”¯.md
â”‚   â”œâ”€â”€ åç¥è¯¦è§£.pdf
â”‚   â”œâ”€â”€ å…«å­—å…¥é—¨.epub
â”‚   â””â”€â”€ æ¡ˆä¾‹åˆ†æ.docx
â”œâ”€â”€ zodiac/         # æ˜Ÿåº§æŠ€èƒ½çŸ¥è¯†åº“
â””â”€â”€ mbti/           # MBTIæŠ€èƒ½çŸ¥è¯†åº“
```

### 2. å¤šæ ¼å¼æ”¯æŒ

| æ ¼å¼ | è½¬æ¢å·¥å…· | è¯´æ˜ |
|------|---------|------|
| PDF | pymupdf4llm | ä¿ç•™è¡¨æ ¼ç»“æ„ |
| EPUB | ebooklib + BeautifulSoup | ç”µå­ä¹¦ |
| DOCX | python-docx | Word æ–‡æ¡£ |
| HTML | html2text | ç½‘é¡µ/æŠ“å–å†…å®¹ |
| MD/TXT | ç›´æ¥è¯»å– | åŸç”Ÿæ”¯æŒ |

### 3. ä¸­è‹±æ–‡æ··åˆæœç´¢
åº”ç”¨å±‚ Jieba åˆ†è¯ + PG `simple` é…ç½®ï¼š
```
è¾“å…¥: "æ¯”è‚©ä»£è¡¨ç‹¬ç«‹èƒ½åŠ›strong independence"
åˆ†è¯: "æ¯”è‚© ä»£è¡¨ ç‹¬ç«‹ èƒ½åŠ› strong independence"
å­˜å‚¨: search_text_preprocessed åˆ—
ç´¢å¼•: to_tsvector('simple', ...) è‡ªåŠ¨ç”Ÿæˆ
```

### 4. è½¯åˆ é™¤ç­–ç•¥
æ–‡ä»¶è¢«åˆ é™¤æ—¶ï¼š
- `status = 'archived'`
- ä¿ç•™æ•°æ®ç”¨äºå®¡è®¡/å›æ»š
- ä¸å‚ä¸æ£€ç´¢

---

## åˆ†å—ç­–ç•¥ (æ ¸å¿ƒè®¾è®¡)

### è®¾è®¡åŸåˆ™

```
å…«å­—/æ˜Ÿåº§/MBTI çŸ¥è¯†ç‰¹ç‚¹:
â”œâ”€â”€ å±‚æ¬¡åˆ†æ˜: å¤©å¹² â†’ åç¥ â†’ æ ¼å±€ â†’ è¿åŠ¿
â”œâ”€â”€ è¡¨æ ¼å¯†é›†: äº”è¡Œå¯¹ç…§è¡¨ã€å¤©å¹²åœ°æ”¯è¡¨
â”œâ”€â”€ æœ¯è¯­å…³è”: "æ¯”è‚©" éœ€è¦è”ç³» "æ—¥ä¸»"ã€"äº”è¡Œ"
â””â”€â”€ ç»“æ„å›ºå®š: 12æ˜Ÿåº§ Ã— Nç»´åº¦, 16ç±»å‹ Ã— Mç‰¹å¾

åˆ†å—åŸåˆ™:
1. è¯­ä¹‰å®Œæ•´æ€§ > é•¿åº¦å‡åŒ€æ€§
2. è¡¨æ ¼/åˆ—è¡¨ä½œä¸ºå®Œæ•´å•å…ƒï¼Œä¸æ‹†åˆ†
3. ä¿ç•™å±‚çº§è·¯å¾„ä¿¡æ¯ (å¦‚ "åç¥ > æ¯”è‚© > ç‰¹ç‚¹")
```

### åˆ†å—å‚æ•°

```python
CHUNK_SIZE = 600          # ç›®æ ‡å—å¤§å° (ä¸­æ–‡å­—ç¬¦)
CHUNK_OVERLAP = 80        # é‡å åŒºåŸŸ (ä¿æŒä¸Šä¸‹æ–‡)
MIN_CHUNK_SIZE = 100      # æœ€å°å— (è¿‡å°åˆ™å‘ä¸Šåˆå¹¶)
MAX_CHUNK_SIZE = 1200     # æœ€å¤§å— (å…è®¸è¡¨æ ¼è¶…é•¿)
```

### ä¸‰é˜¶æ®µåˆ†å—æµç¨‹

```
é˜¶æ®µ 1: ç»“æ„è§£æ (æŒ‰ Markdown æ ‡é¢˜)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åç¥è¯¦è§£
## æ¯”è‚©
### æ¯”è‚©çš„ç‰¹ç‚¹
...

â†’ è§£æä¸ºå±‚çº§ç»“æ„ï¼Œä¿ç•™ section_path

é˜¶æ®µ 2: æ™ºèƒ½åˆå¹¶ (è¿‡å°å—å‘ä¸Šåˆå¹¶)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
### æ¯”è‚©çš„ç‰¹ç‚¹ (50å­—)  â”€â”€â”
### æ¯”è‚©è¿‡æ—º (80å­—)    â”€â”€â”¼â†’ åˆå¹¶ä¸º "æ¯”è‚©" å—
### æ¯”è‚©è¿‡å¼± (60å­—)    â”€â”€â”˜

é˜¶æ®µ 3: å®‰å…¨æ‹†åˆ† (è¿‡é•¿å—åœ¨å¥å­è¾¹ç•Œæ‹†åˆ†)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
## æ ¼å±€åˆ†æ (2000å­—)
â†’ æ‹†åˆ†ä¸º 3 ä¸ªå—ï¼Œä¿ç•™ 80 å­—é‡å 
```

### ç‰¹æ®Šå†…å®¹å¤„ç†

```
è¡¨æ ¼: æ°¸ä¸æ‹†åˆ†ï¼Œå³ä½¿è¶…è¿‡ MAX_CHUNK_SIZE
åˆ—è¡¨: < 800å­—ä¿æŒå®Œæ•´ï¼Œå¦åˆ™æŒ‰é¡¶çº§é¡¹æ‹†åˆ†
ä»£ç å—: ä¿æŒå®Œæ•´
```

---

## æ•°æ®åº“ Schema è®¾è®¡

```sql
-- ===============================================
-- æ–‡ä»¶: migrations/002_knowledge_v2.sql
-- ===============================================

-- æ–‡æ¡£è¡¨ (å…¼ä½œä»»åŠ¡é˜Ÿåˆ—)
CREATE TABLE IF NOT EXISTS knowledge_documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    skill_id TEXT NOT NULL,
    filename TEXT NOT NULL,
    file_path TEXT NOT NULL,         -- å®Œæ•´æ–‡ä»¶è·¯å¾„
    file_hash TEXT NOT NULL,         -- MD5 ç”¨äºæ£€æµ‹å˜æ›´
    file_type TEXT NOT NULL,         -- pdf, md, txt, epub, docx, html
    content_md TEXT,                 -- è½¬æ¢åçš„ Markdown (å½’æ¡£)
    status TEXT DEFAULT 'pending',   -- pending/processing/completed/failed/archived
    error_message TEXT,
    chunk_count INT DEFAULT 0,       -- ç”Ÿæˆçš„çŸ¥è¯†å—æ•°é‡
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(skill_id, filename)
);

-- æ–°ç‰ˆçŸ¥è¯†å—è¡¨
CREATE TABLE IF NOT EXISTS knowledge_chunks_v2 (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID REFERENCES knowledge_documents(id) ON DELETE SET NULL,
    skill_id TEXT NOT NULL,
    chunk_index INT NOT NULL DEFAULT 0,
    content TEXT NOT NULL,
    content_type TEXT DEFAULT 'knowledge',  -- knowledge, theory, pattern, case, example

    -- å±‚çº§è·¯å¾„ä¿¡æ¯ (å¦‚ "åç¥è¯¦è§£ > æ¯”è‚© > ç‰¹ç‚¹")
    section_path TEXT[],             -- ARRAY['åç¥è¯¦è§£', 'æ¯”è‚©', 'ç‰¹ç‚¹']
    section_title TEXT,              -- å½“å‰èŠ‚æ ‡é¢˜

    -- å…ƒæ•°æ®
    metadata JSONB DEFAULT '{}',
    has_table BOOLEAN DEFAULT FALSE,
    has_list BOOLEAN DEFAULT FALSE,

    -- Jieba åˆ†è¯åå­˜å…¥ (ä¸­è‹±æ–‡æ··åˆ)
    search_text_preprocessed TEXT,

    -- è‡ªåŠ¨ç”Ÿæˆå…¨æ–‡ç´¢å¼• (GENERATED ALWAYS)
    search_vector tsvector GENERATED ALWAYS AS (
        to_tsvector('simple', COALESCE(search_text_preprocessed, ''))
    ) STORED,

    -- Gemini 3072ç»´å‘é‡
    embedding vector(3072),

    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_docs_status ON knowledge_documents(status);
CREATE INDEX IF NOT EXISTS idx_docs_skill ON knowledge_documents(skill_id);
CREATE INDEX IF NOT EXISTS idx_chunks_v2_skill ON knowledge_chunks_v2(skill_id);
CREATE INDEX IF NOT EXISTS idx_chunks_v2_doc ON knowledge_chunks_v2(document_id);
CREATE INDEX IF NOT EXISTS idx_chunks_v2_type ON knowledge_chunks_v2(content_type);
CREATE INDEX IF NOT EXISTS idx_chunks_v2_fts ON knowledge_chunks_v2 USING gin(search_vector);
CREATE INDEX IF NOT EXISTS idx_chunks_v2_vec ON knowledge_chunks_v2 USING hnsw(embedding vector_cosine_ops);

-- è¿ç§»æ—§ knowledge_chunks æ•°æ® (ä¿ç•™ç°æœ‰æ•°æ®)
INSERT INTO knowledge_chunks_v2 (skill_id, content, content_type, section_title, metadata, embedding, created_at)
SELECT
    skill_id,
    content,
    COALESCE(content_type, 'knowledge'),
    source_section,
    metadata,
    embedding,
    created_at
FROM knowledge_chunks
WHERE embedding IS NOT NULL
ON CONFLICT DO NOTHING;
```

---

## æ··åˆæœç´¢ SQL å‡½æ•° (RRF ä¸‹æ²‰åˆ°æ•°æ®åº“)

```sql
-- ===============================================
-- æ–‡ä»¶: migrations/002_knowledge_v2.sql (ç»­)
-- ===============================================

CREATE OR REPLACE FUNCTION hybrid_search_v2(
    query_text_processed TEXT,    -- Jieba åˆ†è¯åçš„æŸ¥è¯¢
    query_embedding vector(3072),
    match_skill_id TEXT,
    top_k INT DEFAULT 5,
    rrf_k INT DEFAULT 60
)
RETURNS TABLE (
    id UUID,
    content TEXT,
    content_type TEXT,
    source_section TEXT,
    score FLOAT,
    match_type TEXT
) LANGUAGE sql STABLE AS $$
WITH semantic AS (
    SELECT id, ROW_NUMBER() OVER (ORDER BY embedding <=> query_embedding) as rank
    FROM knowledge_chunks_v2
    WHERE skill_id = match_skill_id AND embedding IS NOT NULL
    ORDER BY embedding <=> query_embedding
    LIMIT top_k * 2
),
keyword AS (
    SELECT id, ROW_NUMBER() OVER (
        ORDER BY ts_rank(search_vector, plainto_tsquery('simple', query_text_processed)) DESC
    ) as rank
    FROM knowledge_chunks_v2
    WHERE skill_id = match_skill_id
      AND search_vector @@ plainto_tsquery('simple', query_text_processed)
    LIMIT top_k * 2
)
SELECT
    COALESCE(s.id, k.id) as id,
    c.content,
    c.content_type,
    c.source_section,
    (COALESCE(1.0/(rrf_k + s.rank), 0.0) * 0.7 +
     COALESCE(1.0/(rrf_k + k.rank), 0.0) * 0.3)::FLOAT as score,
    CASE
        WHEN s.id IS NOT NULL AND k.id IS NOT NULL THEN 'hybrid'
        WHEN s.id IS NOT NULL THEN 'vector'
        ELSE 'keyword'
    END as match_type
FROM semantic s
FULL OUTER JOIN keyword k ON s.id = k.id
JOIN knowledge_chunks_v2 c ON c.id = COALESCE(s.id, k.id)
ORDER BY score DESC
LIMIT top_k;
$$;
```

---

## æ ¸å¿ƒæ–‡ä»¶å®ç°

### 1. å¤šæ ¼å¼è½¬æ¢å™¨

**æ–‡ä»¶**: `apps/api/workers/converters.py`

```python
"""
å¤šæ ¼å¼æ–‡æ¡£ â†’ Markdown è½¬æ¢å™¨
æ”¯æŒ: PDF, EPUB, DOCX, HTML, MD, TXT
"""
import pymupdf4llm
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
from docx import Document
import html2text

class DocumentConverter:
    """ç»Ÿä¸€æ–‡æ¡£è½¬æ¢æ¥å£"""

    @staticmethod
    async def to_markdown(file_path: str, file_type: str) -> str:
        """å°†ä»»æ„æ ¼å¼è½¬æ¢ä¸º Markdown"""

        if file_type == 'pdf':
            # PDF: ä¿ç•™è¡¨æ ¼ç»“æ„
            return pymupdf4llm.to_markdown(file_path, write_images=False)

        elif file_type == 'epub':
            # EPUB: æå–ç« èŠ‚æ–‡æœ¬
            book = epub.read_epub(file_path)
            chapters = []
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    chapters.append(soup.get_text())
            return "\n\n".join(chapters)

        elif file_type == 'docx':
            # DOCX: æå–æ®µè½
            doc = Document(file_path)
            paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
            return "\n\n".join(paragraphs)

        elif file_type == 'html':
            # HTML: è½¬ä¸º Markdown
            with open(file_path, 'r', encoding='utf-8') as f:
                h = html2text.HTML2Text()
                h.ignore_links = False
                return h.handle(f.read())

        else:  # md, txt
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
```

### 2. æ™ºèƒ½åˆ†å—å™¨

**æ–‡ä»¶**: `apps/api/workers/chunker.py`

```python
"""
VibeLife ä¸“ç”¨åˆ†å—å™¨
ç‰¹ç‚¹: ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼Œè¡¨æ ¼ä¸æ‹†åˆ†ï¼Œä¿ç•™å±‚çº§è·¯å¾„
"""
import re
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class Chunk:
    content: str
    section_path: List[str]
    section_title: str
    chunk_index: int
    has_table: bool
    has_list: bool

class VibeLifeChunker:
    CHUNK_SIZE = 600
    CHUNK_OVERLAP = 80
    MIN_CHUNK_SIZE = 100
    MAX_CHUNK_SIZE = 1200

    def chunk(self, markdown: str) -> List[Chunk]:
        """ä¸‰é˜¶æ®µåˆ†å—"""
        # é˜¶æ®µ1: è§£æç»“æ„
        sections = self._parse_structure(markdown)

        # é˜¶æ®µ2: æ™ºèƒ½åˆå¹¶è¿‡å°å—
        merged = self._merge_small_sections(sections)

        # é˜¶æ®µ3: æ‹†åˆ†è¿‡é•¿å—
        chunks = self._split_large_sections(merged)

        return chunks

    def _parse_structure(self, markdown: str) -> List[dict]:
        """æŒ‰ Markdown æ ‡é¢˜è§£æå±‚çº§ç»“æ„"""
        sections = []
        current_path = []
        current_content = []
        current_level = 0

        for line in markdown.split('\n'):
            header_match = re.match(r'^(#{1,4})\s+(.+)$', line)

            if header_match:
                # ä¿å­˜ä¹‹å‰çš„å†…å®¹
                if current_content:
                    sections.append({
                        'path': current_path.copy(),
                        'title': current_path[-1] if current_path else '',
                        'content': '\n'.join(current_content),
                        'level': current_level
                    })
                    current_content = []

                # æ›´æ–°è·¯å¾„
                level = len(header_match.group(1))
                title = header_match.group(2).strip()

                if level > current_level:
                    current_path.append(title)
                elif level == current_level:
                    if current_path:
                        current_path[-1] = title
                    else:
                        current_path.append(title)
                else:
                    current_path = current_path[:level-1] + [title]

                current_level = level
            else:
                current_content.append(line)

        # ä¿å­˜æœ€åä¸€å—
        if current_content:
            sections.append({
                'path': current_path.copy(),
                'title': current_path[-1] if current_path else '',
                'content': '\n'.join(current_content),
                'level': current_level
            })

        return sections

    def _merge_small_sections(self, sections: List[dict]) -> List[dict]:
        """åˆå¹¶è¿‡å°çš„å—åˆ°çˆ¶çº§"""
        merged = []
        buffer = None

        for section in sections:
            content_len = len(section['content'])

            if content_len < self.MIN_CHUNK_SIZE:
                if buffer:
                    buffer['content'] += '\n\n' + section['content']
                else:
                    buffer = section.copy()
            else:
                if buffer:
                    merged.append(buffer)
                    buffer = None
                merged.append(section)

        if buffer:
            merged.append(buffer)

        return merged

    def _split_large_sections(self, sections: List[dict]) -> List[Chunk]:
        """æ‹†åˆ†è¿‡é•¿çš„å—"""
        chunks = []
        chunk_index = 0

        for section in sections:
            content = section['content']
            has_table = '|' in content and '---' in content
            has_list = bool(re.search(r'^[\-\*\d]\s', content, re.MULTILINE))

            # è¡¨æ ¼ä¸æ‹†åˆ†
            if has_table or len(content) <= self.MAX_CHUNK_SIZE:
                chunks.append(Chunk(
                    content=content.strip(),
                    section_path=section['path'],
                    section_title=section['title'],
                    chunk_index=chunk_index,
                    has_table=has_table,
                    has_list=has_list
                ))
                chunk_index += 1
            else:
                # åœ¨å¥å­è¾¹ç•Œæ‹†åˆ†
                sub_chunks = self._split_at_sentences(content)
                for sub in sub_chunks:
                    chunks.append(Chunk(
                        content=sub.strip(),
                        section_path=section['path'],
                        section_title=section['title'],
                        chunk_index=chunk_index,
                        has_table=False,
                        has_list=has_list
                    ))
                    chunk_index += 1

        return chunks

    def _split_at_sentences(self, text: str) -> List[str]:
        """åœ¨å¥å­è¾¹ç•Œæ‹†åˆ†é•¿æ–‡æœ¬"""
        sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s*', text)
        chunks = []
        current = ""

        for sentence in sentences:
            if len(current) + len(sentence) > self.CHUNK_SIZE:
                if current:
                    chunks.append(current)
                current = sentence
            else:
                current += sentence

        if current:
            chunks.append(current)

        return chunks
```

### 3. åå° Worker (æ›¿ä»£ Celery)

**æ–‡ä»¶**: `apps/api/workers/ingestion.py`

```python
"""
Knowledge Ingestion Worker - åŸºäº PG SKIP LOCKED çš„ä»»åŠ¡é˜Ÿåˆ—
"""
import asyncio
import jieba
from .converters import DocumentConverter
from .chunker import VibeLifeChunker

class IngestionWorker:
    def __init__(self, pool, embedding_service):
        self.pool = pool
        self.embedding_service = embedding_service
        self.converter = DocumentConverter()
        self.chunker = VibeLifeChunker()

    async def run_loop(self):
        """åå°å¾ªç¯ï¼šè½®è¯¢ DB é˜Ÿåˆ—ï¼ŒåŸå­æŠ¢å ä»»åŠ¡"""
        print("ğŸš€ Knowledge Ingestion Worker Started...")
        while True:
            try:
                async with self.pool.acquire() as conn:
                    task = await conn.fetchrow("""
                        UPDATE knowledge_documents
                        SET status = 'processing', updated_at = NOW()
                        WHERE id = (
                            SELECT id FROM knowledge_documents
                            WHERE status = 'pending'
                            ORDER BY created_at ASC
                            LIMIT 1
                            FOR UPDATE SKIP LOCKED
                        )
                        RETURNING id, file_path, file_type, skill_id
                    """)

                if not task:
                    await asyncio.sleep(2)
                    continue

                await self.process_document(task)

            except Exception as e:
                print(f"Worker Error: {e}")
                await asyncio.sleep(5)

    async def process_document(self, task):
        """æ ¸å¿ƒå¤„ç†æµç¨‹"""
        try:
            # A. å¤šæ ¼å¼è½¬ Markdown
            md_content = await self.converter.to_markdown(
                task['file_path'], task['file_type']
            )

            # B. æ™ºèƒ½åˆ†å—
            chunks = self.chunker.chunk(md_content)

            # C. æ‰¹é‡å‘é‡åŒ–
            chunk_texts = [c.content for c in chunks]
            embeddings = await self.embedding_service.embed_batch(chunk_texts)

            # D. Jieba åˆ†è¯ + å…¥åº“
            insert_data = []
            for chunk, vec in zip(chunks, embeddings):
                seg_text = " ".join(jieba.cut_for_search(chunk.content))
                insert_data.append((
                    task['id'],
                    task['skill_id'],
                    chunk.chunk_index,
                    chunk.content,
                    chunk.section_path,
                    chunk.section_title,
                    chunk.has_table,
                    chunk.has_list,
                    seg_text,
                    str(vec)
                ))

            async with self.pool.acquire() as conn:
                async with conn.transaction():
                    await conn.executemany("""
                        INSERT INTO knowledge_chunks_v2
                        (document_id, skill_id, chunk_index, content,
                         section_path, section_title, has_table, has_list,
                         search_text_preprocessed, embedding)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                    """, insert_data)

                    await conn.execute("""
                        UPDATE knowledge_documents
                        SET status = 'completed', content_md = $1,
                            chunk_count = $2, updated_at = NOW()
                        WHERE id = $3
                    """, md_content, len(chunks), task['id'])

            print(f"âœ… Processed: {task['file_path']} â†’ {len(chunks)} chunks")

        except Exception as e:
            async with self.pool.acquire() as conn:
                await conn.execute("""
                    UPDATE knowledge_documents
                    SET status = 'failed', error_message = $1, updated_at = NOW()
                    WHERE id = $2
                """, str(e), task['id'])
            print(f"âŒ Failed: {task['file_path']} - {e}")
```

### 4. æ–‡ä»¶å¤¹åŒæ­¥è„šæœ¬

**æ–‡ä»¶**: `apps/api/scripts/sync_knowledge.py`

```python
"""
Knowledge Folder Sync - æ¯æ—¥å‡Œæ™¨ 4:00 cron è§¦å‘
Usage: python scripts/sync_knowledge.py
"""
import asyncio
import hashlib
from pathlib import Path

SKILL_KNOWLEDGE_PATHS = {
    "bazi": "/home/aiscend/work/vibelife/knowledge/bazi",
    "zodiac": "/home/aiscend/work/vibelife/knowledge/zodiac",
    "mbti": "/home/aiscend/work/vibelife/knowledge/mbti"
}

# æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
SUPPORTED_EXTENSIONS = {'.pdf', '.md', '.txt', '.epub', '.docx', '.html'}

def compute_file_hash(filepath: str) -> str:
    with open(filepath, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()

async def sync_skill_folder(skill_id: str, folder_path: str, conn):
    """åŒæ­¥å•ä¸ªæŠ€èƒ½çš„çŸ¥è¯†æ–‡ä»¶å¤¹"""
    path = Path(folder_path)
    if not path.exists():
        print(f"âš ï¸  Folder not found: {folder_path}")
        return

    # 1. æ‰«ææ–‡ä»¶ç³»ç»Ÿ
    current_files = {}
    for f in path.iterdir():
        if f.suffix.lower() in SUPPORTED_EXTENSIONS:
            current_files[f.name] = {
                'path': str(f),
                'hash': compute_file_hash(str(f)),
                'type': f.suffix[1:].lower()
            }

    # 2. æŸ¥è¯¢æ•°æ®åº“ç°æœ‰è®°å½•
    db_records = await conn.fetch("""
        SELECT filename, file_hash, status FROM knowledge_documents
        WHERE skill_id = $1 AND status != 'archived'
    """, skill_id)
    db_files = {r['filename']: r for r in db_records}

    # 3. æ–°å¢æ–‡ä»¶
    for filename, info in current_files.items():
        if filename not in db_files:
            await conn.execute("""
                INSERT INTO knowledge_documents
                (skill_id, filename, file_path, file_hash, file_type, status)
                VALUES ($1, $2, $3, $4, $5, 'pending')
            """, skill_id, filename, info['path'], info['hash'], info['type'])
            print(f"âœ… New: {filename}")

    # 4. ä¿®æ”¹æ–‡ä»¶ (hash å˜åŒ–)
    for filename, info in current_files.items():
        if filename in db_files and db_files[filename]['file_hash'] != info['hash']:
            await conn.execute("""
                DELETE FROM knowledge_chunks_v2
                WHERE document_id = (SELECT id FROM knowledge_documents WHERE skill_id = $1 AND filename = $2)
            """, skill_id, filename)
            await conn.execute("""
                UPDATE knowledge_documents
                SET file_hash = $3, status = 'pending', updated_at = NOW()
                WHERE skill_id = $1 AND filename = $2
            """, skill_id, filename, info['hash'])
            print(f"ğŸ”„ Modified: {filename}")

    # 5. åˆ é™¤æ–‡ä»¶ â†’ è½¯åˆ é™¤/å½’æ¡£
    for filename, record in db_files.items():
        if filename not in current_files and record['status'] != 'archived':
            await conn.execute("""
                UPDATE knowledge_documents
                SET status = 'archived', updated_at = NOW()
                WHERE skill_id = $1 AND filename = $2
            """, skill_id, filename)
            print(f"ğŸ“¦ Archived: {filename}")

async def main():
    from stores.db import get_db_pool, close_db_pool
    pool = await get_db_pool()
    async with pool.acquire() as conn:
        for skill_id, folder in SKILL_KNOWLEDGE_PATHS.items():
            print(f"\nğŸ“‚ Syncing {skill_id}: {folder}")
            await sync_skill_folder(skill_id, folder, conn)
    await close_db_pool()
    print("\nâœ… Sync complete!")

if __name__ == "__main__":
    asyncio.run(main())
```

### 3. æ›´æ–° RetrievalService

**æ–‡ä»¶**: `apps/api/services/knowledge/retrieval.py` (ä¿®æ”¹)

```python
# ç§»é™¤ Python å±‚ RRFï¼Œæ”¹ç”¨ SQL å‡½æ•°
class RetrievalService:
    @classmethod
    async def search(cls, query: str, skill_id: str, top_k: int = 5) -> List[Dict]:
        # 1. Jieba åˆ†è¯
        query_seg = " ".join(jieba.cut(query))

        # 2. ç”Ÿæˆ query embedding
        query_embedding = await EmbeddingService.embed_query(query)

        # 3. è°ƒç”¨ SQL å‡½æ•° (RRF åœ¨æ•°æ®åº“å®Œæˆ)
        async with get_connection() as conn:
            results = await conn.fetch("""
                SELECT * FROM hybrid_search_v2($1, $2, $3, $4)
            """, query_seg, str(query_embedding), skill_id, top_k)

        return [dict(r) for r in results]
```

---

## Crontab é…ç½®

```cron
# æ–‡ä»¶: apps/api/scripts/crontab.example (è¿½åŠ )

# çŸ¥è¯†åº“æ–‡ä»¶å¤¹åŒæ­¥ - æ¯æ—¥å‡Œæ™¨ 4:00
0 4 * * * cd /home/aiscend/work/vibelife/apps/api && python3 scripts/sync_knowledge.py >> /var/log/vibelife/knowledge_sync.log 2>&1
```

---

## æ–°å¢ä¾èµ–

**æ–‡ä»¶**: `apps/api/requirements.txt` (è¿½åŠ )

```
# ä¸­æ–‡åˆ†è¯
jieba>=0.42.1

# å¤šæ ¼å¼è½¬æ¢
pymupdf4llm>=0.0.17        # PDF â†’ Markdown (ä¿ç•™è¡¨æ ¼)
ebooklib>=0.18             # EPUB ç”µå­ä¹¦
python-docx>=1.1.0         # DOCX Word æ–‡æ¡£
html2text>=2024.2.26       # HTML â†’ Markdown
beautifulsoup4>=4.12.0     # HTML è§£æ
```

---

## å®ç°ä»»åŠ¡æ¸…å• (æŒ‰é¡ºåº)

### Phase 1: æ•°æ®åº“è¿ç§»
- [ ] åˆ›å»º `migrations/002_knowledge_v2.sql`
- [ ] æ‰§è¡Œè¿ç§»ï¼šåˆ›å»º `knowledge_documents` è¡¨
- [ ] æ‰§è¡Œè¿ç§»ï¼šåˆ›å»º `knowledge_chunks_v2` è¡¨
- [ ] æ‰§è¡Œè¿ç§»ï¼šè¿ç§»æ—§ chunks æ•°æ®
- [ ] æ‰§è¡Œè¿ç§»ï¼šåˆ›å»º `hybrid_search_v2` SQL å‡½æ•°
- [ ] éªŒè¯ HNSW å‘é‡ç´¢å¼•ç”Ÿæ•ˆ

### Phase 2: Worker æ¨¡å—
- [ ] å®‰è£…ä¾èµ–ï¼š`jieba`, `pymupdf4llm`, `ebooklib`, `python-docx`, `html2text`
- [ ] åˆ›å»º `apps/api/workers/__init__.py`
- [ ] åˆ›å»º `apps/api/workers/converters.py` (å¤šæ ¼å¼è½¬æ¢å™¨)
- [ ] åˆ›å»º `apps/api/workers/chunker.py` (æ™ºèƒ½åˆ†å—å™¨)
- [ ] åˆ›å»º `apps/api/workers/ingestion.py` (åå° Worker)
- [ ] åœ¨ `main.py` lifespan ä¸­å¯åŠ¨ worker

### Phase 3: æ–‡ä»¶å¤¹åŒæ­¥
- [ ] åˆ›å»º `knowledge/bazi/`, `knowledge/zodiac/`, `knowledge/mbti/` ç›®å½•
- [ ] åˆ›å»º `scripts/sync_knowledge.py`
- [ ] æ·»åŠ åˆ° crontab (æ¯æ—¥ 4:00)

### Phase 4: æœåŠ¡é›†æˆ
- [ ] ä¿®æ”¹ `services/knowledge/retrieval.py` ä½¿ç”¨ SQL å‡½æ•°
- [ ] æ·»åŠ  jieba åˆ†è¯åˆ°æŸ¥è¯¢æµç¨‹
- [ ] ç§»é™¤ Python å±‚ RRF é€»è¾‘
- [ ] æ›´æ–° `knowledge_repo.py` é€‚é…æ–°è¡¨

### Phase 5: æµ‹è¯•éªŒè¯
- [ ] ä¸Šä¼ æµ‹è¯•æ–‡æ¡£ (PDF, EPUB, DOCX)
- [ ] éªŒè¯åˆ†å—è´¨é‡
- [ ] éªŒè¯æ£€ç´¢æ•ˆæœ

---

## å¾…ä¿®æ”¹æ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | æ“ä½œ |
|------|------|
| `migrations/002_knowledge_v2.sql` | **æ–°å»º** |
| `apps/api/workers/__init__.py` | **æ–°å»º** |
| `apps/api/workers/converters.py` | **æ–°å»º** |
| `apps/api/workers/chunker.py` | **æ–°å»º** |
| `apps/api/workers/ingestion.py` | **æ–°å»º** |
| `apps/api/scripts/sync_knowledge.py` | **æ–°å»º** |
| `apps/api/requirements.txt` | ä¿®æ”¹ (è¿½åŠ ä¾èµ–) |
| `apps/api/main.py` | ä¿®æ”¹ (å¯åŠ¨ worker) |
| `apps/api/services/knowledge/retrieval.py` | ä¿®æ”¹ (ä½¿ç”¨ SQL å‡½æ•°) |
| `apps/api/stores/knowledge_repo.py` | ä¿®æ”¹ (é€‚é…æ–°è¡¨) |
| `apps/api/scripts/crontab.example` | ä¿®æ”¹ (è¿½åŠ  cron)
