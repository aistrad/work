# VibeLife V6 Architecture

> Version: 4.1 (v6.1) | 2026-01-14
> 详细文档: [docs/archive/v6/VibeLife-Expert-System-v6.md](./VibeLife-Expert-System-v6.md)

**v6.1 变更 (2026-01-14)**:
- Stage 4 改为统一抽取 + LLM 自动审核 (`unified_extractor.py`)
- Case 自动入库: `reasoning_chain` + `guidance_patterns`, score >= 0.6
- Scenario 自动发布: quality >= 0.6 + similarity < 0.5
- 废弃 `scenario_candidates` 和 `scenario_index` 表
- 删除 `scenario_generator.py` 和 `scenario_reviewer.py`

## 项目概述

VibeLife 是一个 AI 驱动的命理咨询平台，支持八字、星座、塔罗、职业咨询等多种技能。

## 技术架构

```
┌─────────────────────────────────────────────────────────────────┐
│           Python CoreAgent 主导 + Next.js 代理 + AI SDK 6        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Browser (AI SDK 6)                                             │
│  ├─ useVibeChat + DefaultChatTransport                          │
│  └─ 请求 /api/v1/* (相对路径)                                    │
│                                                                 │
│  Next.js (香港火山云)                                            │
│  ├─ rewrites 代理: /api/v1/* → Python API                       │
│  └─ 环境变量: VIBELIFE_API_INTERNAL                              │
│                                                                 │
│  Python Backend (本机 aiscend/vibelife)                          │
│  ├─ /chat/v5/stream         → CoreAgent 主入口                  │
│  │      ├─ LLM 智能选择 Skill (无关键词匹配)                     │
│  │      ├─ Tool calling + 执行                                  │
│  │      └─ AI SDK 6 Data Stream Protocol SSE                    │
│  ├─ /bazi/chart             → 八字命盘 API                      │
│  ├─ /zodiac/chart           → 星盘 API                          │
│  ├─ /fortune/*              → 运势数据 API                      │
│  └─ /users/me/profile       → 用户画像                          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

部署环境:
- 测试环境 (aiscend): Python API 端口 8100
- 生产环境 (vibelife): Python API 端口 8000
```

## 核心设计原则

### Anthropic Agent 设计原则

1. **Agentic Loop**: "LLMs using tools based on environmental feedback in a loop"
2. **不要用代码硬编码决策逻辑，让 LLM 自己判断**
3. **Tool Use 最佳实践 (ACI)**:
   - 站在模型角度思考工具是否易用
   - 工具定义应包含示例用法、边界情况
   - 参数命名要清晰

### 三大原则

| 原则 | 说明 |
|------|------|
| **Simplicity** | 保持简单，避免过度工程 |
| **Transparency** | 显式展示 agent 规划步骤 |
| **Documentation & Testing** | 精心设计工具文档，充分测试 |

## 关键文件

```
apps/api/
├── routes/chat_v5.py          # V5 Chat 端点 (CoreAgent)
├── services/agent/core.py     # CoreAgent 实现
├── services/llm/client.py     # 统一 LLM 客户端
└── skills/                    # Skill 定义

apps/web/
├── src/hooks/useVibeChat.ts   # AI SDK 6 集成 (使用相对路径)
├── src/components/chat/       # Chat UI 组件
└── next.config.js             # rewrites 代理配置
```

## LLM 调用规范

**唯一配置源**: `config/models.yaml`

```
config/models.yaml                    ← 唯一配置源 (Single Source of Truth)
        │
        ├─ services/llm/config.py     ← LLMConfig 配置加载模块
        │
        └─ services/llm/client.py     ← LLMClient 统一调用
```

**使用方法**:
```python
from services.llm.config import LLMConfig

# 获取 provider 配置
provider = LLMConfig.get_provider("deepseek")
print(provider.base_url)       # https://api.deepseek.com/anthropic
print(provider.api_key)        # 从环境变量读取
print(provider.is_anthropic()) # True

# 解析模型路由 (含 fallback 链)
selection = LLMConfig.resolve(user_tier="paid", task="chat")
print(selection.model)         # deepseek-chat
print(selection.fallback_chain) # ['glm-4.7', 'claude-opus', 'glm-4-flash']
```

**验证配置**:
```bash
python scripts/validate_llm_config.py
```

**禁止**:
- 前端硬编码模型名称或 API 地址
- 在代码中硬编码 tier 名称或限制值
- 在 Python 代码中硬编码 base_url 默认值（应从 models.yaml 读取）

## Anthropic 兼容 API 配置

DeepSeek 和 GLM 都提供了 Anthropic 兼容的 API 端点，可以复用 Claude SDK 的消息格式和工具调用逻辑。

### DeepSeek V3

**官方文档**: https://api-docs.deepseek.com/guides/anthropic_api

**端点配置**:
```python
base_url = "https://api.deepseek.com/anthropic"
model = "deepseek-chat"  # 或 deepseek-reasoner
```

**环境变量** (用于 Claude Code CLI):
```bash
export ANTHROPIC_BASE_URL=https://api.deepseek.com/anthropic
export ANTHROPIC_AUTH_TOKEN=${DEEPSEEK_API_KEY}
export ANTHROPIC_MODEL=deepseek-chat
export API_TIMEOUT_MS=600000
```

**Python 调用示例**:
```python
# 使用 Anthropic SDK
import anthropic

client = anthropic.Anthropic(
    base_url="https://api.deepseek.com/anthropic",
    api_key=os.getenv("DEEPSEEK_API_KEY")
)

message = client.messages.create(
    model="deepseek-chat",
    max_tokens=1000,
    messages=[{"role": "user", "content": "Hello"}]
)
```

**支持特性**:
- ✅ 流式响应 (stream)
- ✅ 工具调用 (tool_use, tool_result)
- ✅ System prompt
- ✅ Temperature (0.0 ~ 2.0)
- ❌ 图片/文档输入

### GLM 4.7

**官方文档**: https://docs.bigmodel.cn/cn/coding-plan/tool/claude

**端点配置**:
```python
base_url = "https://open.bigmodel.cn/api/anthropic/v1"  # 注意需要 /v1
model = "glm-4.7"  # 或 GLM-4.5-Air
```

**环境变量** (用于 Claude Code CLI):
```bash
export ANTHROPIC_BASE_URL=https://open.bigmodel.cn/api/anthropic
export ANTHROPIC_AUTH_TOKEN=${GLM_API_KEY}
export ANTHROPIC_MODEL=glm-4.7
export API_TIMEOUT_MS=3000000
```

**Python 调用示例**:
```python
# 使用 Anthropic SDK
import anthropic

client = anthropic.Anthropic(
    base_url="https://open.bigmodel.cn/api/anthropic/v1",
    api_key=os.getenv("GLM_API_KEY")
)

message = client.messages.create(
    model="glm-4.7",
    max_tokens=1000,
    messages=[{"role": "user", "content": "Hello"}]
)
```

**模型映射**:
| Anthropic 模型 | GLM 模型 |
|---------------|---------|
| claude-opus | GLM-4.7 |
| claude-sonnet | GLM-4.7 |
| claude-haiku | GLM-4.5-Air |

### LLMClient 实现

`services/llm/client.py` 中统一处理 Anthropic 兼容 API：

```python
# 支持的 Anthropic 兼容 provider
ANTHROPIC_COMPATIBLE_PROVIDERS = ("claude", "anthropic", "deepseek", "glm", "zhipu")

# base_url 配置
url_map = {
    "deepseek": "https://api.deepseek.com/anthropic",
    "glm": "https://open.bigmodel.cn/api/anthropic/v1",
    "zhipu": "https://open.bigmodel.cn/api/anthropic/v1",
    "claude": os.getenv("CLAUDE_BASE_URL", "https://api.anthropic.com"),
}

# 流式调用自动路由到 Anthropic 格式
async def _stream_provider(self, provider, model, messages, tools, ...):
    if provider in ANTHROPIC_COMPATIBLE_PROVIDERS:
        async for chunk in self._stream_claude(model, messages, tools, max_tokens, provider=provider):
            yield chunk
```

**测试状态**:
| Provider | 模型 | 工具调用 | 流式响应 | 测试状态 |
|----------|------|---------|---------|----------|
| deepseek | deepseek-chat | ✅ | ✅ | ✅ 通过 |
| glm | glm-4.7 | ✅ | ✅ | ✅ 通过 |

## 测试环境

- API: http://127.0.0.1:8100
- Web: http://127.0.0.1:8232
- 启动: `scripts/start-test.sh`

## Vercel AI SDK 6 精华

> 官方文档: https://sdk.vercel.ai/docs
> 博客: https://vercel.com/blog/ai-sdk-6

### 核心变化 (v5 → v6)

| 特性 | AI SDK 5 | AI SDK 6 |
|------|----------|----------|
| **请求方式** | `api` 字符串 | `transport` 对象 |
| **发送消息** | `append({ content })` | `sendMessage({ text })` |
| **状态管理** | `isLoading` | `status: 'idle'|'submitted'|'streaming'` |
| **工具结果** | 手动处理 | `addToolResult()` 内置 |

### Transport 架构 (核心创新)

AI SDK 6 引入 Transport 层，解耦前端 Hook 和后端通信：

```typescript
import { useChat } from '@ai-sdk/react';
import { DefaultChatTransport } from 'ai';

// 创建 Transport（可自定义 headers、body）
const transport = new DefaultChatTransport({
  api: '/api/v1/chat/v5/stream',
  headers: { Authorization: `Bearer ${token}` },
  body: { skill: 'bazi', voice_mode: 'warm' },
});

// useChat 使用 transport
const { messages, sendMessage, status } = useChat({ transport });
```

**优势**:
- 前端无需关心后端 URL 变化
- 支持自定义认证、请求体
- 可替换为 WebSocket 等其他传输方式

### Data Stream Protocol (SSE 格式)

后端输出 SSE 必须符合此协议，前端才能正确解析：

| 前缀 | 含义 | 示例 |
|------|------|------|
| `0:` | 文本增量 | `0:"Hello"` |
| `9:` | 工具调用 | `9:{"toolCallId":"x","toolName":"show_chart","args":{}}` |
| `a:` | 工具结果 | `a:{"toolCallId":"x","result":{...}}` |
| `d:` | 完成信号 | `d:{"finishReason":"stop"}` |
| `e:` | 元数据 | `e:{"conversation_id":"xxx"}` |
| `3:` | 错误 | `3:"Error message"` |

**Python 后端实现**:
```python
# 文本增量
yield {"data": f'0:{json.dumps(content)}'}

# 工具调用
yield {"data": f'9:{json.dumps({"toolCallId": id, "toolName": name, "args": args})}'}

# 工具结果
yield {"data": f'a:{json.dumps({"toolCallId": id, "result": result})}'}

# 完成
yield {"data": f'd:{json.dumps({"finishReason": "stop"})}'}
```

### useChat Hook (前端)

```typescript
const {
  messages,           // 消息列表
  status,             // 'idle' | 'submitted' | 'streaming'
  sendMessage,        // 发送消息 ({ text: string })
  addToolResult,      // 添加工具结果
  stop,               // 停止生成
  setMessages,        // 重置消息
  regenerate,         // 重新生成
} = useChat({ transport });

// 计算 isLoading
const isLoading = status === 'submitted' || status === 'streaming';
```

### 工具调用流程

```
1. 用户发送消息
2. 后端返回 9:{toolCallId, toolName, args}
3. 前端渲染工具 UI（Generative UI）
4. 工具执行完成后调用 addToolResult()
5. 后端继续处理，返回最终响应
```

**前端工具结果提交**:
```typescript
// 成功
addToolResult({ toolCallId, tool: toolName, output: result });

// 失败/取消
addToolResult({
  toolCallId,
  tool: toolName,
  state: 'output-error',
  errorText: '用户已取消'
});
```

### VibeLife 集成方式

```
┌─────────────────────────────────────────────────────────────────┐
│                    AI SDK 6 集成架构                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  useVibeChat (前端 Hook)                                        │
│  ├─ DefaultChatTransport → /api/v1/chat/v5/stream              │
│  ├─ 自动处理 SSE 解析                                           │
│  └─ addToolResult() 提交工具结果                                │
│                                                                 │
│  Next.js rewrites (代理层)                                      │
│  └─ /api/v1/* → Python API                                     │
│                                                                 │
│  Python CoreAgent (后端)                                        │
│  ├─ 输出 AI SDK 6 Data Stream Protocol                         │
│  ├─ 0: 文本 / 9: 工具调用 / a: 工具结果 / d: 完成               │
│  └─ SSE via sse-starlette                                      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 关键依赖版本

```json
{
  "ai": "^6.0.20",
  "@ai-sdk/react": "^3.0.20",
  "@ai-sdk/deepseek": "^2.0.4",
  "@ai-sdk/openai-compatible": "^2.0.4"
}
```

**注意**: `@ai-sdk/openai-compatible` 必须 >= 2.0.4，旧版本不兼容 AI SDK 6。

## 参考文档

- [Vercel AI SDK 6 Docs](https://sdk.vercel.ai/docs)
- [AI SDK 6 Stream Protocol](https://sdk.vercel.ai/docs/ai-sdk-ui/stream-protocol)
- [Anthropic Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents)
- [Claude Agent SDK Overview](https://platform.claude.com/docs/en/agent-sdk/overview)
- [Claude Agent SDK Python](https://platform.claude.com/docs/en/agent-sdk/python)

# VibeLife V6 Architecture - 方案 B

> Version: 4.0 | 2026-01-14
> 参考:
> - [Anthropic Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents)
> - [Claude Agent SDK Overview](https://platform.claude.com/docs/en/agent-sdk/overview)
> - [Claude Agent SDK Python](https://platform.claude.com/docs/en/agent-sdk/python)
> - [Claude Agent SDK Demos](https://github.com/anthropics/claude-agent-sdk-demos)

## Claude Agent SDK 核心概念

### SDK vs Client SDK

| 特性 | Client SDK | Agent SDK |
|------|-----------|-----------|
| **工具执行** | 你自己实现 tool loop | Claude 自动处理 |
| **内置工具** | 无 | Read, Write, Edit, Bash, Glob, Grep, WebSearch, WebFetch |
| **适用场景** | 直接 API 调用 | 构建自主 Agent |

```python
# Client SDK: 你自己实现 tool loop
response = client.messages.create(...)
while response.stop_reason == "tool_use":
    result = your_tool_executor(response.tool_use)
    response = client.messages.create(tool_result=result, ...)

# Agent SDK: Claude 自动处理工具
async for message in query(prompt="Fix the bug in auth.py"):
    print(message)
```

### query() vs ClaudeSDKClient

| 特性 | query() | ClaudeSDKClient |
|------|---------|-----------------|
| **Session** | 每次创建新 session | 复用同一 session |
| **对话** | 单次交互 | 多轮对话保持上下文 |
| **中断** | ❌ 不支持 | ✅ 支持 |
| **Hooks** | ❌ 不支持 | ✅ 支持 |
| **自定义工具** | ❌ 不支持 | ✅ 支持 |
| **适用场景** | 一次性任务 | 连续对话 |

### 内置工具

| 工具 | 功能 |
|------|------|
| **Read** | 读取文件 |
| **Write** | 创建文件 |
| **Edit** | 精确编辑文件 |
| **Bash** | 执行终端命令 |
| **Glob** | 按模式查找文件 |
| **Grep** | 正则搜索文件内容 |
| **WebSearch** | 搜索网络 |
| **WebFetch** | 获取网页内容 |
| **AskUserQuestion** | 向用户提问 |

### Subagents (子代理)

```python
# 定义专门的子代理
options = ClaudeAgentOptions(
    allowed_tools=["Read", "Glob", "Grep", "Task"],
    agents={
        "code-reviewer": AgentDefinition(
            description="Expert code reviewer for quality and security reviews.",
            prompt="Analyze code quality and suggest improvements.",
            tools=["Read", "Glob", "Grep"]
        )
    }
)
```

### Hooks (钩子)

在 agent 生命周期的关键点运行自定义代码：

| Hook | 触发时机 |
|------|---------|
| **PreToolUse** | 工具执行前 |
| **PostToolUse** | 工具执行后 |
| **UserPromptSubmit** | 用户提交 prompt 时 |
| **Stop** | 停止执行时 |
| **SubagentStop** | 子代理停止时 |

```python
async def validate_bash_command(input_data, tool_use_id, context):
    if 'rm -rf /' in input_data.get('tool_input', {}).get('command', ''):
        return {
            'hookSpecificOutput': {
                'permissionDecision': 'deny',
                'permissionDecisionReason': 'Dangerous command blocked'
            }
        }
    return {}

options = ClaudeAgentOptions(
    hooks={
        'PreToolUse': [HookMatcher(matcher='Bash', hooks=[validate_bash_command])]
    }
)
```

---

## Anthropic Agent 设计原则

### Workflows vs Agents

| 类型 | 定义 | 适用场景 |
|------|------|---------|
| **Workflows** | LLM 和工具通过**预定义代码路径**编排 | 流程固定、可预测的任务 |
| **Agents** | LLM **动态指导**自己的流程和工具使用 | 需要灵活决策的任务 |

**VibeLife V6 采用 Agent 模式**：CoreAgent 动态选择 Skill、决定是否收集信息、选择合适的工具。

### Agentic Loop 核心

```
┌─────────────────────────────────────────────────────────────┐
│  "LLMs using tools based on environmental feedback in a loop" │
└─────────────────────────────────────────────────────────────┘

用户消息 → LLM 思考 → 工具调用 → 环境反馈 → LLM 继续思考 → ...
                ↑                              │
                └──────────────────────────────┘
```

**关键点**：
- 每步从环境获取 "ground truth"（工具调用结果）
- LLM 根据反馈动态调整下一步行动
- **不要用代码硬编码决策逻辑，让 LLM 自己判断**

### Tool Use 最佳实践 (ACI - Agent-Computer Interface)

1. **站在模型角度思考工具是否易用**
   - 工具定义应包含示例用法、边界情况、输入格式要求
   - 参数命名要清晰，像给初级开发者写 docstring

2. **Poka-yoke（防错设计）**
   - 修改参数使错误更难发生
   - 例：强制使用绝对路径而非相对路径

3. **给模型足够 token "think"**
   - 不要急于让模型输出结果
   - 允许模型在工具调用前思考

4. **格式贴近自然文本**
   - 避免复杂的格式要求（如精确计数、字符串转义）

### 三大原则

| 原则 | 说明 |
|------|------|
| **Simplicity** | 保持简单，避免过度工程 |
| **Transparency** | 显式展示 agent 规划步骤 |
| **Documentation & Testing** | 精心设计工具文档，充分测试 |

### Agentic Loop 实现模式 (来自 anthropic-cookbook)

```python
def agent_loop(task: str, tools: list, max_iterations: int = 10):
    messages = [{"role": "user", "content": task}]

    for _ in range(max_iterations):
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            messages=messages,
            tools=tools
        )

        # 检查是否需要执行工具
        if response.stop_reason == "tool_use":
            tool_results = []
            for block in response.content:
                if block.type == "tool_use":
                    result = execute_tool(block.name, block.input)
                    tool_results.append({
                        "type": "tool_result",
                        "tool_use_id": block.id,
                        "content": result
                    })

            # 添加 assistant 消息和工具结果
            messages.append({"role": "assistant", "content": response.content})
            messages.append({"role": "user", "content": tool_results})
        else:
            # 结束循环，返回最终响应
            return response.content

    return messages[-1]
```

**关键点**：
- `stop_reason == "tool_use"` 判断是否需要执行工具
- 工具结果作为 `tool_result` 添加到消息中
- 循环直到 `stop_reason` 不是 `tool_use`
- **让 LLM 决定何时停止，不要用代码硬编码停止条件**

### Tool 定义规范

```python
tools = [
    {
        "name": "search",
        "description": "Search for information. Include examples and edge cases.",
        "input_schema": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The search query. Example: 'Python async tutorial'"
                }
            },
            "required": ["query"]
        }
    }
]
```

**工具描述最佳实践**：
- 包含示例用法
- 说明边界情况
- 参数命名清晰
- 像给初级开发者写 docstring

---

## 架构说明

**当前使用方案 B**（Python CoreAgent 主导 + AI SDK 6 兼容 SSE）

> 详细设计文档见: [VibeLife-Expert-System-v6.md](./VibeLife-Expert-System-v6.md)

## 运行时架构

```
┌─────────────────────────────────────────────────────────────────┐
│                    VibeLife V6 运行时架构                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. LLM 路由 (use_skill 一次决定 skill + scenario + confidence) │
│  2. 加载 SKILL.md + scenario.md (构建 System Prompt)            │
│  3. 执行 SOP (LLM 根据需要动态调用全局工具 + Skill 级工具)       │
│  4. 输出交付                                                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## 架构流程

```
┌─────────────────────────────────────────────────────────────────┐
│       Python CoreAgent 主导 + Next.js 代理 + AI SDK 6 兼容       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Browser (AI SDK 6)                                             │
│  ├─ useVibeChat + DefaultChatTransport                          │
│  └─ 请求 /api/v1/* (相对路径，通过 Next.js 代理)                  │
│                                                                 │
│  Next.js (香港火山云)                                            │
│  ├─ rewrites: /api/v1/* → VIBELIFE_API_INTERNAL                 │
│  └─ 透明代理 SSE 流式响应                                        │
│                                                                 │
│  Python Backend (本机)                                           │
│  ├─ /chat/v5/stream         → CoreAgent 主入口                  │
│  │      ├─ LLM 智能选择 Skill (无关键词匹配)                     │
│  │      ├─ Tool calling + 执行                                  │
│  │      └─ AI SDK 6 Data Stream Protocol SSE                    │
│  ├─ /bazi/chart             → 八字命盘 API                      │
│  ├─ /zodiac/chart           → 星盘 API                          │
│  ├─ /fortune/*              → 运势数据 API                      │
│  └─ /users/me/profile       → 用户画像                          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

时序图:
┌─────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────┐
│Browser  │     │Next.js      │     │Python API   │     │LLM API  │
│         │     │(香港火山云)  │     │(本机)       │     │(DeepSeek)│
└────┬────┘     └──────┬──────┘     └──────┬──────┘     └────┬────┘
     │                 │                   │                 │
     │ 1.发送消息      │                   │                 │
     │  /api/v1/chat/  │                   │                 │
     │  v5/stream      │                   │                 │
     │────────────────>│ 2.rewrites 代理   │                 │
     │                 │──────────────────>│ 3.CoreAgent     │
     │                 │                   │────────────────>│
     │                 │                   │<────────────────│
     │                 │<──────────────────│ 4.SSE响应       │
     │<────────────────│ 5.转发SSE         │                 │
```

## 核心设计

1. **Python CoreAgent 主导** - 后端统一处理 LLM 调用、Skill 选择、Tool 执行
2. **AI SDK 6 兼容 SSE** - 输出符合 AI SDK 6 Data Stream Protocol 的 SSE 格式
3. **前端零改动** - 复用现有 useVibeChat + DefaultChatTransport
4. **LLM 智能路由** - CoreAgent 通过 LLM 判断用户意图，选择合适的 Skill
5. **统一 LLM 调用** - 所有 LLM 调用通过 `services/llm/client.py`

## Context 构建

```python
# Python: /api/v1/context/build
ContextBuilder.build(
    skill=skill,           # bazi/zodiac
    voice_mode=voice_mode, # warm/sarcastic/wise
    current_message=msg,
    profile=profile,       # 用户画像
    skill_data=skill_data, # 命盘数据
    history=history,       # 对话历史
)
→ (system_prompt, messages)
```

- 话题检测: career/relationship/fortune/self/general
- 智能裁剪: 根据话题选择相关 Profile 字段
- Identity Prism: 始终注入核心画像摘要

## 工具系统

### 工具层级架构

| 层级 | 定义位置 | 加载时机 | 示例 |
|------|---------|---------|------|
| 全局工具 | 系统��� | 始终可用 | search_db, ask_user_question, show_insight |
| Skill 级工具 | SKILL.md | 路由到该 skill 后加载 | calculate_bazi, show_bazi_chart |

### 全局工具

```typescript
const globalTools = {
  search_db,           // 统一数据库查询（知识/案例）
  ask_user_question,   // 主动追问用户
  show_service_menu,   // 展示服务目录
  show_insight,        // 通用洞察卡片
  show_report,         // 通用报告
};
```

### Skill 级工具示例

**bazi skill:**
```typescript
const baziTools = {
  collect_bazi_info,   // 收集出生信息
  calculate_bazi,      // 排盘计算
  show_bazi_chart,     // 展示命盘图
  show_bazi_fortune,   // 展示运势分析
  show_bazi_kline,     // 展示运势K线
};
```

工具执行调用 Python 数据 API，结果返回前端渲染 Generative UI。

## 文件结构

```
apps/api/
├── routes/
│   ├── chat_v5.py        # V5 Chat 端点 (CoreAgent 主入口)
│   ├── context.py        # /context/build API
│   ├── fortune.py        # 运势数据 API
│   ├── report.py         # 报告 API
│   └── relationship.py   # 关系分析 API
├── services/
│   ├── agent/            # CoreAgent 实现
│   └── vibe_engine/
│       ├── context.py    # ContextBuilder
│       ├── tools.py      # 工具定义
│       └── profile_cache.py

apps/web/
├── src/hooks/useVibeChat.ts   # AI SDK 6 集成 (相对路径)
├── src/components/chat/       # Chat UI 组件
└── next.config.js             # rewrites 代理配置
```

## 端点

| 端点 | 说明 |
|------|------|
| `/api/v1/context/build` | Context 构建 API (核心) |
| `/api/v1/context/messages/save` | 消息持久化 |
| `/api/v1/context/quota/record` | 配额记录 |
| `/api/v1/bazi/chart` | 八字命盘 API |
| `/api/v1/zodiac/chart` | 星盘 API |
| `/api/v1/zodiac/transit` | 行运分析 API |
| `/api/v1/fortune/*` | 运势数据 API |
| `/api/v1/report/*` | 报告 API |
| `/api/v1/relationship/*` | 关系分析 API |

## LLM 调用规范

**前后端统一使用 `config/models.yaml` 作为唯一配置源**

```
config/models.yaml       ← 唯一配置源
        │
        ├─ Python: services/llm/client.py (LLMClient)
        │
        └─ Next.js: /context/build API 返回 llm_config
           - provider: 提供商 (deepseek/glm/google/openai-compatible)
           - model: 模型 ID
           - base_url: deepseek/glm/openai-compatible 需要
           - api_key: 从后端配置获取（前端无需 process.env）
```

**llm_config 结构**:
```python
class LLMConfig(BaseModel):
    provider: str
    model: str
    base_url: Optional[str] = None
    api_key: str  # 从后端配置获取
```

**Provider 映射规则**:
| provider | 模型 ID 格式 | SDK | base_url | 测试状态 |
|----------|-------------|-----|----------|----------|
| deepseek | `deepseek-chat` (官方) | @ai-sdk/deepseek | 不需要 | ✅ 通过 |
| glm | `glm-4.7` 等 | @ai-sdk/openai-compatible | 必需 | ✅ 通过 |
| google | `gemini-xxx` | @ai-sdk/google | 不需要 | - |

**关键发现**:
- GLM 必须使用 `@ai-sdk/openai-compatible` 而不是 `@ai-sdk/openai`
- 需要升级 `@ai-sdk/openai-compatible` 到 2.0.4（旧版本 0.2.16 不兼容 AI SDK 6）
- DeepSeek 官方 API 使用 `@ai-sdk/deepseek`，火山引擎使用 `@ai-sdk/openai`

**前端代码**:
```typescript
// API key 从后端 llm_config 获取，无需 process.env
function getLLMModel(config: { provider: string; model: string; base_url?: string; api_key: string }) {
  switch (config.provider) {
    case 'deepseek':
      // 官方 API 使用 @ai-sdk/deepseek
      const isOfficialAPI = !config.base_url || config.base_url.includes('api.deepseek.com');
      if (isOfficialAPI) {
        return createDeepSeek({ apiKey: config.api_key })(config.model);
      }
      // 火山引擎等第三方使用 @ai-sdk/openai
      return createOpenAI({ baseURL: config.base_url, apiKey: config.api_key })(config.model);
    case 'glm':
      // GLM 必须使用 @ai-sdk/openai-compatible
      return createOpenAICompatible({
        name: 'glm',
        baseURL: config.base_url,
        apiKey: config.api_key,
      })(config.model);
    case 'google':
      return createGoogleGenerativeAI({ apiKey: config.api_key })(config.model);
  }
}
```

**优势**:
- 修改 API key 只需重启后端，前端无需重启
- 配置集中管理，避免前后端不一致
- 支持运行时动态切换模型

**禁止**:
- 前端硬编码模型名称或 API 地址
- 前端从 process.env 读取 API key（应从 llm_config 获取）

## 数据流

```
用户输入 → useVibeChat → /api/v1/chat/v5/stream (相对路径)
                              │
                              ▼
                         Next.js rewrites
                              │
                              ▼
                    Python API /chat/v5/stream
                              │
                              ├─ [A] 入口配额检查
                              │      → 配额不足返回 429
                              │
                              ├─ [B] CoreAgent 处理
                              │      ├─ 构建 context (profile + skill_data)
                              │      ├─ LLM 调用 (DeepSeek/GLM)
                              │      ├─ Tool calling + 执行
                              │      └─ 流式输出 SSE
                              │
                              └─ [C] 保存消息 + 记录配额
                              │
                              ▼
                    SSE 响应 (AI SDK 6 Protocol)
                              │
                              ▼
                    Next.js 转发 → Browser 渲染
```

## 可靠性设计

### 入口配额检查
- 在调用 LLM 前先检查配额，避免浪费资源
- 配额不足返回 HTTP 429 + `QUOTA_EXCEEDED` 错误码

### 消息保存重试
- `onFinish` 回调中的保存操作带重试机制（最多 3 次）
- 重试间隔：500ms → 1000ms → 1500ms
- 用户消息、助手消息、配额记录并行保存

## 与 AI SDK 6 集成

| 效果 | 实现方式 |
|------|---------|
| 打字机效果 | `streamText` 原生支持 |
| 工具调用 | `tool()` + `execute` |
| Generative UI | 工具结果 + 前端组件 |
| 多轮对话 | Context API 返回 messages |

---

## Skill 系统

```
apps/api/skills/
├── core/     # 始终激活 - Vibe 人格
├── bazi/     # 八字命理
└── zodiac/   # 西方占星
```

Skill 内容通过 ContextBuilder 注入到 system_prompt。

---

## 对话引擎 (DialogueEngine)

> 详细设计见: [dialogue-engine.md](./dialogue-engine.md)

配置文件驱动的对话流程引擎，用于 Onboarding 等特殊场景。

### 架构

```
apps/api/services/dialogue/
├── engine.py              # 流程引擎核心
├── rhythm.py              # 节奏控制
├── extractors.py          # 信息提取器
└── flows/
    ├── onboarding.yaml    # 新用户引导
    └── deep_talk.yaml     # 深度对话
```

### 步骤类型

| 类型 | 说明 |
|------|------|
| `template` | 预写模板，支持变量插值 |
| `generate` | 调用 LLM 生成 |
| `extract` | 从用户输入提取信息 |
| `tool` | 调用前端工具 (Generative UI) |
| `action` | 执行后端动作 |

---

## 权益系统 (Entitlement)

**配置文件**: `apps/api/services/entitlement/service.py`

**单一配置源**:
```python
# SINGLE SOURCE OF TRUTH
TIER_CONFIG = {
    "free": {"daily_limit": 3},
    "paid": {"daily_limit": 200},
}
DEFAULT_TIER = "paid"
```

**Tier 配置**:
| Tier | 每日对话限制 | 说明 |
|------|-------------|------|
| free | 3 | 免费用户 |
| paid | 200 | 付费用户 |

**规范**:
- 所有 tier 相关的值必须从 `TIER_CONFIG` 读取
- 禁止在代码中硬编码 tier 名称或限制值
- 新用户默认 tier 由 `DEFAULT_TIER` 控制
- `upgrade_to_paid()` 和 `downgrade_to_free()` 从配置读取 limit
