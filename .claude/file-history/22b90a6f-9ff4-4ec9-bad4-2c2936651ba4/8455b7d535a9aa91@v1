#!/usr/bin/env python3
"""
Knowledge Builder Pipeline - Unified CLI Entry Point

Usage:
    python scripts/build_knowledge.py --skill bazi --stages all
    python scripts/build_knowledge.py --skill bazi --stages 0,1,2,3
    python scripts/build_knowledge.py --skill bazi --stages 4a,4b
    python scripts/build_knowledge.py --skill bazi --stages 5 --review
    python scripts/build_knowledge.py --skill bazi --stages 6 --report
"""
import asyncio
import argparse
import logging
import sys
import os
from pathlib import Path
from datetime import datetime

# Add project root to path
# parents[0]=scripts, [1]=api, [2]=apps, [3]=vibelife
project_root = Path(__file__).resolve().parents[3]
api_path = project_root / "apps" / "api"
if str(api_path) not in sys.path:
    sys.path.insert(0, str(api_path))

from dotenv import load_dotenv
load_dotenv(project_root / ".env")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger("knowledge_builder")

# Data directories
DATA_DIR = Path("/data/vibelife")
SKILLS_DIR = api_path / "skills"


class KnowledgeBuilder:
    """
    Unified knowledge building pipeline.

    Stages:
        0: Format conversion (source → converted MD)
        1: Parse & chunk
        2: Vectorize
        3: Store in database
        4a: Extract cases
        4b: Generate scenario candidates
        5: Review & publish scenarios
        6: Quality check & report
    """

    def __init__(self, skill_id: str):
        self.skill_id = skill_id
        self.source_dir = DATA_DIR / "knowledge" / skill_id / "source"
        self.converted_dir = DATA_DIR / "knowledge" / skill_id / "converted"
        self.skill_dir = SKILLS_DIR / skill_id

    async def run_stage_0(self):
        """Stage 0: Format conversion (supports subdirectories)"""
        logger.info("=" * 60)
        logger.info("Stage 0: Format Conversion")
        logger.info("=" * 60)

        from workers.converters import DocumentConverter

        converter = DocumentConverter()
        self.source_dir.mkdir(parents=True, exist_ok=True)
        self.converted_dir.mkdir(parents=True, exist_ok=True)

        # Recursively find all files in source directory
        source_files = list(self.source_dir.rglob("*"))
        converted_count = 0

        for source_file in source_files:
            if source_file.is_file() and converter.can_convert(str(source_file)):
                # Preserve subdirectory structure in converted dir
                rel_path = source_file.relative_to(self.source_dir)
                converted_subdir = self.converted_dir / rel_path.parent
                converted_subdir.mkdir(parents=True, exist_ok=True)
                converted_path = converted_subdir / f"{source_file.stem}.converted.md"

                if converted_path.exists():
                    logger.info(f"  Skip (exists): {rel_path}")
                    continue

                try:
                    md_content = converter.convert(str(source_file))
                    converted_path.write_text(md_content, encoding="utf-8")
                    converted_count += 1
                    logger.info(f"  Converted: {rel_path} → {converted_path.name}")
                except Exception as e:
                    logger.error(f"  Failed: {rel_path} - {e}")

        logger.info(f"Stage 0 complete: {converted_count} files converted")
        return converted_count

    async def run_stages_1_2_3(self):
        """Stages 1-3: Chunk, vectorize, store (supports subdirectories)"""
        logger.info("=" * 60)
        logger.info("Stages 1-3: Chunk → Vectorize → Store")
        logger.info("=" * 60)

        from workers.ingestion import IngestionWorker

        worker = IngestionWorker()
        # Recursively find all MD files
        md_files = list(self.converted_dir.rglob("*.md"))

        total_chunks = 0
        for md_file in md_files:
            try:
                result = await worker.process_file(str(md_file), self.skill_id)
                chunks = result.get("chunk_count", 0)
                total_chunks += chunks
                logger.info(f"  Processed: {md_file.name} ({chunks} chunks)")
            except Exception as e:
                logger.error(f"  Failed: {md_file.name} - {e}")

        logger.info(f"Stages 1-3 complete: {total_chunks} total chunks")
        return total_chunks

    async def run_stage_4a(self, limit: int = 100):
        """Stage 4a: Extract cases"""
        logger.info("=" * 60)
        logger.info("Stage 4a: Case Extraction")
        logger.info("=" * 60)

        from workers.case_extractor import CaseExtractor
        from stores.db import get_connection

        # Fetch chunks
        async with get_connection() as conn:
            rows = await conn.fetch(
                """
                SELECT id, chunk_text as content
                FROM knowledge_chunks
                WHERE skill_id = $1
                LIMIT $2
                """,
                self.skill_id,
                limit,
            )
            chunks = [dict(row) for row in rows]

        if not chunks:
            logger.warning("No chunks found for case extraction")
            return 0

        extractor = CaseExtractor()
        cases = await extractor.extract_from_chunks(chunks, self.skill_id)
        saved = await extractor.save_cases(cases)

        logger.info(f"Stage 4a complete: {len(cases)} cases extracted, {saved} saved")
        return saved

    async def run_stage_4b(self, limit: int = 50):
        """Stage 4b: Generate scenario candidates"""
        logger.info("=" * 60)
        logger.info("Stage 4b: Scenario Generation")
        logger.info("=" * 60)

        from workers.scenario_generator import ScenarioGenerator
        from stores.db import get_connection

        # Fetch chunks
        async with get_connection() as conn:
            rows = await conn.fetch(
                """
                SELECT id, chunk_text as content, chunk_type
                FROM knowledge_chunks
                WHERE skill_id = $1
                ORDER BY chunk_type DESC
                LIMIT $2
                """,
                self.skill_id,
                limit,
            )
            chunks = [dict(row) for row in rows]

        if not chunks:
            logger.warning("No chunks found for scenario generation")
            return 0

        generator = ScenarioGenerator()
        scenarios = await generator.generate_from_chunks(chunks, self.skill_id)
        saved = await generator.save_candidates(scenarios)

        logger.info(f"Stage 4b complete: {len(scenarios)} scenarios generated, {saved} saved")
        return saved

    async def run_stage_5(self, interactive: bool = True):
        """Stage 5: Review & publish scenarios"""
        logger.info("=" * 60)
        logger.info("Stage 5: Scenario Review & Publish")
        logger.info("=" * 60)

        from workers.scenario_reviewer import ScenarioReviewer, review_cli

        if interactive:
            await review_cli()
        else:
            # Auto-approve all pending
            reviewer = ScenarioReviewer()
            pending = await reviewer.list_pending(self.skill_id)

            for s in pending:
                await reviewer.approve(s["skill_id"], s["scenario_id"])
                logger.info(f"  Auto-approved: {s['scenario_id']}")

            logger.info(f"Stage 5 complete: {len(pending)} scenarios approved")
            return len(pending)

    async def run_stage_6(self, save_report: bool = True):
        """Stage 6: Quality check"""
        logger.info("=" * 60)
        logger.info("Stage 6: Quality Check")
        logger.info("=" * 60)

        from workers.quality_checker import QualityChecker

        checker = QualityChecker()
        report = await checker.generate_report(self.skill_id)

        print(checker.format_report(report))

        if save_report:
            report_id = await checker.save_report(report)
            logger.info(f"Report saved with ID: {report_id}")

        return report.overall_score

    async def run_all(self):
        """Run complete pipeline"""
        logger.info("=" * 60)
        logger.info(f"Knowledge Builder Pipeline - {self.skill_id}")
        logger.info(f"Started: {datetime.now().isoformat()}")
        logger.info("=" * 60)

        results = {}

        # Stage 0
        results["stage_0"] = await self.run_stage_0()

        # Stages 1-3
        results["stages_1_3"] = await self.run_stages_1_2_3()

        # Stage 4a
        results["stage_4a"] = await self.run_stage_4a()

        # Stage 4b
        results["stage_4b"] = await self.run_stage_4b()

        # Stage 5 (non-interactive for full pipeline)
        results["stage_5"] = await self.run_stage_5(interactive=False)

        # Stage 6
        results["stage_6"] = await self.run_stage_6()

        logger.info("=" * 60)
        logger.info("Pipeline Complete!")
        logger.info(f"Results: {results}")
        logger.info("=" * 60)

        return results


async def main():
    parser = argparse.ArgumentParser(
        description="Knowledge Builder Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run all stages
  python build_knowledge.py --skill bazi --stages all

  # Run specific stages
  python build_knowledge.py --skill bazi --stages 0,1,2,3
  python build_knowledge.py --skill bazi --stages 4a,4b

  # Interactive review
  python build_knowledge.py --skill bazi --stages 5 --review

  # Quality report only
  python build_knowledge.py --skill bazi --stages 6
        """
    )

    parser.add_argument(
        "--skill", "-s",
        required=True,
        help="Skill ID (bazi, zodiac, tarot, career)"
    )
    parser.add_argument(
        "--stages",
        default="all",
        help="Stages to run: all, 0, 1-3, 4a, 4b, 5, 6 (comma-separated)"
    )
    parser.add_argument(
        "--review",
        action="store_true",
        help="Interactive review mode for stage 5"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=100,
        help="Limit for chunk processing (default: 100)"
    )

    args = parser.parse_args()

    builder = KnowledgeBuilder(args.skill)

    if args.stages == "all":
        await builder.run_all()
    else:
        stages = [s.strip() for s in args.stages.split(",")]

        for stage in stages:
            if stage == "0":
                await builder.run_stage_0()
            elif stage in ("1", "2", "3", "1-3"):
                await builder.run_stages_1_2_3()
            elif stage == "4a":
                await builder.run_stage_4a(args.limit)
            elif stage == "4b":
                await builder.run_stage_4b(args.limit)
            elif stage == "5":
                await builder.run_stage_5(interactive=args.review)
            elif stage == "6":
                await builder.run_stage_6()
            else:
                logger.warning(f"Unknown stage: {stage}")


if __name__ == "__main__":
    asyncio.run(main())
