#!/usr/bin/env python3
"""
Knowledge Builder Pipeline - Unified CLI Entry Point

Usage:
    python scripts/build_knowledge.py --skill bazi --stages all
    python scripts/build_knowledge.py --skill bazi --stages 0,1,2,3
    python scripts/build_knowledge.py --skill bazi --stages 4a,4b
    python scripts/build_knowledge.py --skill bazi --stages 5 --review
    python scripts/build_knowledge.py --skill bazi --stages 6 --report
"""
import asyncio
import argparse
import logging
import sys
import os
from pathlib import Path
from datetime import datetime

# Add project root to path
# parents[0]=scripts, [1]=api, [2]=apps, [3]=vibelife
project_root = Path(__file__).resolve().parents[3]
api_path = project_root / "apps" / "api"
if str(api_path) not in sys.path:
    sys.path.insert(0, str(api_path))

from dotenv import load_dotenv
load_dotenv(project_root / ".env")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger("knowledge_builder")

# Data directories
DATA_DIR = Path("/data/vibelife")
SKILLS_DIR = api_path / "skills"


class KnowledgeBuilder:
    """
    Unified knowledge building pipeline.

    Stages:
        0: Format conversion (source → converted MD)
        1: Parse & chunk
        2: Vectorize
        3: Store in database
        4a: Extract cases
        4b: Generate scenario candidates
        5: Review & publish scenarios
        6: Quality check & report
    """

    def __init__(self, skill_id: str):
        self.skill_id = skill_id
        self.source_dir = DATA_DIR / "knowledge" / skill_id / "source"
        self.converted_dir = DATA_DIR / "knowledge" / skill_id / "converted"
        self.skill_dir = SKILLS_DIR / skill_id

    async def run_stage_0(self):
        """Stage 0: Format conversion (supports subdirectories)"""
        logger.info("=" * 60)
        logger.info("Stage 0: Format Conversion")
        logger.info("=" * 60)

        from workers.converters import DocumentConverter

        converter = DocumentConverter()
        self.source_dir.mkdir(parents=True, exist_ok=True)
        self.converted_dir.mkdir(parents=True, exist_ok=True)

        # Recursively find all files in source directory
        source_files = list(self.source_dir.rglob("*"))
        converted_count = 0

        for source_file in source_files:
            if source_file.is_file() and converter.can_convert(str(source_file)):
                # Preserve subdirectory structure in converted dir
                rel_path = source_file.relative_to(self.source_dir)
                converted_subdir = self.converted_dir / rel_path.parent
                converted_subdir.mkdir(parents=True, exist_ok=True)
                converted_path = converted_subdir / f"{source_file.stem}.converted.md"

                if converted_path.exists():
                    logger.info(f"  Skip (exists): {rel_path}")
                    continue

                try:
                    md_content = converter.convert(str(source_file))
                    converted_path.write_text(md_content, encoding="utf-8")
                    converted_count += 1
                    logger.info(f"  Converted: {rel_path} → {converted_path.name}")
                except Exception as e:
                    logger.error(f"  Failed: {rel_path} - {e}")

        logger.info(f"Stage 0 complete: {converted_count} files converted")
        return converted_count

    async def run_stages_1_2_3(self):
        """Stages 1-3: Chunk, vectorize, store (supports subdirectories)"""
        logger.info("=" * 60)
        logger.info("Stages 1-3: Chunk → Vectorize → Store")
        logger.info("=" * 60)

        from workers.ingestion import IngestionWorker

        worker = IngestionWorker()
        # Recursively find all MD files
        md_files = list(self.converted_dir.rglob("*.md"))

        total_chunks = 0
        for md_file in md_files:
            try:
                result = await worker.process_file(str(md_file), self.skill_id)
                chunks = result.get("chunk_count", 0)
                total_chunks += chunks
                logger.info(f"  Processed: {md_file.name} ({chunks} chunks)")
            except Exception as e:
                logger.error(f"  Failed: {md_file.name} - {e}")

        logger.info(f"Stages 1-3 complete: {total_chunks} total chunks")
        return total_chunks

    async def run_stage_4(self, force_reextract: bool = False):
        """
        Stage 4: Unified Extraction from MD Files (recommended)

        This is the new unified extraction that:
        - Reads directly from /data/vibelife/knowledge/{skill}/converted/*.md
        - Extracts both Cases and Scenarios in a single LLM call
        - Supports incremental processing via MD5 hash tracking
        - Outputs:
          - Cases → cases table (status='pending')
          - Scenarios → extracted/scenarios/*.md (candidates for review)

        Args:
            force_reextract: If True, reprocess all files regardless of MD5 status
        """
        logger.info("=" * 60)
        logger.info("Stage 4: Unified Extraction (from MD files)")
        logger.info("=" * 60)

        from workers.unified_extractor import UnifiedExtractor

        extractor = UnifiedExtractor(self.skill_id)
        results = await extractor.process_pending_files(force_reextract=force_reextract)

        logger.info(
            f"Stage 4 complete: {results['cases_count']} cases, "
            f"{results['scenarios_count']} scenarios from "
            f"{results['files_processed']} files "
            f"({results['files_skipped']} skipped)"
        )
        return results

    async def run_stage_6(self, save_report: bool = True):
        """Stage 6: Quality check"""
        logger.info("=" * 60)
        logger.info("Stage 6: Quality Check")
        logger.info("=" * 60)

        from workers.quality_checker import QualityChecker

        checker = QualityChecker()
        report = await checker.generate_report(self.skill_id)

        print(checker.format_report(report))

        if save_report:
            report_id = await checker.save_report(report)
            logger.info(f"Report saved with ID: {report_id}")

        return report.overall_score

    async def run_all(self):
        """Run complete pipeline"""
        logger.info("=" * 60)
        logger.info(f"Knowledge Builder Pipeline - {self.skill_id}")
        logger.info(f"Started: {datetime.now().isoformat()}")
        logger.info("=" * 60)

        results = {}

        # Stage 0: Format conversion
        results["stage_0"] = await self.run_stage_0()

        # Stages 1-3: Chunk, vectorize, store
        results["stages_1_3"] = await self.run_stages_1_2_3()

        # Stage 4: Unified extraction with auto-review
        results["stage_4"] = await self.run_stage_4()

        # Stage 6: Quality check
        results["stage_6"] = await self.run_stage_6()

        logger.info("=" * 60)
        logger.info("Pipeline Complete!")
        logger.info(f"Results: {results}")
        logger.info("=" * 60)

        return results


async def main():
    parser = argparse.ArgumentParser(
        description="Knowledge Builder Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run all stages (0 → 1-3 → 4 → 6)
  python build_knowledge.py --skill bazi --stages all

  # Run specific stages
  python build_knowledge.py --skill bazi --stages 0,1,2,3

  # Stage 4: Unified extraction with LLM auto-review
  python build_knowledge.py --skill bazi --stages 4
  python build_knowledge.py --skill bazi --stages 4 --force-reextract

  # Stage 6: Quality report only
  python build_knowledge.py --skill bazi --stages 6
        """
    )

    parser.add_argument(
        "--skill", "-s",
        required=True,
        help="Skill ID (bazi, zodiac, tarot, career)"
    )
    parser.add_argument(
        "--stages",
        default="all",
        help="Stages to run: all, 0, 1-3, 4, 6 (comma-separated)"
    )
    parser.add_argument(
        "--force-reextract",
        action="store_true",
        help="Force re-extraction of all files (ignores MD5 cache)"
    )

    args = parser.parse_args()

    builder = KnowledgeBuilder(args.skill)

    if args.stages == "all":
        await builder.run_all()
    else:
        stages = [s.strip() for s in args.stages.split(",")]

        for stage in stages:
            if stage == "0":
                await builder.run_stage_0()
            elif stage in ("1", "2", "3", "1-3"):
                await builder.run_stages_1_2_3()
            elif stage == "4":
                # Unified extraction with LLM auto-review
                force = getattr(args, 'force_reextract', False)
                await builder.run_stage_4(force_reextract=force)
            elif stage == "6":
                await builder.run_stage_6()
            else:
                logger.warning(f"Unknown stage: {stage}")


if __name__ == "__main__":
    asyncio.run(main())
