---
name: vibelife-skill
description: |
  VibeLife Skill - çŸ¥è¯†åº“ä¸ä¸“å®¶ç³»ç»Ÿæ„å»ºå·¥å…· v6ã€‚
  åŸºäº VibeLife Expert System v6 æ¶æ„ï¼Œæ•´åˆçŸ¥è¯†æ„å»ºæµæ°´çº¿å…¨æµç¨‹ã€‚
  è§¦å‘è¯ï¼švibelife-skill, å»ºåº“, å»ºskill, çŸ¥è¯†åº“æ„å»º, skillæ„å»º
---

# VibeLife Skill v6 - çŸ¥è¯†åº“æ„å»ºå·¥å…·

## å¿«é€Ÿå¯¼èˆª

> **æ¸è¿›å¼åŠ è½½**: å…ˆè¯»æœ¬èŠ‚(L1-60)è·å–ç´¢å¼•ï¼Œå†æŒ‰éœ€è·³è½¬åˆ°å¯¹åº”æ¨¡å—

| æ¨¡å— | è¡Œå· | è¯´æ˜ |
|------|------|------|
| [æ¦‚è¿°](#æ¦‚è¿°) | L30-48 | æµæ°´çº¿æ¶æ„ã€æ ¸å¿ƒåŸåˆ™ |
| [ç›®å½•ç»“æ„](#ç›®å½•ç»“æ„) | L50-84 | çŸ¥è¯†æºã€Skillç›®å½•ã€å·¥å…·ç³»ç»Ÿ |
| [Stage 0-3](#stage-0-3-æ ¼å¼è½¬æ¢ä¸å…¥åº“) | L86-103 | æ ¼å¼è½¬æ¢ã€åˆ‡å—ã€å‘é‡åŒ–ã€å…¥åº“ |
| [Stage 4 è‡ªåŠ¨](#stage-4-ç»Ÿä¸€æŠ½å–) | L105-175 | ç»Ÿä¸€æŠ½å– + LLM è‡ªåŠ¨å®¡æ ¸ |
| [Stage 4M æ‰‹å·¥](#stage-4m-æ‰‹å·¥æŠ½å–) | L177-452 | Claude Code æ‰‹å·¥æŠ½å–æµç¨‹ (Subagent å¹¶è¡Œ) â˜… |
| [Stage 6](#stage-6-è´¨é‡æ£€æŸ¥) | L454-462 | è´¨é‡æ£€æŸ¥ |
| [å·¥å…·å®šä¹‰](#å·¥å…·å®šä¹‰) | L464-507 | use_skill, search_db |
| [æ¨¡æ¿ç´¢å¼•](#æ¨¡æ¿æ–‡ä»¶) | L509-545 | æ‰€æœ‰æ¨¡æ¿æ–‡ä»¶å¼•ç”¨ |
| [CLI/API](#ä½¿ç”¨æ–¹å¼) | L547-575 | å‘½ä»¤è¡Œå’ŒAPIç«¯ç‚¹ |
| [é™„å½•](#é™„å½•) | L577-612 | è‡ªæ£€æµç¨‹ã€æ ¸å¿ƒåŸåˆ™ã€ç¦æ­¢è¡Œä¸º |

---

## æ¦‚è¿°

åŸºäº VibeLife Expert System v6 æ¶æ„ï¼Œæä¾›ä»åŸå§‹èµ„æ–™åˆ°å¯ç”¨ä¸“å®¶ç³»ç»Ÿçš„å®Œæ•´æµæ°´çº¿ã€‚

**æ ¸å¿ƒåŸåˆ™**ï¼šæ‰€æœ‰æ¶‰åŠå¤§æ¨¡å‹çš„æ­¥éª¤éƒ½åœ¨ Claude Code ä¸­å®Œæˆï¼Œè‡ªåŠ¨åŒ–æµç¨‹ä¸è°ƒç”¨å¤–éƒ¨ LLM APIã€‚

**æµæ°´çº¿æ¶æ„**:
```
Stage 0: æ ¼å¼è½¬æ¢ (è‡ªåŠ¨åŒ–) â†’ PDF/EPUB/DOCX â†’ Markdown
Stage 1-3: åˆ‡å— â†’ å‘é‡åŒ– â†’ å…¥åº“ (è‡ªåŠ¨åŒ–)
Stage 4: ç»Ÿä¸€æŠ½å– + LLM è‡ªåŠ¨å®¡æ ¸
         â”œâ”€ Cases â†’ è‡ªåŠ¨è¯„åˆ†å…¥åº“ (score >= 0.6)
         â””â”€ Scenarios â†’ ç›¸ä¼¼åº¦æ£€æµ‹åè‡ªåŠ¨å‘å¸ƒ/èåˆ
Stage 6: è´¨é‡æ£€æŸ¥ (è‡ªåŠ¨åŒ–)
```

**å¤§æ–‡ä»¶å¤„ç†**: æ–‡ä»¶ > 30K tokens æ—¶è‡ªåŠ¨è°ƒç”¨ vibe-manus å¹¶è¡Œå¤„ç†æ¡†æ¶

---

## ç›®å½•ç»“æ„

### çŸ¥è¯†æºç›®å½•
```
/data/vibelife/knowledge/{skill_id}/
â”œâ”€â”€ source/                    # åŸå§‹æºæ–‡ä»¶
â”œâ”€â”€ converted/                 # è½¬æ¢åçš„ Markdown
â””â”€â”€ extracted/
    â””â”€â”€ extraction_records.json  # æŠ½å–è®°å½• (Cases + Scenarios)
```

### Skill ç›®å½•
```
/home/aiscend/work/vibelife/apps/api/skills/{skill_id}/
â”œâ”€â”€ SKILL.md                   # Skill æ ¸å¿ƒå®šä¹‰
â”œâ”€â”€ scenarios/{scenario_id}.md # MiniSkill åœºæ™¯åº“
â””â”€â”€ tools/
    â”œâ”€â”€ tools.yaml             # å·¥å…·å®šä¹‰ (YAML)
    â””â”€â”€ handlers.py            # å·¥å…·æ‰§è¡Œå™¨ (@tool_handler)
```

### å·¥å…·ç³»ç»Ÿæ¶æ„
```
å…¨å±€å·¥å…· (skills/core/tools/):
â”œâ”€â”€ search_db          # ç»Ÿä¸€æ•°æ®åº“æŸ¥è¯¢
â”œâ”€â”€ ask_user_question  # ä¸»åŠ¨è¿½é—®ç”¨æˆ·
â”œâ”€â”€ request_info       # è¯·æ±‚ç”¨æˆ·æä¾›ä¿¡æ¯
â”œâ”€â”€ show_service_menu  # å±•ç¤ºæœåŠ¡ç›®å½•
â”œâ”€â”€ show_insight       # é€šç”¨æ´å¯Ÿå¡ç‰‡
â””â”€â”€ show_report        # é€šç”¨æŠ¥å‘Š

Skillçº§å·¥å…·: å®šä¹‰åœ¨å„ skills/{skill}/tools/ ä¸­
```

---

## Stage 0-3: æ ¼å¼è½¬æ¢ä¸å…¥åº“

### Stage 0: æ ¼å¼è½¬æ¢

| è¾“å…¥æ ¼å¼ | è½¬æ¢æ–¹æ³• | è¾“å‡º |
|---------|---------|------|
| PDF | pymupdf4llm | MD |
| EPUB | ebooklib + html2text | MD |
| DOCX | python-docx | MD |
| HTML | BeautifulSoup + html2text | MD |

### Stage 1-3: åˆ‡å— â†’ å‘é‡åŒ– â†’ å…¥åº“

- **åˆ‡å—**: max_tokens=2000, min_tokens=200, overlap=50
- **å‘é‡åŒ–**: BAAI/bge-m3, 1024ç»´, batch_size=4
- **å…¥åº“**: PostgreSQL + pgvector + tsvector

---

## Stage 4: ç»Ÿä¸€æŠ½å–

**å®ç°**: `apps/api/workers/unified_extractor.py`

**æ•°æ®æº**: `/data/vibelife/knowledge/{skill}/converted/*.md`

**è¾“å‡º**:
- Cases â†’ `cases` è¡¨ (è‡ªåŠ¨å®¡æ ¸å…¥åº“)
- Scenarios â†’ `skills/{skill}/scenarios/*.md` (è‡ªåŠ¨å‘å¸ƒ)

### å”¯ä¸€æ ‡è¯†è®¾è®¡

```
location_id = {source_file}:{section}:{index}
ç¤ºä¾‹: "ä¸œæ–¹ä»£ç å¯ç¤ºå½•.converted.md:ç¬¬ä¸‰ç« å¤§è¿åˆ†æ:0"
```

### è®°å½•æ–‡ä»¶ç»“æ„ (extraction_records.json)

```json
{
  "skill_id": "bazi",
  "schema_version": "2.0",
  "source_files": {
    "æ–‡ä»¶å.md": {"md5": "...", "last_extracted_at": "..."}
  },
  "cases": {
    "{location_id}": {
      "name": "æ¡ˆä¾‹åç§°",
      "core_data": {},
      "reasoning_chain": [],
      "guidance_patterns": [],
      "quality_score": 0.85,
      "status": "approved|pending",
      "db_id": "CASE_bazi_xxx"
    }
  },
  "scenarios": {
    "{location_id}": {
      "scenario_id": "...",
      "status": "published_new|published_merge|pending"
    }
  }
}
```

### Case å…¥åº“æ¡ä»¶

| æ¡ä»¶ | æƒé‡ | è¯´æ˜ |
|------|------|------|
| `reasoning_chain` | 50% | â‰¥3æ­¥å¾—æ»¡åˆ† |
| `guidance_patterns` | 50% | â‰¥2ä¸ªå¾—æ»¡åˆ† |
| **æ€»åˆ†** | **â‰¥0.6** | è¾¾æ ‡è‡ªåŠ¨ approved |

**è¯„åˆ†å…¬å¼**:
- rc >= 3 â†’ 0.50 | rc >= 2 â†’ 0.40 | rc >= 1 â†’ 0.25
- gp >= 2 â†’ 0.50 | gp >= 1 â†’ 0.35

**é‡æ–°æŠ½å–**: score < 0.6 æ—¶è§¦å‘ RE_EXTRACTION_PROMPT è¡¥å……æŠ½å–ï¼ˆæœ€å¤š1æ¬¡ï¼‰

### Scenario å‘å¸ƒå†³ç­–

| æ¡ä»¶ | åŠ¨ä½œ |
|------|------|
| Q >= 0.6 AND S < 0.5 | æ–°å‘å¸ƒ |
| Q >= 0.7 AND S >= 0.5 | èåˆå‘å¸ƒ |
| å…¶ä»– | ä¿å­˜å€™é€‰ |

**ç›¸ä¼¼åº¦è®¡ç®—**: è§¦å‘è¯Jaccard(60%) + åç§°ç›¸ä¼¼(20%) + SOPç›¸ä¼¼(20%)

---

## Stage 4M: æ‰‹å·¥æŠ½å–

> **è§¦å‘æ¡ä»¶**: LLM API ä¸å¯ç”¨ã€éœ€è¦ç²¾ç»†æ§åˆ¶ã€ç‰¹å®šç« èŠ‚å¤„ç†

### æ ¸å¿ƒåŸåˆ™

âš ï¸ **åˆ†ç¦»"è¯†åˆ«"å’Œ"å»é‡"ï¼Œé˜²æ­¢ç¡®è®¤åè§**
- Phase 1-3: çº¯ç²¹è¯†åˆ«ï¼Œ**ç¦æ­¢å‚è€ƒå·²æœ‰è®°å½•**
- Phase 4-6: åŠ è½½å·²æœ‰è®°å½•ï¼Œè¿›è¡Œå»é‡å’Œè¯„ä¼°

### Phase 1: å‡†å¤‡é˜¶æ®µ

```
â˜ æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
â˜ è®¡ç®—æºæ–‡ä»¶ MD5
â˜ å¢é‡æ£€æµ‹: MD5 æœªå˜ä¸”éå¼ºåˆ¶æ¨¡å¼ â†’ è·³è¿‡
âš ï¸ ç¦æ­¢åŠ è½½ extraction_records.json çš„ cases/scenarios
```

**è¾“å‡º**:
```
â˜ æºæ–‡ä»¶è·¯å¾„: [path]
â˜ æºæ–‡ä»¶ MD5: [hash]
â˜ å¢é‡æ£€æµ‹ç»“æœ: [éœ€å¤„ç†/è·³è¿‡]
```

### Phase 2: åˆ†æ®µä¸åˆ‡å—

**ç›®æ ‡**ï¼šæŒ‰å­—ç¬¦æ•° + ç« èŠ‚é€»è¾‘åˆ‡å—ï¼Œä¸ºå¹¶è¡Œå¤„ç†åšå‡†å¤‡

```
2.1 è¯»å–æºæ–‡ä»¶ï¼Œè·å–æ€»è¡Œæ•°å’Œå­—ç¬¦æ•°
2.2 è¯†åˆ«ç« èŠ‚è¾¹ç•Œ (grep "^#" æˆ–å…¶ä»–æ ‡é¢˜æ¨¡å¼)
2.3 åˆ‡å—ç­–ç•¥:
    â€¢ æ¯ä¸ª chunk ç›®æ ‡å¤§å°: ~50K å­—ç¬¦ (~20K tokens)
    â€¢ ä¼˜å…ˆåœ¨ç« èŠ‚è¾¹ç•Œåˆ‡åˆ†
    â€¢ å¦‚æœå•ç« èŠ‚è¶…è¿‡ 50K å­—ç¬¦ï¼Œåœ¨æ®µè½è¾¹ç•Œåˆ‡åˆ†
2.4 ç”Ÿæˆ chunks åˆ—è¡¨ï¼Œæ¯ä¸ª chunk åŒ…å«:
    â€¢ chunk_index: åºå·
    â€¢ chunk_name: ç« èŠ‚åæˆ–èŒƒå›´æè¿°
    â€¢ file_path: æºæ–‡ä»¶ç»å¯¹è·¯å¾„
    â€¢ start_line: èµ·å§‹è¡Œå·
    â€¢ end_line: ç»“æŸè¡Œå·
2.5 åˆ›å»º work_state.json (ç”¨äºæ–­ç‚¹ç»­ä¼ å’Œç»“æœæ±‡æ€»)
```

**è¾“å‡º**:
```
â˜ æºæ–‡ä»¶: [path]
â˜ æ€»å¤§å°: [X KB], [Y è¡Œ], ä¼°ç®— tokens: [Z K]
â˜ åˆ‡å—æ•°é‡: [N]
â˜ chunks åˆ—è¡¨:
  | index | name | lines | ä¼°ç®—tokens |
  |-------|------|-------|------------|
  | 0 | å‰è¨€ | 1-299 | ~5K |
  | 1 | ç¬¬ä¸€ç«  | 300-529 | ~4K |
  | ... | ... | ... | ... |
â˜ work_state.json å·²åˆ›å»º: [path]
```

**work_state.json ç»“æ„** (å¢å¼ºç‰ˆ):
```json
{
  "task_id": "extract_{filename}_{timestamp}",
  "source_file": "/path/to/file.md",
  "source_md5": "hash",
  "status": "in_progress",
  "created_at": "2026-01-15T00:00:00",
  "config": {
    "max_turns": 50,           // subagent æœ€å¤§è½®æ¬¡
    "check_interval": 30,      // ç›‘æ§æ£€æŸ¥é—´éš”(ç§’)
    "timeout_seconds": 300,    // å•ä¸ª chunk è¶…æ—¶(ç§’)
    "max_retries": 2           // æœ€å¤§é‡è¯•æ¬¡æ•°
  },
  "chunks": [
    {
      "index": 0,
      "name": "å‰è¨€",
      "start_line": 1,
      "end_line": 299,
      "status": "pending",     // pending | in_progress | done | timeout | failed
      "task_id": null,         // âœ… æ–°å¢: Task tool è¿”å›çš„ ID
      "agent_id": null,        // âœ… æ–°å¢: ç”¨äº resume çš„ agent ID
      "started_at": null,
      "completed_at": null,
      "last_checked": null,    // âœ… æ–°å¢: ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´
      "retry_count": 0         // âœ… æ–°å¢: é‡è¯•æ¬¡æ•°
    }
  ],
  "result_refs": {
    "cases": [],      // å­˜å‚¨æ–‡ä»¶è·¯å¾„å¼•ç”¨ï¼Œå¦‚ ["cases/chunk0_case001.json"]
    "scenarios": []   // å­˜å‚¨æ–‡ä»¶è·¯å¾„å¼•ç”¨ï¼Œå¦‚ ["scenarios/chunk0_scenario001.json"]
  }
}
```

### Phase 3: å¹¶è¡Œè¯†åˆ«ä¸ç›‘æ§ (Subagent æ¨¡å¼) â˜…

**ç›®æ ‡**ï¼šå¯åŠ¨ subagent å¹¶è¡Œå¤„ç†å„ chunkï¼Œä½¿ç”¨ TaskOutput ç›‘æ§çŠ¶æ€ï¼Œæ”¯æŒè¶…æ—¶é‡è¯•å’Œæ–­ç‚¹æ¢å¤

#### 3.1 å¯åŠ¨ Subagents (åå°æ¨¡å¼)

```
for chunk in pending_chunks:
    result = Task(
        description=f"æŠ½å– chunk {chunk.index} ({chunk.name})",
        prompt=SUBAGENT_PROMPT,  # è§ä¸‹æ–¹
        subagent_type="general-purpose",
        run_in_background=true,   # âœ… åå°è¿è¡Œ
        max_turns=50              # âœ… é™åˆ¶æœ€å¤§è½®æ¬¡ï¼Œé˜²æ­¢æ— é™è¿è¡Œ
    )

    # âœ… ä¿å­˜ task_id åˆ° work_state (å…³é”®!)
    chunk.task_id = result.task_id
    chunk.status = "in_progress"
    chunk.started_at = now()
    save_work_state()
```

**Subagent Prompt æ¨¡æ¿** (ä¼˜åŒ–ç‰ˆ - åˆ†æ®µé˜…è¯» + æ–‡ä»¶å†™å…¥æ¨¡å¼):
```
ä½ æ˜¯çŸ¥è¯†æŠ½å– subagentï¼Œè´Ÿè´£å¤„ç†ä¸€ä¸ª chunkã€‚

ä»»åŠ¡ä¿¡æ¯:
- extracted_dir: {extracted_dir}  # å¦‚ /data/vibelife/knowledge/bazi/extracted
- work_state_path: {extracted_dir}/work_state.json
- chunk_index: {index}
- source_file: {source_file}
- start_line: {start_line}
- end_line: {end_line}
- chunk_name: {chunk_name}

## â˜… åˆ†æ®µé˜…è¯»ç­–ç•¥ (æ ¸å¿ƒ: é¿å… context è†¨èƒ€)

âš ï¸ å†…å­˜ç®¡ç†è§„åˆ™:
1. ç¦æ­¢ä¸€æ¬¡æ€§è¯»å–æ•´ä¸ª chunk
2. ä½¿ç”¨ Read(file, offset=N, limit=300) åˆ†æ®µè¯»å–ï¼Œæ¯æ®µ ~300 è¡Œ
3. æ¯è¯†åˆ«ä¸€ä¸ª case/scenarioï¼Œç«‹å³å†™å…¥æ–‡ä»¶
4. å†™å…¥åä¸å†å¼•ç”¨è¯¥å¯¹è±¡çš„å®Œæ•´å†…å®¹
5. æ¯å¤„ç†å®Œä¸€æ®µï¼Œæ›´æ–° work_state.json çš„ current_offset

æ‰§è¡Œæ­¥éª¤:

### Step 1: åˆ†æ®µé˜…è¯»å¾ªç¯
```python
offset = start_line
case_seq = 0
scenario_seq = 0
BATCH_SIZE = 300  # æ¯æ¬¡è¯»å– 300 è¡Œ

while offset < end_line:
    # è¯»å–ä¸€æ®µ
    content = Read(source_file, offset=offset, limit=BATCH_SIZE)

    # åœ¨å½“å‰æ®µä¸­è¯†åˆ« cases å’Œ scenarios
    # ... è¯†åˆ«é€»è¾‘ ...

    # ç«‹å³å†™å…¥æ–‡ä»¶ (è§ Step 2-3)

    # æ›´æ–°è¿›åº¦ (è§ Step 4)
    offset += BATCH_SIZE
```

### Step 2: è¯†åˆ«å¹¶ç«‹å³å†™å…¥ Case
è¯†åˆ« Case çš„æ¡ä»¶:
â€¢ æ˜¯å¦åŒ…å«å…·ä½“åˆ†ææ¡ˆä¾‹ï¼Ÿ(æœ‰å…«å­—/å‘½ç›˜/å…·ä½“äººç‰©)
â€¢ æ˜¯å¦æœ‰å®Œæ•´æ¨ç†è¿‡ç¨‹ (reasoning_chain)?
â€¢ æ˜¯å¦æœ‰å¯å¤ç”¨æŒ‡å¯¼å»ºè®® (guidance_patterns)?

è¯†åˆ«åˆ° case å:
1. æ„å»º case å¯¹è±¡ï¼Œæ·»åŠ  location_id = f"{source_file}:{chunk_name}:{seq}"
2. ç«‹å³å†™å…¥: Write({extracted_dir}/cases/chunk{index}_case{seq:03d}.json)
3. æ¸…ç©ºå˜é‡ï¼Œä¸å†æŒæœ‰å®Œæ•´ case å¯¹è±¡
4. case_seq += 1

### Step 3: è¯†åˆ«å¹¶ç«‹å³å†™å…¥ Scenario
è¯†åˆ« Scenario çš„æ¡ä»¶:
â€¢ æ˜¯å¦å®šä¹‰äº†å¯æœåŠ¡ç”¨æˆ·çš„æµç¨‹ï¼Ÿ
â€¢ æ˜¯å¦æœ‰æ˜ç¡®è§¦å‘æ¡ä»¶ (primary_triggers)?
â€¢ æ˜¯å¦æœ‰å¯æ‰§è¡Œ SOP æ­¥éª¤ (sop_phases)?

è¯†åˆ«åˆ° scenario å:
1. æ„å»º scenario å¯¹è±¡
2. ç«‹å³å†™å…¥: Write({extracted_dir}/scenarios/chunk{index}_scenario{seq:03d}.json)
3. æ¸…ç©ºå˜é‡
4. scenario_seq += 1

### Step 4: å¢é‡æ›´æ–° work_state.json
æ¯å¤„ç†å®Œä¸€æ®µ (~300è¡Œ)ï¼Œæ›´æ–°è¿›åº¦:
```python
# ä½¿ç”¨ Bash + Python åŸå­æ›´æ–°
Bash: python3 -c "
import json, fcntl
with open('{work_state_path}', 'r+') as f:
    fcntl.flock(f, fcntl.LOCK_EX)
    d = json.load(f)
    d['chunks'][{index}]['current_offset'] = {current_offset}
    d['chunks'][{index}]['processed_cases'] = {case_seq}
    d['chunks'][{index}]['processed_scenarios'] = {scenario_seq}
    f.seek(0); f.truncate()
    json.dump(d, f, ensure_ascii=False)
    fcntl.flock(f, fcntl.LOCK_UN)
"
```

### Step 5: å®Œæˆæ—¶æ›´æ–°çŠ¶æ€
å…¨éƒ¨å¤„ç†å®Œæˆå:
```python
# æ›´æ–° chunk çŠ¶æ€ä¸º doneï¼Œè¿½åŠ æ–‡ä»¶å¼•ç”¨
Bash: python3 -c "
import json, fcntl, glob
with open('{work_state_path}', 'r+') as f:
    fcntl.flock(f, fcntl.LOCK_EX)
    d = json.load(f)
    d['chunks'][{index}]['status'] = 'done'
    # è¿½åŠ æ–‡ä»¶å¼•ç”¨
    case_files = sorted(glob.glob('{extracted_dir}/cases/chunk{index}_case*.json'))
    scenario_files = sorted(glob.glob('{extracted_dir}/scenarios/chunk{index}_scenario*.json'))
    d['result_refs']['cases'].extend([f.split('/')[-1] for f in case_files])
    d['result_refs']['scenarios'].extend([f.split('/')[-1] for f in scenario_files])
    f.seek(0); f.truncate()
    json.dump(d, f, ensure_ascii=False)
    fcntl.flock(f, fcntl.LOCK_UN)
"
```

### Step 6: è¿”å›ç®€çŸ­æ‘˜è¦
è¿”å›æ ¼å¼: "chunk {index} done: {case_seq} cases, {scenario_seq} scenarios"

## ç¦æ­¢è¡Œä¸º

âš ï¸ ç¦æ­¢å‚è€ƒ extraction_records.json
âš ï¸ ç¦æ­¢ä¸€æ¬¡æ€§è¯»å–æ•´ä¸ª chunk (å¿…é¡»åˆ†æ®µ)
âš ï¸ ç¦æ­¢åœ¨ context ä¸­ä¿ç•™å®Œæ•´ case/scenario å¯¹è±¡
âš ï¸ ç¦æ­¢åœ¨è¿”å›å€¼ä¸­åŒ…å«å®Œæ•´çš„ case/scenario å†…å®¹
âš ï¸ ç¦æ­¢è·³è¿‡ä»»ä½• section çš„å®é™…é˜…è¯»
```

#### 3.2 ç›‘æ§å¾ªç¯ (ä½¿ç”¨ TaskOutput) â˜…

```
config = {
    check_interval: 30,      # æ£€æŸ¥é—´éš”(ç§’)
    timeout_seconds: 300,    # å•ä¸ª chunk è¶…æ—¶(5åˆ†é’Ÿ)
    max_retries: 2           # æœ€å¤§é‡è¯•æ¬¡æ•°
}

while has_running_chunks:
    for chunk in chunks where status == "in_progress":

        # âœ… ä½¿ç”¨ TaskOutput éé˜»å¡æ£€æŸ¥çŠ¶æ€
        result = TaskOutput(
            task_id=chunk.task_id,
            block=false,           # éé˜»å¡
            timeout=5000           # 5ç§’è¶…æ—¶
        )

        if result.status == "completed":
            chunk.status = "done"
            chunk.agent_id = result.agent_id  # âœ… ä¿å­˜ç”¨äºæ¢å¤
            chunk.completed_at = now()

        elif result.status == "failed":
            handle_failure(chunk)

        elif result.status == "running":
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if now() - chunk.started_at > timeout_seconds:
                chunk.status = "timeout"
                handle_timeout(chunk)

        save_work_state()

    # æ˜¾ç¤ºè¿›åº¦
    display_progress()

    # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
    sleep(check_interval)
```

#### 3.3 è¶…æ—¶å’Œå¤±è´¥å¤„ç†

```python
def handle_failure(chunk):
    if chunk.retry_count < max_retries and chunk.agent_id:
        # âœ… ä½¿ç”¨ resume æ¢å¤ä¹‹å‰çš„ subagent
        result = Task(
            resume=chunk.agent_id,
            prompt="ç»§ç»­å¤„ç†ï¼Œä¸Šæ¬¡å¯èƒ½å› é”™è¯¯ä¸­æ–­"
        )
        chunk.task_id = result.task_id
        chunk.status = "in_progress"
        chunk.retry_count += 1
    else:
        chunk.status = "failed"

def handle_timeout(chunk):
    if chunk.retry_count < max_retries:
        # é‡æ–°å¯åŠ¨æ–°çš„ subagent
        result = Task(
            description=f"é‡è¯• chunk {chunk.index}",
            prompt=SUBAGENT_PROMPT,
            subagent_type="general-purpose",
            run_in_background=true,
            max_turns=50
        )
        chunk.task_id = result.task_id
        chunk.status = "in_progress"
        chunk.started_at = now()
        chunk.retry_count += 1
    else:
        chunk.status = "failed"
```

#### 3.4 æ–­ç‚¹ç»­ä¼  (ä¼šè¯æ¢å¤å)

```
# æ£€æµ‹åˆ° work_state.json å­˜åœ¨
if work_state.status == "in_progress":
    for chunk in work_state.chunks:
        if chunk.status == "in_progress":
            if chunk.agent_id:
                # âœ… å°è¯•æ¢å¤ä¹‹å‰çš„ subagent
                result = Task(
                    resume=chunk.agent_id,
                    prompt="ç»§ç»­å¤„ç†..."
                )
                chunk.task_id = result.task_id
            else:
                # é‡æ–°å¯åŠ¨
                restart_chunk(chunk)
```

#### 3.5 æ±‡æ€»ç»“æœ (æµå¼å¤„ç†æ¨¡å¼) â˜…

```
â€¢ ä» work_state.result_refs è·å–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
â€¢ é€ä¸ªè¯»å– case/scenario æ–‡ä»¶è¿›è¡Œå¤„ç†ï¿½ï¿½ï¿½é¿å…å…¨é‡åŠ è½½
â€¢ å¤„ç†å®Œä¸€ä¸ªæ–‡ä»¶åç«‹å³é‡Šæ”¾å†…å­˜
```

**Subagent æ–‡ä»¶å†™å…¥ç¤ºä¾‹** (ä¼˜åŒ–ç‰ˆ):
```python
import json, fcntl, os
from datetime import datetime

def save_results_to_files(extracted_dir, chunk_index, cases, scenarios):
    """å°† cases å’Œ scenarios å†™å…¥ç‹¬ç«‹æ–‡ä»¶ï¼Œè¿”å›æ–‡ä»¶è·¯å¾„åˆ—è¡¨"""
    case_refs = []
    scenario_refs = []

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(f"{extracted_dir}/cases", exist_ok=True)
    os.makedirs(f"{extracted_dir}/scenarios", exist_ok=True)

    # å†™å…¥æ¯ä¸ª case åˆ°ç‹¬ç«‹æ–‡ä»¶
    for i, case in enumerate(cases, 1):
        file_path = f"{extracted_dir}/cases/chunk{chunk_index}_case{i:03d}.json"
        with open(file_path, 'w') as f:
            json.dump(case, f, ensure_ascii=False, indent=2)
        case_refs.append(f"cases/chunk{chunk_index}_case{i:03d}.json")

    # å†™å…¥æ¯ä¸ª scenario åˆ°ç‹¬ç«‹æ–‡ä»¶
    for i, scenario in enumerate(scenarios, 1):
        file_path = f"{extracted_dir}/scenarios/chunk{chunk_index}_scenario{i:03d}.json"
        with open(file_path, 'w') as f:
            json.dump(scenario, f, ensure_ascii=False, indent=2)
        scenario_refs.append(f"scenarios/chunk{chunk_index}_scenario{i:03d}.json")

    return case_refs, scenario_refs

def update_work_state(work_state_path, chunk_index, case_refs, scenario_refs):
    """åŸå­æ›´æ–° work_state.jsonï¼Œåªå­˜å‚¨æ–‡ä»¶å¼•ç”¨"""
    with open(work_state_path, 'r+') as f:
        fcntl.flock(f, fcntl.LOCK_EX)
        try:
            data = json.load(f)
            # è¿½åŠ æ–‡ä»¶å¼•ç”¨ (ä¸æ˜¯å®Œæ•´å†…å®¹!)
            data['result_refs']['cases'].extend(case_refs)
            data['result_refs']['scenarios'].extend(scenario_refs)
            # æ›´æ–° chunk çŠ¶æ€
            data['chunks'][chunk_index]['status'] = 'done'
            data['chunks'][chunk_index]['completed_at'] = datetime.now().isoformat()
            data['chunks'][chunk_index]['result_count'] = {
                'cases': len(case_refs),
                'scenarios': len(scenario_refs)
            }
            # å†™å›
            f.seek(0)
            f.truncate()
            json.dump(data, f, ensure_ascii=False, indent=2)
        finally:
            fcntl.flock(f, fcntl.LOCK_UN)
```

**Phase 3 è¾“å‡º**:
```
ğŸ“Š Subagent è¿›åº¦ç›‘æ§
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk â”‚ åç§°             â”‚ çŠ¶æ€     â”‚ Task ID  â”‚ è€—æ—¶    â”‚ é‡è¯•    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0     â”‚ å‰è¨€~ç¬¬å…­ç«       â”‚ âœ… done  â”‚ a1b2c3   â”‚ 2m 30s  â”‚ 0       â”‚
â”‚ 1     â”‚ ç¬¬ä¸ƒç« ~ç¬¬åç«     â”‚ ğŸ”„ è¿è¡Œä¸­â”‚ d4e5f6   â”‚ 1m 45s  â”‚ 0       â”‚
â”‚ 2     â”‚ ç¬¬åä¸€ç« ~ç¬¬äºŒåç« â”‚ ğŸ”„ è¿è¡Œä¸­â”‚ g7h8i9   â”‚ 1m 20s  â”‚ 0       â”‚
â”‚ 3     â”‚ ç¬¬äºŒåä¸€ç« ~ç¬¬29ç« â”‚ âš ï¸ é‡è¯•ä¸­â”‚ j0k1l2   â”‚ 0m 30s  â”‚ 1       â”‚
â”‚ 4     â”‚ ç¬¬30ç«            â”‚ âœ… done  â”‚ m3n4o5   â”‚ 45s     â”‚ 0       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
å®Œæˆ: 2/5 | è¿è¡Œä¸­: 2 | é‡è¯•ä¸­: 1

â˜ æ±‡æ€»ç»“æœ:
  - new_cases[]: [N] ä¸ª (åˆ—å‡ºåç§°å’Œ location_id)
  - new_scenarios[]: [M] ä¸ª
```

### Phase 4: Case å»é‡ä¸è¯„ä¼° (æµå¼å¤„ç†) â˜…

> **å‰ç½®æ¡ä»¶**: æ‰€æœ‰ subagent å®Œæˆåæ‰å¼€å§‹æ­¤é˜¶æ®µ
> æ­¤æ—¶æ‰å…è®¸åŠ è½½å·²æœ‰è®°å½•

```
4.0 ç¡®è®¤ Phase 3 å®Œæˆ
    â€¢ æ£€æŸ¥ work_state.json ä¸­æ‰€æœ‰ chunks çš„ status == "done"
    â€¢ â˜… ä» work_state.result_refs.cases è·å–æ–‡ä»¶è·¯å¾„åˆ—è¡¨ (ä¸åŠ è½½å†…å®¹)

4.1 åŠ è½½ extraction_records.json (åªåŠ è½½ location_id ç´¢å¼•ç”¨äºå»é‡)

4.2 â˜… æµå¼å¤„ç†æ¯ä¸ª case æ–‡ä»¶:
    for case_ref in work_state.result_refs.cases:
        case = load_json(f"{extracted_dir}/{case_ref}")  # å•ä¸ªåŠ è½½
        # å»é‡åˆ¤æ–­
        if case.location_id in existing_ids:
            if content_same: decision = "existing"
            else: decision = "updated"
        else:
            decision = "new"
        # è¯„ä¼°
        score = calculate_quality_score(case)
        case.decision = "approved" if score >= 0.6 else "pending"
        # â˜… ç«‹å³å†™å…¥ç»“æœï¼Œé‡Šæ”¾å†…å­˜
        append_to_phase4_results(case.location_id, decision, score)
        del case  # é‡Šæ”¾å†…å­˜

4.3 æ±‡æ€»ç»Ÿè®¡ (ä¸ä¿ç•™å®Œæ•´ case å¯¹è±¡)
```

**è¾“å‡º**:
```
â˜ åŠ è½½ extraction_records.json: [æˆåŠŸ]
â˜ å·²æœ‰ cases æ•°é‡: [N]
â˜ Case å»é‡ä¸è¯„ä¼°è¡¨:
  | location_id | status | rc | gp | score | decision |
  |-------------|--------|----|----|-------|----------|
  | ...         | new    | 3  | 2  | 1.0   | approved |
```

### Phase 5: Scenario å»é‡ä¸å‘å¸ƒ (æµå¼å¤„ç†) â˜…

```
5.1 åŠ è½½å·²æœ‰ scenarios/*.md æ–‡ä»¶ååˆ—è¡¨ (ä¸åŠ è½½å†…å®¹)

5.2 â˜… æµå¼å¤„ç†æ¯ä¸ª scenario æ–‡ä»¶:
    for scenario_ref in work_state.result_refs.scenarios:
        scenario = load_json(f"{extracted_dir}/{scenario_ref}")  # å•ä¸ªåŠ è½½
        # è®¡ç®—ç›¸ä¼¼åº¦ (åªä¸å·²æœ‰ scenario_id æ¯”è¾ƒ)
        max_sim = calculate_similarity(scenario.scenario_id, existing_ids)
        quality = calculate_quality(scenario)
        # å†³ç­–
        if quality >= 0.6 and max_sim < 0.5:
            decision = "new"
        elif quality >= 0.7 and max_sim >= 0.5:
            decision = "merge"
        else:
            decision = "pending"
        # â˜… ç«‹å³å†™å…¥ç»“æœ
        append_to_phase5_results(scenario.scenario_id, quality, max_sim, decision)
        del scenario  # é‡Šæ”¾å†…å­˜

5.3 æ±‡æ€»ç»Ÿè®¡
```

**è¾“å‡º**:
```
â˜ å·²æœ‰ scenarios æ•°é‡: [N]
â˜ Scenario å»é‡ä¸è¯„ä¼°è¡¨:
  | scenario_id | quality | max_sim | decision |
  |-------------|---------|---------|----------|
  | ...         | 0.75    | 0.3     | new      |
```

### Phase 6: ä¿å­˜ä¸åŒæ­¥ (æµå¼å¤„ç†) â˜…

```
6.1 â˜… æµå¼åŒæ­¥ approved cases åˆ° DB:
    for case_ref in approved_case_refs:
        case = load_json(f"{extracted_dir}/{case_ref}")
        sync_to_db(case)
        del case

6.2 â˜… æµå¼å‘å¸ƒ approved scenarios:
    for scenario_ref in approved_scenario_refs:
        scenario = load_json(f"{extracted_dir}/{scenario_ref}")
        publish_to_md(scenario)
        del scenario

6.3 æ›´æ–° stats
6.4 ä¿å­˜ extraction_records.json
6.5 æ¸…ç†ä¸´æ—¶æ–‡ä»¶:
    â€¢ åˆ é™¤ extracted/cases/*.json
    â€¢ åˆ é™¤ extracted/scenarios/*.json
    â€¢ åˆ é™¤ work_state.json (å¯é€‰ä¿ç•™ç”¨äºå®¡è®¡)
```

**è¾“å‡º**:
```
â˜ DB åŒæ­¥: [æ‰§è¡Œ/è·³è¿‡] (new: N, updated: M)
â˜ Scenario å‘å¸ƒ: [æ‰§è¡Œ/è·³è¿‡] (new: N, merged: M)
â˜ Stats æ›´æ–°: total_cases=X, approved=Y, pending=Z
â˜ extraction_records.json ä¿å­˜: [æˆåŠŸ/å¤±è´¥]
â˜ ä¸´æ—¶æ–‡ä»¶æ¸…ç†: [å®Œæˆ]
```

### æ•°æ®ç»“æ„å‚è€ƒ

**Case**:
```json
{
  "name": "æ¡ˆä¾‹åç§°",
  "core_data": {"year": "ç”²å­", "month": "...", "day": "...", "hour": "...", "gender": "ç”·/å¥³"},
  "features": {"daymaster": "...", "strength": "...", "pattern": "...", "key_gods": []},
  "thinking_frameworks_used": ["æ¶æ„1", "æ¶æ„2"],
  "reasoning_chain": [
    {"step": 1, "framework": "...", "observation": "...", "analysis": "...", "conclusion": "..."}
  ],
  "guidance_patterns": [
    {"pattern_name": "...", "condition": "...", "advice": "...", "source": "..."}
  ]
}
```

**Scenario**:
```json
{
  "scenario_id": "snake_case_id",
  "name": "ä¸­æ–‡åç§°",
  "level": "entry|standard|professional",
  "billing": "free|basic|premium",
  "primary_triggers": ["è§¦å‘è¯1", "è§¦å‘è¯2"],
  "sop_phases": [
    {"phase": 1, "name": "é˜¶æ®µå", "type": "required|optional|reactive", "tools": []}
  ]
}
```

### ç¦æ­¢è¡Œä¸º

- âŒ Phase 1-3 åŠ è½½æˆ–å‚è€ƒå·²æœ‰ cases/scenarios
- âŒ è·³è¿‡ä»»ä½• section çš„å®é™…é˜…è¯»
- âŒ ç”¨"å·²æœ‰è¦†ç›–"ä½œä¸ºè·³è¿‡è¯†åˆ«çš„ç†ç”±
- âŒ å‡è®¾"è¿™æ˜¯ç†è®ºç« èŠ‚"è€Œä¸å®é™…é˜…è¯»
- âŒ Phase 6 åªä¿å­˜ JSON è€Œä¸åŒæ­¥ DB
- âŒ **Subagent è¿”å›å®Œæ•´ case/scenario å†…å®¹** (å¿…é¡»å†™å…¥æ–‡ä»¶ï¼Œåªè¿”å›çŠ¶æ€æ‘˜è¦)
- âŒ **work_state.json å­˜å‚¨å®Œæ•´ç»“æœå¯¹è±¡** (åªå­˜å‚¨æ–‡ä»¶è·¯å¾„å¼•ç”¨)
- âŒ **Phase 4-6 å…¨é‡åŠ è½½æ‰€æœ‰ç»“æœåˆ°å†…å­˜** (å¿…é¡»æµå¼å¤„ç†)

---

## Stage 6: è´¨é‡æ£€æŸ¥

| æŒ‡æ ‡ | ç›®æ ‡ |
|------|------|
| çŸ¥è¯†è¦†ç›–ç‡ | â‰¥300 chunks |
| æ¡ˆä¾‹åˆ†å¸ƒ | â‰¥50 cases |
| æ£€ç´¢è´¨é‡ | â‰¥70% |

---

## å·¥å…·å®šä¹‰

### use_skill

```python
{
  "name": "use_skill",
  "description": "æ¿€æ´»ä¸“ä¸šæŠ€èƒ½å¹¶é€‰æ‹©æœåŠ¡åœºæ™¯",
  "parameters": {
    "skill": {"type": "string", "enum": ["bazi", "zodiac", "career", "tarot"]},
    "scenario": {"type": "string", "description": "åœºæ™¯ID"},
    "confidence": {"type": "string", "enum": ["high", "medium", "low"]}
  }
}
```

**bazi scenarios**:
| scenario | é€‚ç”¨æƒ…å†µ |
|----------|---------|
| basic_reading | çœ‹å‘½ç›˜ã€ç®—å‘½ã€åˆæ¬¡å’¨è¯¢ |
| personality | æ€§æ ¼åˆ†æã€å¤©èµ‹ |
| career | äº‹ä¸šã€å·¥ä½œã€èŒä¸š |
| wealth | è´¢è¿ã€æŠ•èµ„ |
| relationship | æ„Ÿæƒ…ã€å©šå§» |
| yearly_fortune | ä»Šå¹´è¿åŠ¿ã€æµå¹´ |
| dayun | å¤§è¿ã€åå¹´è¿ |
| life_blueprint | å®Œæ•´åˆ†æã€äººç”Ÿè§„åˆ’ |

### search_db

```python
{
  "name": "search_db",
  "description": "ä»æ•°æ®åº“æ£€ç´¢çŸ¥è¯†æˆ–æ¡ˆä¾‹",
  "parameters": {
    "table": {"enum": ["knowledge_chunks", "cases"]},
    "query": {"type": "string"},
    "filters": {"type": "object", "description": "skill_id, scenario_id, tags ç­‰"},
    "top_k": {"type": "integer", "default": 5}
  }
}
```

---

## æ¨¡æ¿æ–‡ä»¶

æ‰€æœ‰æ¨¡æ¿ä½äº `templates/` ç›®å½•ï¼š

### æ ¸å¿ƒæ¨¡æ¿

| æ¨¡æ¿ | æ–‡ä»¶ | ç”¨é€” |
|------|------|------|
| SKILL æ¨¡æ¿ | SKILL_TEMPLATE.md | Skill æ ¸å¿ƒå®šä¹‰ |
| Scenario æ¨¡æ¿ | SCENARIO_TEMPLATE.md | MiniSkill åœºæ™¯ |
| å·¥å…·å®šä¹‰æ¨¡æ¿ | TOOLS_TEMPLATE.yaml | å·¥å…· YAML è§„èŒƒ |
| å·¥å…·æ‰§è¡Œå™¨æ¨¡æ¿ | HANDLERS_TEMPLATE.py | @tool_handler |

### Prompt æ¨¡æ¿

| æ¨¡æ¿ | æ–‡ä»¶ | ç”¨é€” |
|------|------|------|
| **ç»Ÿä¸€æŠ½å–** | prompts/UNIFIED_EXTRACTION_PROMPT.md | Stage 4 ä¸» Prompt |
| **é‡æ–°æŠ½å–** | prompts/RE_EXTRACTION_PROMPT.md | Case è¡¥å……æŠ½å– |
| æœ¯è¯­æå– | prompts/TERM_EXTRACTION_PROMPT.md | æœ¯è¯­æå– |

### Schema æ¨¡æ¿

| æ¨¡æ¿ | æ–‡ä»¶ |
|------|------|
| Skill Schema | schemas/SKILL_SCHEMAS.md |
| æ•°æ®åº“ Schema | schemas/DATABASE_SCHEMAS.sql |

### å‰ç«¯æ¨¡æ¿

| æ¨¡æ¿ | æ–‡ä»¶ |
|------|------|
| å¡ç‰‡ç»„ä»¶ | CARD_TEMPLATE.tsx |
| å¡ç‰‡æ³¨å†Œè¡¨ | CARD_REGISTRY.tsx |
| å·¥å…·-å¡ç‰‡æ˜ å°„ | TOOL_CARD_MAPPING.md |

---

## ä½¿ç”¨æ–¹å¼

### CLI å‘½ä»¤

```bash
# å®Œæ•´æµæ°´çº¿
python scripts/build_knowledge.py --skill bazi --stages all

# ç‰¹å®šé˜¶æ®µ
python scripts/build_knowledge.py --skill bazi --stages 0        # æ ¼å¼è½¬æ¢
python scripts/build_knowledge.py --skill bazi --stages 1-3      # å…¥åº“
python scripts/build_knowledge.py --skill bazi --stages 4        # ç»Ÿä¸€æŠ½å–
python scripts/build_knowledge.py --skill bazi --stages 4 --force-reextract  # å¼ºåˆ¶é‡æŠ½
python scripts/build_knowledge.py --skill bazi --stages 6        # è´¨é‡æŠ¥å‘Š

# å¤§æ–‡ä»¶å¤„ç† (è‡ªåŠ¨è°ƒç”¨ vibe-manus)
python scripts/build_knowledge.py --skill bazi --stages 4 --use-manus
```

### API ç«¯ç‚¹

| ç«¯ç‚¹ | æ–¹æ³• | æè¿° |
|------|------|------|
| `/api/knowledge/build` | POST | å¯åŠ¨æ„å»ºæµæ°´çº¿ |
| `/api/knowledge/build/{task_id}` | GET | è·å–ä»»åŠ¡çŠ¶æ€ |
| `/api/knowledge/quality/check` | POST | è¿è¡Œè´¨é‡æ£€æŸ¥ |
| `/api/knowledge/stats/{skill_id}` | GET | è·å–ç»Ÿè®¡ |

---

## é™„å½•

### è‡ªæ£€æµç¨‹

```bash
# Stage 0: æ£€æŸ¥è½¬æ¢æ–‡ä»¶
ls -la /data/vibelife/knowledge/{skill_id}/converted/*.md | wc -l

# Stage 1-3: æ£€æŸ¥å…¥åº“
psql -c "SELECT COUNT(*) FROM knowledge_chunks WHERE skill_id = '{skill_id}';"

# Stage 4: æ£€æŸ¥æŠ½å–
psql -c "SELECT COUNT(*) FROM cases WHERE skill_id = '{skill_id}' AND status = 'approved';"
ls -la apps/api/skills/{skill_id}/scenarios/*.md | wc -l

# Stage 6: è´¨é‡æŠ¥å‘Š
python scripts/build_knowledge.py --skill {skill_id} --stages 6
curl http://localhost:8100/api/knowledge/stats/{skill_id}
```

### æ ¸å¿ƒåŸåˆ™

1. **è´¨é‡ä¼˜å…ˆ**ï¼šåªå…¥åº“é«˜è´¨é‡å†…å®¹
2. **æ·±åº¦ä¼˜å…ˆ**ï¼šæå–å¤§æ¨¡å‹ä¸çŸ¥é“çš„ä¸“ä¸šçŸ¥è¯†
3. **å¯è¿½æº¯**ï¼šæ‰€æœ‰è§„åˆ™æ ‡æ³¨æ¥æº
4. **å¯è¿­ä»£**ï¼šæ”¯æŒå¢é‡æ›´æ–°
5. **æ¨¡å—åŒ–**ï¼šå„é˜¶æ®µå¯ç‹¬ç«‹æ‰§è¡Œ

### ç¦æ­¢è¡Œä¸º

1. ç¦æ­¢åˆ›å»ºéª¨æ¶/å ä½æ–‡ä»¶
2. ç¦æ­¢å…¥åº“ç©ºå†…å®¹ (< 50KB)
3. ç¦æ­¢çŒœæµ‹/ç¼–å†™å†…å®¹
4. ç¦æ­¢è·³è¿‡è´¨é‡è¯„ä¼°
5. ç¦æ­¢æå–å¸¸è¯†æ€§è§„åˆ™
6. ç¦æ­¢æ— æ¥æºçš„è§„åˆ™
