"""
VibeExtractor - 配置驱动 + LLM 执行的 Vibe 提取引擎

设计原则：
1. 配置即规则：所有提取/转换/合并规则在 JSON 配置
2. LLM 即引擎：所有执行逻辑由 LLM 完成，无硬编码算法
3. 代码只做 I/O：代码只负责读配置、调 LLM、写数据库
"""

import json
import re
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
from uuid import UUID
from datetime import datetime

logger = logging.getLogger(__name__)

# 配置文件路径
CONFIG_PATH = Path(__file__).parent.parent.parent / "config" / "vibe_extract.json"


class VibeExtractor:
    """
    配置驱动 + LLM 执行的 Vibe 提取引擎

    代码职责（极简）：
    1. 读取配置
    2. 收集输入数据
    3. 构建 LLM Prompt
    4. 调用 LLM
    5. 写入数据库

    所有业务逻辑由 LLM 根据配置执行
    """

    _instance: Optional["VibeExtractor"] = None
    _config: Optional[Dict] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if self._config is None:
            self._config = self._load_config()

    @property
    def config(self) -> Dict:
        return self._config or {}

    def _load_config(self) -> Dict:
        """加载配置文件"""
        try:
            if CONFIG_PATH.exists():
                with open(CONFIG_PATH, "r", encoding="utf-8") as f:
                    return json.load(f)
            else:
                logger.warning(f"Config file not found: {CONFIG_PATH}")
                return {}
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            return {}

    def reload_config(self):
        """重新加载配置（用于热更新）"""
        self._config = self._load_config()

    async def execute(
        self,
        user_id: UUID,
        trigger: str,
        context: Optional[Dict] = None
    ) -> Dict:
        """
        统一执行入口

        Args:
            user_id: 用户 ID
            trigger: 触发类型 (on_conversation_turn | cron | on_skill_update)
            context: 额外上下文 (如 conversation_id, skill_id, skill_data)

        Returns:
            LLM 输出的结构化结果
        """
        from stores.unified_profile_repo import UnifiedProfileRepository

        # 1. 找到匹配的 extractors
        extractors = self._match_extractors(trigger)

        if not extractors:
            logger.debug(f"No extractors matched for trigger: {trigger}")
            return {}

        results = {}
        for extractor_name, extractor_config in extractors.items():
            try:
                # 2. 收集输入数据
                input_data = await self._collect_input(user_id, extractor_config, context)

                if not input_data:
                    logger.debug(f"No input data for extractor: {extractor_name}")
                    continue

                # 3. 构建 LLM Prompt
                prompt = self._build_prompt(extractor_config, input_data)

                # 4. 调用 LLM
                llm_output = await self._call_llm(prompt, extractor_name)

                if not llm_output:
                    logger.debug(f"No output from LLM for extractor: {extractor_name}")
                    continue

                # 5. 写入数据库
                await self._write_output(user_id, extractor_config, llm_output)

                results[extractor_name] = llm_output
                logger.info(f"Extractor {extractor_name} completed for user {user_id}")

            except Exception as e:
                logger.error(f"Extractor {extractor_name} failed: {e}")

        return results

    def _match_extractors(self, trigger: str) -> Dict[str, Dict]:
        """根据触发类型匹配提取器"""
        extractors = self.config.get("extractors", {})
        matched = {}

        for name, config in extractors.items():
            extractor_trigger = config.get("trigger", "")
            if trigger == extractor_trigger or trigger in extractor_trigger:
                matched[name] = config

        return matched

    async def _collect_input(
        self,
        user_id: UUID,
        extractor_config: Dict,
        context: Optional[Dict]
    ) -> Dict:
        """收集输入数据"""
        from stores.unified_profile_repo import UnifiedProfileRepository
        from stores.message_repo import MessageRepository

        input_config = extractor_config.get("input", {})
        input_data = {}

        # 收集消息
        if "messages" in input_config:
            msg_config = input_config["messages"]
            params = msg_config.get("params", {})
            limit = params.get("limit", 10)

            conversation_id = context.get("conversation_id") if context else None
            if conversation_id:
                messages = await MessageRepository.get_recent_messages(
                    UUID(conversation_id) if isinstance(conversation_id, str) else conversation_id,
                    limit=limit
                )
                input_data["messages"] = [
                    {"role": m.get("role"), "content": m.get("content")}
                    for m in messages
                ]

        # 收集 Skill 数据
        if "skills" in input_config:
            profile = await UnifiedProfileRepository.get_profile(user_id)
            if profile:
                skills_data = profile.get("skills", {})
                skill_fields = input_config["skills"].get("fields", [])
                input_data["skills"] = {
                    k: v for k, v in skills_data.items()
                    if k in skill_fields or not skill_fields
                }

        # 收集当前 Vibe
        if "current_profile" in input_config:
            profile = await UnifiedProfileRepository.get_profile(user_id)
            if profile:
                vibe = profile.get("vibe", {})
                path = input_config["current_profile"].get("path", "")
                if path == "profile":
                    input_data["current_profile"] = vibe.get("profile", {})
                else:
                    input_data["current_vibe"] = vibe

        # 添加上下文中的额外数据
        if context:
            if "skill_data" in context:
                input_data["skill_data"] = context["skill_data"]
            if "skill_id" in context:
                input_data["skill_id"] = context["skill_id"]

        return input_data

    def _build_prompt(self, extractor_config: Dict, input_data: Dict) -> str:
        """
        构建 LLM Prompt

        Prompt 结构：
        1. 配置规则（schema + merge_rules + transform 等）
        2. 输入数据
        3. 执行指令
        """
        prompt_parts = []

        # 1. Schema 定义
        schema = self.config.get("schema", {})
        prompt_parts.append("## Schema 定义")
        prompt_parts.append("```json")
        prompt_parts.append(json.dumps(schema, ensure_ascii=False, indent=2))
        prompt_parts.append("```")

        # 2. 提取规则
        prompt_parts.append("\n## 提取规则")
        prompt_parts.append("```json")
        # 只包含关键配置，避免 prompt 过长
        rule_config = {
            "output": extractor_config.get("output", {}),
            "detect_rules": extractor_config.get("detect_rules", []),
            "rules": extractor_config.get("rules", []),
        }
        prompt_parts.append(json.dumps(rule_config, ensure_ascii=False, indent=2))
        prompt_parts.append("```")

        # 3. 输入数据
        prompt_parts.append("\n## 输入数据")
        prompt_parts.append("```json")
        prompt_parts.append(json.dumps(input_data, ensure_ascii=False, indent=2))
        prompt_parts.append("```")

        # 4. 执行指令
        instruction = extractor_config.get("instruction", "根据规则提取信息，输出 JSON。")
        prompt_parts.append("\n## 执行指令")
        prompt_parts.append(instruction)

        # 5. 输出要求
        prompt_parts.append("\n## 输出要求")
        prompt_parts.append("只输出 JSON，不要其他内容。确保符合 Schema 定义。")
        prompt_parts.append("如果无法提取任何有效信息，返回空对象 {} 或空数组 []。")

        return "\n".join(prompt_parts)

    async def _call_llm(self, prompt: str, extractor_name: str) -> Optional[Dict]:
        """调用 LLM 并解析 JSON 输出"""
        from services.llm.chat import chat

        try:
            response = await chat(
                messages=[{"role": "user", "content": prompt}],
                system="你是一个精确的数据提取和转换引擎。根据配置规则处理数据，输出结构化 JSON。不要添加任何解释或说明，只输出 JSON。",
                capability="fast" if extractor_name in ["realtime", "event_detector"] else "analysis",
                temperature=0.1,  # 低温度确保稳定输出
            )

            # 解析 JSON
            content = response.content if hasattr(response, "content") else str(response)

            # 尝试提取 JSON
            json_match = re.search(r"(\{[\s\S]*\}|\[[\s\S]*\])", content)
            if json_match:
                return json.loads(json_match.group(1))

            logger.warning(f"No JSON found in LLM response for {extractor_name}")
            return None

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON from LLM: {e}")
            return None
        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            return None

    async def _write_output(self, user_id: UUID, config: Dict, output: Any):
        """写入数据库"""
        from stores.unified_profile_repo import UnifiedProfileRepository

        output_config = config.get("output", {})
        target = output_config.get("target", "")
        strategy = output_config.get("strategy", "overwrite")

        if not target or not output:
            return

        if target == "vibe.current":
            await UnifiedProfileRepository.update_vibe_current(user_id, output)
        elif target == "vibe.profile":
            merge_rules = output_config.get("merge_rules", {})
            await UnifiedProfileRepository.update_vibe_profile(user_id, output, merge_rules)
        elif target == "vibe.timeline":
            if output:
                events = output if isinstance(output, list) else [output]
                for event in events:
                    if event:  # 跳过空事件
                        await UnifiedProfileRepository.append_vibe_timeline(user_id, event)
        elif target.startswith("vibe.profile."):
            # 处理嵌套路径，如 vibe.profile.identity
            path = target.replace("vibe.profile.", "")
            await UnifiedProfileRepository.update_vibe_profile_path(user_id, path, output)

    async def build_context(self, user_id: UUID, skill_id: Optional[str] = None) -> str:
        """
        构建 Context 注入字符串

        也由 LLM 执行，根据 context_injection 配置
        """
        from stores.unified_profile_repo import UnifiedProfileRepository
        from services.llm.chat import chat

        # 获取 Vibe 数据
        profile = await UnifiedProfileRepository.get_profile(user_id)
        if not profile:
            return ""

        vibe = profile.get("vibe", {})
        if not vibe:
            return ""

        config = self.config.get("context_injection", {})

        # 应用 skill_overrides
        if skill_id and skill_id in config.get("skill_overrides", {}):
            override = config["skill_overrides"][skill_id]
            vibe = self._filter_vibe(vibe, override)

        # 如果 vibe 数据为空，返回空字符串
        if not vibe.get("current") and not vibe.get("profile"):
            return ""

        prompt = f"""
## 配置
{json.dumps(config, ensure_ascii=False)}

## Vibe 数据
{json.dumps(vibe, ensure_ascii=False)}

## 指令
{config.get('template', '生成用户上下文描述')}

只输出文本，不要 JSON。如果数据为空，返回空字符串。
"""
        try:
            response = await chat(
                messages=[{"role": "user", "content": prompt}],
                system="你是一个精确的文本生成器。根据配置生成简洁的用户上下文描述。",
                capability="fast",
                temperature=0.1,
            )
            content = response.content if hasattr(response, "content") else str(response)
            return content.strip()
        except Exception as e:
            logger.error(f"Failed to build context: {e}")
            return ""

    def _filter_vibe(self, vibe: Dict, override: Dict) -> Dict:
        """根据 skill_overrides 过滤 vibe 数据"""
        include = override.get("include", [])
        exclude = override.get("exclude", [])

        if not include and not exclude:
            return vibe

        filtered = {}

        def get_nested(d: Dict, path: str) -> Any:
            keys = path.split(".")
            result = d
            for key in keys:
                if isinstance(result, dict) and key in result:
                    result = result[key]
                else:
                    return None
            return result

        def set_nested(d: Dict, path: str, value: Any):
            keys = path.split(".")
            current = d
            for key in keys[:-1]:
                if key not in current:
                    current[key] = {}
                current = current[key]
            current[keys[-1]] = value

        # 包含指定路径
        for path in include:
            value = get_nested(vibe, path)
            if value is not None:
                set_nested(filtered, path, value)

        # 如果没有 include，复制全部
        if not include:
            filtered = json.loads(json.dumps(vibe))

        # 排除指定路径
        for path in exclude:
            keys = path.split(".")
            current = filtered
            for key in keys[:-1]:
                if isinstance(current, dict) and key in current:
                    current = current[key]
                else:
                    break
            else:
                if isinstance(current, dict) and keys[-1] in current:
                    del current[keys[-1]]

        return filtered

    async def sync_skill_data(
        self,
        user_id: UUID,
        skill_id: str,
        skill_data: Dict
    ) -> bool:
        """
        Skill 数据同步（Skill 数据更新时调用）

        读取 extractors.skill_sync 配置执行
        """
        return await self.execute(
            user_id,
            "on_skill_update",
            context={"skill_id": skill_id, "skill_data": skill_data}
        ) != {}


# 单例实例
_extractor: Optional[VibeExtractor] = None


def get_extractor() -> VibeExtractor:
    """获取 VibeExtractor 单例"""
    global _extractor
    if _extractor is None:
        _extractor = VibeExtractor()
    return _extractor
