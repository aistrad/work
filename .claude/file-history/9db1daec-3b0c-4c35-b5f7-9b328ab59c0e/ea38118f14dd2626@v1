#!/usr/bin/env python3
"""
Test UsageService functionality

This script tests all UsageService methods to verify:
1. Recording LLM calls with cost estimation
2. Recording skill usage
3. Recording tool calls
4. Recording conversations
5. Querying daily/monthly usage
6. Querying usage by skill
7. Querying usage by model
"""

import asyncio
import sys
from pathlib import Path
from uuid import uuid4, UUID
from datetime import date
import logging

# Add apps/api to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.usage.service import UsageService

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def test_record_llm_call():
    """Test recording LLM calls"""
    logger.info("=" * 80)
    logger.info("Test 1: Record LLM Call")
    logger.info("=" * 80)

    test_user_id = UUID("12345678-1234-5678-1234-567812345678")

    # Test different models
    test_cases = [
        {"model": "gpt-4o", "input": 1000, "output": 500},
        {"model": "glm-4-flash", "input": 2000, "output": 1000},
        {"model": "deepseek-reasoner", "input": 500, "output": 300},
    ]

    for case in test_cases:
        await UsageService.record_llm_call(
            user_id=test_user_id,
            model=case["model"],
            input_tokens=case["input"],
            output_tokens=case["output"],
            skill_id="bazi",
            metadata={"test": True}
        )
        logger.info(f"✓ Recorded {case['model']}: {case['input']}/{case['output']} tokens")

    logger.info("✓ LLM call recording test passed\n")


async def test_record_skill_use():
    """Test recording skill usage"""
    logger.info("=" * 80)
    logger.info("Test 2: Record Skill Use")
    logger.info("=" * 80)

    test_user_id = UUID("12345678-1234-5678-1234-567812345678")

    skills = ["bazi", "zodiac", "tarot"]

    for skill in skills:
        await UsageService.record_skill_use(
            user_id=test_user_id,
            skill_id=skill
        )
        logger.info(f"✓ Recorded skill use: {skill}")

    logger.info("✓ Skill use recording test passed\n")


async def test_record_tool_call():
    """Test recording tool calls"""
    logger.info("=" * 80)
    logger.info("Test 3: Record Tool Call")
    logger.info("=" * 80)

    test_user_id = UUID("12345678-1234-5678-1234-567812345678")

    tools = [
        ("compute_bazi_chart", "bazi"),
        ("compute_natal_chart", "zodiac"),
        ("draw_tarot_cards", "tarot"),
    ]

    for tool_name, skill_id in tools:
        await UsageService.record_tool_call(
            user_id=test_user_id,
            tool_name=tool_name,
            skill_id=skill_id
        )
        logger.info(f"✓ Recorded tool call: {tool_name} (skill: {skill_id})")

    logger.info("✓ Tool call recording test passed\n")


async def test_query_daily_usage():
    """Test querying daily usage"""
    logger.info("=" * 80)
    logger.info("Test 4: Query Daily Usage")
    logger.info("=" * 80)

    test_user_id = UUID("12345678-1234-5678-1234-567812345678")

    usage = await UsageService.get_daily_usage(test_user_id)

    logger.info(f"Today's usage:")
    logger.info(f"  LLM calls: {usage['llm_calls']}")
    logger.info(f"  Conversations: {usage['conversations']}")
    logger.info(f"  Tokens: {usage['tokens']}")
    logger.info(f"  Cost: ${usage['cost']:.4f}")

    logger.info("✓ Daily usage query test passed\n")


async def test_query_monthly_usage():
    """Test querying monthly usage"""
    logger.info("=" * 80)
    logger.info("Test 5: Query Monthly Usage")
    logger.info("=" * 80)

    test_user_id = UUID("12345678-1234-5678-1234-567812345678")

    usage = await UsageService.get_monthly_usage(test_user_id)

    logger.info(f"This month's usage:")
    logger.info(f"  LLM calls: {usage['llm_calls']}")
    logger.info(f"  Conversations: {usage['conversations']}")
    logger.info(f"  Tokens: {usage['tokens']}")
    logger.info(f"  Cost: ${usage['cost']:.4f}")

    logger.info("✓ Monthly usage query test passed\n")


async def test_query_by_skill():
    """Test querying usage by skill"""
    logger.info("=" * 80)
    logger.info("Test 6: Query Usage by Skill")
    logger.info("=" * 80)

    test_user_id = UUID("12345678-1234-5678-1234-567812345678")

    skill_usage = await UsageService.get_usage_by_skill(
        user_id=test_user_id,
        month=date.today()
    )

    logger.info(f"Usage by skill:")
    for item in skill_usage:
        logger.info(f"  {item['skill_id']:10s} - "
                   f"LLM: {item['llm_calls']:3d} | "
                   f"Skill: {item['skill_uses']:3d} | "
                   f"Tool: {item['tool_calls']:3d} | "
                   f"Tokens: {item['tokens']:6d} | "
                   f"Cost: ${item['cost']:.4f}")

    logger.info("✓ Skill usage query test passed\n")


async def test_query_by_model():
    """Test querying usage by model"""
    logger.info("=" * 80)
    logger.info("Test 7: Query Usage by Model")
    logger.info("=" * 80)

    test_user_id = UUID("12345678-1234-5678-1234-567812345678")

    model_usage = await UsageService.get_usage_by_model(
        user_id=test_user_id,
        month=date.today()
    )

    logger.info(f"Usage by model:")
    for item in model_usage:
        logger.info(f"  {item['model']:20s} - "
                   f"Calls: {item['calls']:3d} | "
                   f"Input: {item['input_tokens']:6d} | "
                   f"Output: {item['output_tokens']:6d} | "
                   f"Cost: ${item['cost']:.4f}")

    logger.info("✓ Model usage query test passed\n")


async def test_cost_estimation():
    """Test cost estimation accuracy"""
    logger.info("=" * 80)
    logger.info("Test 8: Cost Estimation")
    logger.info("=" * 80)

    test_user_id = uuid4()

    # Test GPT-4o: $2.5/1M input, $10/1M output
    await UsageService.record_llm_call(
        user_id=test_user_id,
        model="gpt-4o",
        input_tokens=1_000_000,  # 1M tokens
        output_tokens=500_000     # 0.5M tokens
    )

    usage = await UsageService.get_monthly_usage(test_user_id)
    expected_cost = (1.0 * 2.5) + (0.5 * 10.0)  # $2.5 + $5.0 = $7.5

    logger.info(f"GPT-4o cost estimation:")
    logger.info(f"  Input: 1M tokens × $2.5 = $2.50")
    logger.info(f"  Output: 0.5M tokens × $10 = $5.00")
    logger.info(f"  Expected: ${expected_cost:.4f}")
    logger.info(f"  Actual: ${usage['cost']:.4f}")

    if abs(usage['cost'] - expected_cost) < 0.01:
        logger.info("✓ Cost estimation test passed\n")
    else:
        logger.error(f"✗ Cost mismatch: expected ${expected_cost}, got ${usage['cost']}\n")


async def run_all_tests():
    """Run all tests"""
    logger.info("=" * 80)
    logger.info("UsageService Comprehensive Test Suite")
    logger.info("=" * 80)
    logger.info("")

    try:
        await test_record_llm_call()
        await test_record_skill_use()
        await test_record_tool_call()
        await test_query_daily_usage()
        await test_query_monthly_usage()
        await test_query_by_skill()
        await test_query_by_model()
        await test_cost_estimation()

        logger.info("=" * 80)
        logger.info("✓ All tests passed!")
        logger.info("=" * 80)

        return True

    except Exception as e:
        logger.error("=" * 80)
        logger.error(f"✗ Test failed: {e}")
        logger.error("=" * 80)
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(run_all_tests())
    sys.exit(0 if success else 1)
