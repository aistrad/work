/**
 * Model Provider Configuration for Fortune AI
 *
 * Uses OpenAI-compatible provider for GLM API.
 * Can be extended to support Claude, Gemini, etc.
 */
import { createOpenAI } from '@ai-sdk/openai';

// GLM-4.7 via OpenAI-compatible API (zhipu.ai)
export const glmProvider = createOpenAI({
  baseURL: process.env.GLM_BASE_URL || 'https://open.bigmodel.cn/api/paas/v4',
  apiKey: process.env.GLM_API_KEY || ''
});

// Model selection helper
export type ModelName = 'glm-4.7-flash' | 'glm-4-plus' | 'glm-4-long';

export function getModel(name: ModelName = 'glm-4.7-flash') {
  // Use .chat() to force Chat Completions API instead of Responses API
  // GLM (Zhipu AI) only supports /v4/chat/completions, not /v4/responses
  switch (name) {
    case 'glm-4.7-flash':
      return glmProvider.chat('glm-4-flash');
    case 'glm-4-plus':
      return glmProvider.chat('glm-4-plus');
    case 'glm-4-long':
      return glmProvider.chat('glm-4-long');
    default:
      return glmProvider.chat('glm-4-flash');
  }
}

// Select model based on context or user preference
export function selectModel(options?: {
  userPreference?: string;
  contextLength?: number;
}) {
  // Default to flash for speed
  let modelName: ModelName = 'glm-4.7-flash';

  // Use long context model if needed
  if (options?.contextLength && options.contextLength > 100000) {
    modelName = 'glm-4-long';
  }

  // Respect user preference
  if (options?.userPreference === 'deep') {
    modelName = 'glm-4-plus';
  }

  return getModel(modelName);
}
