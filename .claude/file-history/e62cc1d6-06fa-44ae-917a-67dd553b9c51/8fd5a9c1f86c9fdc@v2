"""
LLM 配置管理模块 - models.yaml 作为唯一配置源 (Single Source of Truth)

所有 LLM 相关配置都从 config/models.yaml 读取
环境变量仅用于敏感信息 (API Key)

使用方法:
    from services.llm.config import LLMConfig, get_llm_config

    config = get_llm_config()
    provider = config.get_provider("deepseek")
    selection = config.resolve(user_tier="paid", task="chat")
"""
import os
import re
import yaml
import logging
from typing import Optional, Dict, List, Any
from dataclasses import dataclass, field
from pathlib import Path

logger = logging.getLogger(__name__)

# 配置文件路径
CONFIG_PATH = Path(__file__).parent.parent.parent / "config" / "models.yaml"

# Tier limits for quota control (保持向后兼容)
TIER_LIMITS = {
    "free": {"calls": 50, "tokens": 100000},
    "pro": {"calls": 500, "tokens": 1000000},
    "premium": {"calls": 500, "tokens": 1000000},
    "paid": {"calls": 500, "tokens": 1000000},
    "vip": {"calls": -1, "tokens": -1},
}


@dataclass
class ProviderConfig:
    """Provider 配置 - 从 models.yaml providers 段读取"""
    id: str
    name: str
    base_url: str
    api_key_env: str
    api_protocol: str = "anthropic"  # "anthropic" | "openai"
    test_endpoint: str = "/messages"
    enabled: bool = True
    timeout: int = 120
    backup_urls: List[str] = field(default_factory=list)
    backup_key_env: Optional[str] = None
    openai_base_url: Optional[str] = None

    @property
    def api_key(self) -> str:
        """从环境变量获取 API Key"""
        return os.getenv(self.api_key_env, "")

    @property
    def backup_api_key(self) -> str:
        """从环境变量获取备用 API Key"""
        return os.getenv(self.backup_key_env, "") if self.backup_key_env else ""

    def is_anthropic(self) -> bool:
        """是否使用 Anthropic 协议"""
        return self.api_protocol == "anthropic"


@dataclass
class ModelSelection:
    """模型选择结果 - 保持向后兼容"""
    provider: str
    model: str
    max_tokens: int = 4096
    fallback_chain: List[str] = field(default_factory=list)
    # 新增字段
    provider_config: Optional[ProviderConfig] = None


class LLMConfig:
    """
    LLM 配置管理器 - 单例模式

    从 models.yaml 加载所有配置，提供统一的访问接口
    """
    _instance: Optional["LLMConfig"] = None
    _config: Optional[Dict[str, Any]] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    @classmethod
    def load(cls, force_reload: bool = False) -> Dict[str, Any]:
        """加载配置文件"""
        if cls._config is not None and not force_reload:
            return cls._config

        if not CONFIG_PATH.exists():
            logger.warning(f"配置文件不存在: {CONFIG_PATH}")
            cls._config = {}
            return cls._config

        try:
            with open(CONFIG_PATH, "r", encoding="utf-8") as f:
                raw_config = yaml.safe_load(f)
            cls._config = cls._expand_env_vars(raw_config)
            logger.debug(f"LLM 配置加载完成")
        except Exception as e:
            logger.error(f"加载配置失败: {e}")
            cls._config = {}

        return cls._config

    @classmethod
    def _expand_env_vars(cls, obj: Any) -> Any:
        """递归展开环境变量 ${VAR_NAME:default_value}"""
        if isinstance(obj, str):
            pattern = r'\$\{([^}:]+)(?::([^}]*))?\}'
            def replace(match):
                var_name = match.group(1)
                default = match.group(2) or ""
                return os.getenv(var_name, default)
            return re.sub(pattern, replace, obj)
        elif isinstance(obj, dict):
            return {k: cls._expand_env_vars(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [cls._expand_env_vars(item) for item in obj]
        return obj

    @classmethod
    def reload(cls):
        """重新加载配置"""
        cls._config = None
        cls.load(force_reload=True)

    # ========== Provider 配置 ==========

    @classmethod
    def get_provider(cls, provider_id: str) -> Optional[ProviderConfig]:
        """获取 provider 配置"""
        config = cls.load()
        providers = config.get("providers", {})

        # 支持别名
        if provider_id in ("zhipu",):
            provider_id = "glm"
        elif provider_id == "anthropic":
            provider_id = "claude"

        provider_data = providers.get(provider_id)
        if not provider_data:
            return None

        return ProviderConfig(
            id=provider_id,
            name=provider_data.get("name", provider_id),
            base_url=provider_data.get("base_url", ""),
            api_key_env=provider_data.get("api_key_env", f"{provider_id.upper()}_API_KEY"),
            api_protocol=provider_data.get("api_protocol", "anthropic"),
            test_endpoint=provider_data.get("test_endpoint", "/messages"),
            enabled=provider_data.get("enabled", True),
            timeout=provider_data.get("timeout", 120),
            backup_urls=provider_data.get("backup_urls", []),
            backup_key_env=provider_data.get("backup_key_env"),
            openai_base_url=provider_data.get("openai_base_url"),
        )

    @classmethod
    def get_provider_config(cls, provider: str) -> Dict[str, Any]:
        """获取 provider 原始配置 (向后兼容)"""
        config = cls.load()
        providers = config.get("providers", {})
        return providers.get(provider, {})

    @classmethod
    def get_api_key(cls, provider: str) -> str:
        """获取 provider 的 API Key"""
        p = cls.get_provider(provider)
        return p.api_key if p else ""

    @classmethod
    def get_base_url(cls, provider: str) -> str:
        """获取 provider 的 base URL"""
        p = cls.get_provider(provider)
        return p.base_url if p else ""

    @classmethod
    def get_timeout(cls, provider: str) -> int:
        """获取 provider 的超时时间"""
        p = cls.get_provider(provider)
        return p.timeout if p else 120

    @classmethod
    def is_anthropic_protocol(cls, provider: str) -> bool:
        """判断是否使用 Anthropic 协议"""
        p = cls.get_provider(provider)
        return p.is_anthropic() if p else False

    @classmethod
    def list_providers(cls) -> List[str]:
        """列出所有启用的 provider"""
        config = cls.load()
        providers = config.get("providers", {})
        return [pid for pid, p in providers.items() if p.get("enabled", True)]

    # ========== 模型路由 ==========

    @classmethod
    def resolve(cls, user_tier: str = "free", task: str = "chat") -> ModelSelection:
        """
        解析模型选择

        Args:
            user_tier: 用户层级 (free, paid, pro, premium, vip)
            task: 任务类型 (chat, analysis, vision, image_gen, tools)

        Returns:
            ModelSelection 包含 provider, model, fallback_chain
        """
        config = cls.load()
        if not config:
            return ModelSelection(provider="glm", model="glm-4-flash", fallback_chain=[])

        # 1. 检查 tier 覆盖
        tier_overrides = config.get("tier_overrides", {})
        if user_tier in tier_overrides:
            tier_config = tier_overrides[user_tier].get(task)
            if tier_config:
                primary = tier_config.get("primary")
                fallback = tier_config.get("fallback", [])
                return cls._create_selection(primary, fallback, config)

        # 2. 使用默认配置
        defaults = config.get("defaults", {})
        task_config = defaults.get(task, defaults.get("chat", {}))
        primary = task_config.get("primary", "glm-4-flash")
        fallback = task_config.get("fallback", [])

        return cls._create_selection(primary, fallback, config)

    @classmethod
    def _create_selection(cls, model_id: str, fallback_chain: list, config: dict) -> ModelSelection:
        """从 model_id 创建 ModelSelection"""
        models = config.get("models", {})
        model_config = models.get(model_id, {})

        provider_id = model_config.get("provider", "glm")
        model_name = model_config.get("model_name", model_id)
        provider_config = cls.get_provider(provider_id)

        # 添加全局兜底
        global_fallback = config.get("global_fallback", "glm-4-flash")
        full_chain = list(fallback_chain)
        if global_fallback not in full_chain and global_fallback != model_id:
            full_chain.append(global_fallback)

        return ModelSelection(
            provider=provider_id,
            model=model_name,
            fallback_chain=full_chain,
            provider_config=provider_config
        )

    @classmethod
    def get_model_config(cls, model_id: str) -> Dict[str, Any]:
        """获取模型配置"""
        config = cls.load()
        models = config.get("models", {})
        return models.get(model_id, {})

    @classmethod
    def list_models(cls, provider_id: Optional[str] = None) -> List[str]:
        """列出模型 (可按 provider 过滤)"""
        config = cls.load()
        models = config.get("models", {})
        if provider_id:
            return [mid for mid, m in models.items() if m.get("provider") == provider_id]
        return list(models.keys())

    # ========== Fallback 配置 ==========

    @classmethod
    def get_fallback_triggers(cls) -> Dict[str, Any]:
        """获取 fallback 触发条件"""
        config = cls.load()
        return config.get("fallback_triggers", {
            "on_api_error": True,
            "on_quota_exceeded": True,
            "timeout": 180,
            "max_retries": 2
        })

    @classmethod
    def get_global_fallback(cls) -> str:
        """获取全局兜底模型"""
        config = cls.load()
        return config.get("global_fallback", "glm-4-flash")


# 向后兼容别名
ModelConfig = LLMConfig


def get_model_config() -> LLMConfig:
    """获取 LLMConfig 实例 (向后兼容)"""
    return LLMConfig()


def get_llm_config() -> LLMConfig:
    """获取 LLMConfig 实例 (LLMConfig 本身是单例)"""
    return LLMConfig()
