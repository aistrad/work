# VibeLife V5 Architecture

> Version: 3.2 | 2026-01-13
> 详细文档: [docs/archive/v5/SPEC-v5.md](./docs/archive/v5/SPEC-v5.md)

## 项目概述

VibeLife 是一个 AI 驱动的命理咨询平台，支持八字、星座、塔罗、职业咨询等多种技能。

## 技术架构

```
┌─────────────────────────────────────────────────────────────────┐
│                Python CoreAgent 主导 + AI SDK 6 兼容             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Browser (AI SDK 6)                                             │
│  ├─ useVibeChat + DefaultChatTransport                          │
│  └─ 直接调用 Python API (无 Next.js 中转)                        │
│                                                                 │
│  Python Backend (CoreAgent)                                     │
│  ├─ /chat/v5/stream         → CoreAgent 主入口                  │
│  │      ├─ LLM 智能选择 Skill (无关键词匹配)                     │
│  │      ├─ Tool calling + 执行                                  │
│  │      └─ AI SDK 6 Data Stream Protocol SSE                    │
│  ├─ /bazi/chart             → 八字命盘 API                      │
│  ├─ /zodiac/chart           → 星盘 API                          │
│  ├─ /fortune/*              → 运势数据 API                      │
│  └─ /users/me/profile       → 用户画像                          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## 核心设计原则

### Anthropic Agent 设计原则

1. **Agentic Loop**: "LLMs using tools based on environmental feedback in a loop"
2. **不要用代码硬编码决策逻辑，让 LLM 自己判断**
3. **Tool Use 最佳实践 (ACI)**:
   - 站在模型角度思考工具是否易用
   - 工具定义应包含示例用法、边界情况
   - 参数命名要清晰

### 三大原则

| 原则 | 说明 |
|------|------|
| **Simplicity** | 保持简单，避免过度工程 |
| **Transparency** | 显式展示 agent 规划步骤 |
| **Documentation & Testing** | 精心设计工具文档，充分测试 |

## 关键文件

```
apps/api/
├── routes/chat_v5.py          # V5 Chat 端点
├── services/agent/core.py     # CoreAgent 实现
├── services/llm/client.py     # 统一 LLM 客户端
└── skills/                    # Skill 定义

apps/web/
├── src/hooks/useVibeChat.ts   # AI SDK 6 集成
└── src/components/chat/       # Chat UI 组件
```

## LLM 调用规范

**唯一配置源**: `config/models.yaml`

```
config/models.yaml                    ← 唯一配置源 (Single Source of Truth)
        │
        ├─ services/llm/config.py     ← LLMConfig 配置加载模块
        │
        └─ services/llm/client.py     ← LLMClient 统一调用
```

**使用方法**:
```python
from services.llm.config import LLMConfig

# 获取 provider 配置
provider = LLMConfig.get_provider("deepseek")
print(provider.base_url)       # https://api.deepseek.com/anthropic
print(provider.api_key)        # 从环境变量读取
print(provider.is_anthropic()) # True

# 解析模型路由 (含 fallback 链)
selection = LLMConfig.resolve(user_tier="paid", task="chat")
print(selection.model)         # deepseek-chat
print(selection.fallback_chain) # ['glm-4.7', 'claude-opus', 'glm-4-flash']
```

**验证配置**:
```bash
python scripts/validate_llm_config.py
```

**禁止**:
- 前端硬编码模型名称或 API 地址
- 在代码中硬编码 tier 名称或限制值
- 在 Python 代码中硬编码 base_url 默认值（应从 models.yaml 读取）

## Anthropic 兼容 API 配置

DeepSeek 和 GLM 都提供了 Anthropic 兼容的 API 端点，可以复用 Claude SDK 的消息格式和工具调用逻辑。

### DeepSeek V3

**官方文档**: https://api-docs.deepseek.com/guides/anthropic_api

**端点配置**:
```python
base_url = "https://api.deepseek.com/anthropic"
model = "deepseek-chat"  # 或 deepseek-reasoner
```

**环境变量** (用于 Claude Code CLI):
```bash
export ANTHROPIC_BASE_URL=https://api.deepseek.com/anthropic
export ANTHROPIC_AUTH_TOKEN=${DEEPSEEK_API_KEY}
export ANTHROPIC_MODEL=deepseek-chat
export API_TIMEOUT_MS=600000
```

**Python 调用示例**:
```python
# 使用 Anthropic SDK
import anthropic

client = anthropic.Anthropic(
    base_url="https://api.deepseek.com/anthropic",
    api_key=os.getenv("DEEPSEEK_API_KEY")
)

message = client.messages.create(
    model="deepseek-chat",
    max_tokens=1000,
    messages=[{"role": "user", "content": "Hello"}]
)
```

**支持特性**:
- ✅ 流式响应 (stream)
- ✅ 工具调用 (tool_use, tool_result)
- ✅ System prompt
- ✅ Temperature (0.0 ~ 2.0)
- ❌ 图片/文档输入

### GLM 4.7

**官方文档**: https://docs.bigmodel.cn/cn/coding-plan/tool/claude

**端点配置**:
```python
base_url = "https://open.bigmodel.cn/api/anthropic/v1"  # 注意需要 /v1
model = "glm-4.7"  # 或 GLM-4.5-Air
```

**环境变量** (用于 Claude Code CLI):
```bash
export ANTHROPIC_BASE_URL=https://open.bigmodel.cn/api/anthropic
export ANTHROPIC_AUTH_TOKEN=${GLM_API_KEY}
export ANTHROPIC_MODEL=glm-4.7
export API_TIMEOUT_MS=3000000
```

**Python 调用示例**:
```python
# 使用 Anthropic SDK
import anthropic

client = anthropic.Anthropic(
    base_url="https://open.bigmodel.cn/api/anthropic/v1",
    api_key=os.getenv("GLM_API_KEY")
)

message = client.messages.create(
    model="glm-4.7",
    max_tokens=1000,
    messages=[{"role": "user", "content": "Hello"}]
)
```

**模型映射**:
| Anthropic 模型 | GLM 模型 |
|---------------|---------|
| claude-opus | GLM-4.7 |
| claude-sonnet | GLM-4.7 |
| claude-haiku | GLM-4.5-Air |

### LLMClient 实现

`services/llm/client.py` 中统一处理 Anthropic 兼容 API：

```python
# 支持的 Anthropic 兼容 provider
ANTHROPIC_COMPATIBLE_PROVIDERS = ("claude", "anthropic", "deepseek", "glm", "zhipu")

# base_url 配置
url_map = {
    "deepseek": "https://api.deepseek.com/anthropic",
    "glm": "https://open.bigmodel.cn/api/anthropic/v1",
    "zhipu": "https://open.bigmodel.cn/api/anthropic/v1",
    "claude": os.getenv("CLAUDE_BASE_URL", "https://api.anthropic.com"),
}

# 流式调用自动路由到 Anthropic 格式
async def _stream_provider(self, provider, model, messages, tools, ...):
    if provider in ANTHROPIC_COMPATIBLE_PROVIDERS:
        async for chunk in self._stream_claude(model, messages, tools, max_tokens, provider=provider):
            yield chunk
```

**测试状态**:
| Provider | 模型 | 工具调用 | 流式响应 | 测试状态 |
|----------|------|---------|---------|----------|
| deepseek | deepseek-chat | ✅ | ✅ | ✅ 通过 |
| glm | glm-4.7 | ✅ | ✅ | ✅ 通过 |

## 测试环境

- API: http://127.0.0.1:8100
- Web: http://127.0.0.1:8232
- 启动: `scripts/start-test.sh`

## 参考文档

- [Anthropic Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents)
- [Claude Agent SDK Overview](https://platform.claude.com/docs/en/agent-sdk/overview)
- [Claude Agent SDK Python](https://platform.claude.com/docs/en/agent-sdk/python)

# VibeLife V5 Architecture - 方案 B

> Version: 3.2 | 2026-01-13
> 参考:
> - [Anthropic Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents)
> - [Claude Agent SDK Overview](https://platform.claude.com/docs/en/agent-sdk/overview)
> - [Claude Agent SDK Python](https://platform.claude.com/docs/en/agent-sdk/python)
> - [Claude Agent SDK Demos](https://github.com/anthropics/claude-agent-sdk-demos)

## Claude Agent SDK 核心概念

### SDK vs Client SDK

| 特性 | Client SDK | Agent SDK |
|------|-----------|-----------|
| **工具执行** | 你自己实现 tool loop | Claude 自动处理 |
| **内置工具** | 无 | Read, Write, Edit, Bash, Glob, Grep, WebSearch, WebFetch |
| **适用场景** | 直接 API 调用 | 构建自主 Agent |

```python
# Client SDK: 你自己实现 tool loop
response = client.messages.create(...)
while response.stop_reason == "tool_use":
    result = your_tool_executor(response.tool_use)
    response = client.messages.create(tool_result=result, ...)

# Agent SDK: Claude 自动处理工具
async for message in query(prompt="Fix the bug in auth.py"):
    print(message)
```

### query() vs ClaudeSDKClient

| 特性 | query() | ClaudeSDKClient |
|------|---------|-----------------|
| **Session** | 每次创建新 session | 复用同一 session |
| **对话** | 单次交互 | 多轮对话保持上下文 |
| **中断** | ❌ 不支持 | ✅ 支持 |
| **Hooks** | ❌ 不支持 | ✅ 支持 |
| **自定义工具** | ❌ 不支持 | ✅ 支持 |
| **适用场景** | 一次性任务 | 连续对话 |

### 内置工具

| 工具 | 功能 |
|------|------|
| **Read** | 读取文件 |
| **Write** | 创建文件 |
| **Edit** | 精确编辑文件 |
| **Bash** | 执行终端命令 |
| **Glob** | 按模式查找文件 |
| **Grep** | 正则搜索文件内容 |
| **WebSearch** | 搜索网络 |
| **WebFetch** | 获取网页内容 |
| **AskUserQuestion** | 向用户提问 |

### Subagents (子代理)

```python
# 定义专门的子代理
options = ClaudeAgentOptions(
    allowed_tools=["Read", "Glob", "Grep", "Task"],
    agents={
        "code-reviewer": AgentDefinition(
            description="Expert code reviewer for quality and security reviews.",
            prompt="Analyze code quality and suggest improvements.",
            tools=["Read", "Glob", "Grep"]
        )
    }
)
```

### Hooks (钩子)

在 agent 生命周期的关键点运行自定义代码：

| Hook | 触发时机 |
|------|---------|
| **PreToolUse** | 工具执行前 |
| **PostToolUse** | 工具执行后 |
| **UserPromptSubmit** | 用户提交 prompt 时 |
| **Stop** | 停止执行时 |
| **SubagentStop** | 子代理停止时 |

```python
async def validate_bash_command(input_data, tool_use_id, context):
    if 'rm -rf /' in input_data.get('tool_input', {}).get('command', ''):
        return {
            'hookSpecificOutput': {
                'permissionDecision': 'deny',
                'permissionDecisionReason': 'Dangerous command blocked'
            }
        }
    return {}

options = ClaudeAgentOptions(
    hooks={
        'PreToolUse': [HookMatcher(matcher='Bash', hooks=[validate_bash_command])]
    }
)
```

---

## Anthropic Agent 设计原则

### Workflows vs Agents

| 类型 | 定义 | 适用场景 |
|------|------|---------|
| **Workflows** | LLM 和工具通过**预定义代码路径**编排 | 流程固定、可预测的任务 |
| **Agents** | LLM **动态指导**自己的流程和工具使用 | 需要灵活决策的任务 |

**VibeLife V5 采用 Agent 模式**：CoreAgent 动态选择 Skill、决定是否收集信息、选择合适的工具。

### Agentic Loop 核心

```
┌─────────────────────────────────────────────────────────────┐
│  "LLMs using tools based on environmental feedback in a loop" │
└─────────────────────────────────────────────────────────────┘

用户消息 → LLM 思考 → 工具调用 → 环境反馈 → LLM 继续思考 → ...
                ↑                              │
                └──────────────────────────────┘
```

**关键点**：
- 每步从环境获取 "ground truth"（工具调用结果）
- LLM 根据反馈动态调整下一步行动
- **不要用代码硬编码决策逻辑，让 LLM 自己判断**

### Tool Use 最佳实践 (ACI - Agent-Computer Interface)

1. **站在模型角度思考工具是否易用**
   - 工具定义应包含示例用法、边界情况、输入格式要求
   - 参数命名要清晰，像给初级开发者写 docstring

2. **Poka-yoke（防错设计）**
   - 修改参数使错误更难发生
   - 例：强制使用绝对路径而非相对路径

3. **给模型足够 token "think"**
   - 不要急于让模型输出结果
   - 允许模型在工具调用前思考

4. **格式贴近自然文本**
   - 避免复杂的格式要求（如精确计数、字符串转义）

### 三大原则

| 原则 | 说明 |
|------|------|
| **Simplicity** | 保持简单，避免过度工程 |
| **Transparency** | 显式展示 agent 规划步骤 |
| **Documentation & Testing** | 精心设计工具文档，充分测试 |

### Agentic Loop 实现模式 (来自 anthropic-cookbook)

```python
def agent_loop(task: str, tools: list, max_iterations: int = 10):
    messages = [{"role": "user", "content": task}]

    for _ in range(max_iterations):
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            messages=messages,
            tools=tools
        )

        # 检查是否需要执行工具
        if response.stop_reason == "tool_use":
            tool_results = []
            for block in response.content:
                if block.type == "tool_use":
                    result = execute_tool(block.name, block.input)
                    tool_results.append({
                        "type": "tool_result",
                        "tool_use_id": block.id,
                        "content": result
                    })

            # 添加 assistant 消息和工具结果
            messages.append({"role": "assistant", "content": response.content})
            messages.append({"role": "user", "content": tool_results})
        else:
            # 结束循环，返回最终响应
            return response.content

    return messages[-1]
```

**关键点**：
- `stop_reason == "tool_use"` 判断是否需要执行工具
- 工具结果作为 `tool_result` 添加到消息中
- 循环直到 `stop_reason` 不是 `tool_use`
- **让 LLM 决定何时停止，不要用代码硬编码停止条件**

### Tool 定义规范

```python
tools = [
    {
        "name": "search",
        "description": "Search for information. Include examples and edge cases.",
        "input_schema": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The search query. Example: 'Python async tutorial'"
                }
            },
            "required": ["query"]
        }
    }
]
```

**工具描述最佳实践**：
- 包含示例用法
- 说明边界情况
- 参数命名清晰
- 像给初级开发者写 docstring

---

## 架构说明

**当前使用方案 B**（Python CoreAgent 主导 + AI SDK 6 兼容 SSE）

> 详细设计文档见: [chat-architecture-v5.md](./chat-architecture-v5.md)

## 架构流程

```
┌─────────────────────────────────────────────────────────────────┐
│                Python CoreAgent 主导 + AI SDK 6 兼容             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Browser (AI SDK 6)                                             │
│  ├─ useVibeChat + DefaultChatTransport                          │
│  └─ 直接调用 Python API (无 Next.js 中转)                        │
│                                                                 │
│  Python Backend (CoreAgent)                                     │
│  ├─ /chat/v5/stream         → CoreAgent 主入口                  │
│  │      ├─ LLM 智能选择 Skill (无关键词匹配)                     │
│  │      ├─ Tool calling + 执行                                  │
│  │      └─ AI SDK 6 Data Stream Protocol SSE                    │
│  ├─ /bazi/chart             → 八字命盘 API                      │
│  ├─ /zodiac/chart           → 星盘 API                          │
│  ├─ /fortune/*              → 运势数据 API                      │
│  └─ /users/me/profile       → 用户画像                          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## 核心设计

1. **Python CoreAgent 主导** - 后端统一处理 LLM 调用、Skill 选择、Tool 执行
2. **AI SDK 6 兼容 SSE** - 输出符合 AI SDK 6 Data Stream Protocol 的 SSE 格式
3. **前端零改动** - 复用现有 useVibeChat + DefaultChatTransport
4. **LLM 智能路由** - CoreAgent 通过 LLM 判断用户意图，选择合适的 Skill
5. **统一 LLM 调用** - 所有 LLM 调用通过 `services/llm/client.py`

## Context 构建

```python
# Python: /api/v1/context/build
ContextBuilder.build(
    skill=skill,           # bazi/zodiac
    voice_mode=voice_mode, # warm/sarcastic/wise
    current_message=msg,
    profile=profile,       # 用户画像
    skill_data=skill_data, # 命盘数据
    history=history,       # 对话历史
)
→ (system_prompt, messages)
```

- 话题检测: career/relationship/fortune/self/general
- 智能裁剪: 根据话题选择相关 Profile 字段
- Identity Prism: 始终注入核心画像摘要

## 工具系统

```typescript
// Next.js: 8 个核心工具
const tools = {
  show_bazi_chart,      // 八字命盘
  show_bazi_fortune,    // 大运流年
  show_bazi_kline,      // 运势 K 线
  show_zodiac_chart,    // 星盘
  show_zodiac_transit,  // 行运分析
  show_report,          // 报告展示
  show_relationship,    // 关系分析
  request_info,         // 信息收集
};
```

工具执行调用 Python 数据 API，结果返回前端渲染 Generative UI。

## 文件结构

```
apps/api/
├── routes/
│   ├── context.py        # /context/build API (核心)
│   ├── fortune.py        # 运势数据 API
│   ├── report.py         # 报告 API
│   └── relationship.py   # 关系分析 API
├── services/
│   └── vibe_engine/
│       ├── context.py    # ContextBuilder
│       ├── tools.py      # 工具定义
│       └── profile_cache.py

apps/web/
└── src/app/api/chat/
    └── route.ts          # AI SDK streamText (核心)
```

## 端点

| 端点 | 说明 |
|------|------|
| `/api/v1/context/build` | Context 构建 API (核心) |
| `/api/v1/context/messages/save` | 消息持久化 |
| `/api/v1/context/quota/record` | 配额记录 |
| `/api/v1/bazi/chart` | 八字命盘 API |
| `/api/v1/zodiac/chart` | 星盘 API |
| `/api/v1/zodiac/transit` | 行运分析 API |
| `/api/v1/fortune/*` | 运势数据 API |
| `/api/v1/report/*` | 报告 API |
| `/api/v1/relationship/*` | 关系分析 API |

## LLM 调用规范

**前后端统一使用 `config/models.yaml` 作为唯一配置源**

```
config/models.yaml       ← 唯一配置源
        │
        ├─ Python: services/llm/client.py (LLMClient)
        │
        └─ Next.js: /context/build API 返回 llm_config
           - provider: 提供商 (deepseek/glm/google/openai-compatible)
           - model: 模型 ID
           - base_url: deepseek/glm/openai-compatible 需要
           - api_key: 从后端配置获取（前端无需 process.env）
```

**llm_config 结构**:
```python
class LLMConfig(BaseModel):
    provider: str
    model: str
    base_url: Optional[str] = None
    api_key: str  # 从后端配置获取
```

**Provider 映射规则**:
| provider | 模型 ID 格式 | SDK | base_url | 测试状态 |
|----------|-------------|-----|----------|----------|
| deepseek | `deepseek-chat` (官方) | @ai-sdk/deepseek | 不需要 | ✅ 通过 |
| glm | `glm-4.7` 等 | @ai-sdk/openai-compatible | 必需 | ✅ 通过 |
| google | `gemini-xxx` | @ai-sdk/google | 不需要 | - |

**关键发现**:
- GLM 必须使用 `@ai-sdk/openai-compatible` 而不是 `@ai-sdk/openai`
- 需要升级 `@ai-sdk/openai-compatible` 到 2.0.4（旧版本 0.2.16 不兼容 AI SDK 6）
- DeepSeek 官方 API 使用 `@ai-sdk/deepseek`，火山引擎使用 `@ai-sdk/openai`

**前端代码**:
```typescript
// API key 从后端 llm_config 获取，无需 process.env
function getLLMModel(config: { provider: string; model: string; base_url?: string; api_key: string }) {
  switch (config.provider) {
    case 'deepseek':
      // 官方 API 使用 @ai-sdk/deepseek
      const isOfficialAPI = !config.base_url || config.base_url.includes('api.deepseek.com');
      if (isOfficialAPI) {
        return createDeepSeek({ apiKey: config.api_key })(config.model);
      }
      // 火山引擎等第三方使用 @ai-sdk/openai
      return createOpenAI({ baseURL: config.base_url, apiKey: config.api_key })(config.model);
    case 'glm':
      // GLM 必须使用 @ai-sdk/openai-compatible
      return createOpenAICompatible({
        name: 'glm',
        baseURL: config.base_url,
        apiKey: config.api_key,
      })(config.model);
    case 'google':
      return createGoogleGenerativeAI({ apiKey: config.api_key })(config.model);
  }
}
```

**优势**:
- 修改 API key 只需重启后端，前端无需重启
- 配置集中管理，避免前后端不一致
- 支持运行时动态切换模型

**禁止**:
- 前端硬编码模型名称或 API 地址
- 前端从 process.env 读取 API key（应从 llm_config 获取）

## 数据流

```
用户输入 → useChat → /api/chat (Next.js)
                         │
                         ├─ 0. GET /context/quota/check
                         │      → 配额不足返回 429
                         │
                         ├─ 1. POST /context/build
                         │      ← system_prompt + messages + llm_config
                         │      ← conversation_id (新会话时自动生成)
                         │
                         ├─ 2. streamText(官方 SDK)
                         │      → 根据 llm_config.provider 选择 SDK
                         │
                         ├─ 3. 工具调用 → Python API
                         │      ← 工具结果 → Generative UI
                         │
                         └─ 4. onFinish 回调（带重试）
                                ├─ POST /context/messages/save (并行，重试 2 次)
                                └─ POST /context/quota/record (并行，重试 2 次)
                         │
                         ▼
                    toTextStreamResponse() → 前端渲染
```

## 可靠性设计

### 入口配额检查
- 在调用 LLM 前先检查配额，避免浪费资源
- 配额不足返回 HTTP 429 + `QUOTA_EXCEEDED` 错误码

### 消息保存重试
- `onFinish` 回调中的保存操作带重试机制（最多 3 次）
- 重试间隔：500ms → 1000ms → 1500ms
- 用户消息、助手消息、配额记录并行保存

## 与 AI SDK 6 集成

| 效果 | 实现方式 |
|------|---------|
| 打字机效果 | `streamText` 原生支持 |
| 工具调用 | `tool()` + `execute` |
| Generative UI | 工具结果 + 前端组件 |
| 多轮对话 | Context API 返回 messages |

---

## Skill 系统

```
apps/api/skills/
├── core/     # 始终激活 - Vibe 人格
├── bazi/     # 八字命理
└── zodiac/   # 西方占星
```

Skill 内容通过 ContextBuilder 注入到 system_prompt。

---

## 对话引擎 (DialogueEngine)

> 详细设计见: [dialogue-engine.md](./dialogue-engine.md)

配置文件驱动的对话流程引擎，用于 Onboarding 等特殊场景。

### 架构

```
apps/api/services/dialogue/
├── engine.py              # 流程引擎核心
├── rhythm.py              # 节奏控制
├── extractors.py          # 信息提取器
└── flows/
    ├── onboarding.yaml    # 新用户引导
    └── deep_talk.yaml     # 深度对话
```

### 步骤类型

| 类型 | 说明 |
|------|------|
| `template` | 预写模板，支持变量插值 |
| `generate` | 调用 LLM 生成 |
| `extract` | 从用户输入提取信息 |
| `tool` | 调用前端工具 (Generative UI) |
| `action` | 执行后端动作 |

---

## 权益系统 (Entitlement)

**配置文件**: `apps/api/services/entitlement/service.py`

**单一配置源**:
```python
# SINGLE SOURCE OF TRUTH
TIER_CONFIG = {
    "free": {"daily_limit": 3},
    "paid": {"daily_limit": 200},
}
DEFAULT_TIER = "paid"
```

**Tier 配置**:
| Tier | 每日对话限制 | 说明 |
|------|-------------|------|
| free | 3 | 免费用户 |
| paid | 200 | 付费用户 |

**规范**:
- 所有 tier 相关的值必须从 `TIER_CONFIG` 读取
- 禁止在代码中硬编码 tier 名称或限制值
- 新用户默认 tier 由 `DEFAULT_TIER` 控制
- `upgrade_to_paid()` 和 `downgrade_to_free()` 从配置读取 limit
