"""
LLM 调用性能测试 (Mock)

测试 LLM 调用路径的非网络开销
"""
import pytest
import asyncio
import time
from typing import List
from unittest.mock import AsyncMock, patch, MagicMock


class TestLLMRouterBenchmark:
    """LLM 路由器性能测试"""

    def test_model_config_load(self):
        """模型配置加载 - 目标: < 50ms"""
        from services.model_router.config import load_model_config

        times: List[float] = []
        for _ in range(10):
            start = time.perf_counter()
            config = load_model_config(force_reload=True)
            times.append((time.perf_counter() - start) * 1000)

        avg_ms = sum(times) / len(times)

        print(f"\n  Config load (cold): avg={avg_ms:.3f}ms")
        print(f"  Providers: {len(config.providers)}")
        print(f"  Models: {len(config.models)}")

        assert avg_ms < 50.0, f"Config load avg {avg_ms:.3f}ms exceeds 50ms target"

    def test_model_config_cached(self):
        """模型配置缓存 - 目标: < 0.5ms"""
        from services.model_router.config import get_model_config

        # 预热
        get_model_config()

        times: List[float] = []
        for _ in range(1000):
            start = time.perf_counter()
            get_model_config()
            times.append((time.perf_counter() - start) * 1000)

        avg_ms = sum(times) / len(times)

        print(f"\n  Config load (cached): avg={avg_ms:.6f}ms")

        assert avg_ms < 0.5, f"Cached config load avg {avg_ms:.6f}ms exceeds 0.5ms target"

    def test_fallback_chain(self):
        """Fallback 链获取 - 目标: < 1ms"""
        from services.model_router.config import get_model_config

        config = get_model_config()

        times: List[float] = []
        for _ in range(100):
            start = time.perf_counter()
            chain = config.get_fallback_chain("chat", "free")
            times.append((time.perf_counter() - start) * 1000)

        avg_ms = sum(times) / len(times)

        print(f"\n  get_fallback_chain: avg={avg_ms:.3f}ms")
        print(f"  Chain length: {len(chain) if chain else 0}")

        assert avg_ms < 1.0, f"Fallback chain avg {avg_ms:.3f}ms exceeds 1ms target"

    def test_router_config_lookup(self):
        """路由器配置查找 - 目标: < 5ms"""
        from services.model_router.config import get_model_config

        config = get_model_config()

        times: List[float] = []
        for _ in range(100):
            start = time.perf_counter()
            # 测试配置查找性能
            model = config.get_model("glm:glm-4-flash")
            chain = config.get_fallback_chain("chat", "free")
            times.append((time.perf_counter() - start) * 1000)

        avg_ms = sum(times) / len(times)
        p95_ms = sorted(times)[95]

        print(f"\n  Config lookup: avg={avg_ms:.3f}ms, p95={p95_ms:.3f}ms")
        print(f"  Model found: {model is not None}")
        print(f"  Chain length: {len(chain) if chain else 0}")

        assert avg_ms < 5.0, f"Config lookup avg {avg_ms:.3f}ms exceeds 5ms target"


class TestQuotaBenchmark:
    """配额检查性能测试 (Mock 版)"""

    def test_model_context_creation(self):
        """ModelContext 创建性能 - 目标: < 0.5ms"""
        from services.model_router.models import ModelContext

        times: List[float] = []
        for _ in range(1000):
            start = time.perf_counter()
            context = ModelContext(
                user_id="test_user",
                user_tier="free",
                skill="bazi",
                task="chat"
            )
            times.append((time.perf_counter() - start) * 1000)

        avg_ms = sum(times) / len(times)
        p95_ms = sorted(times)[int(len(times) * 0.95)]

        print(f"\n  ModelContext creation: avg={avg_ms:.6f}ms, p95={p95_ms:.6f}ms")

        assert avg_ms < 0.5, f"Context creation avg {avg_ms:.6f}ms exceeds 0.5ms target"

    def test_model_selection_creation(self):
        """ModelSelection 创建性能 - 目标: < 0.5ms"""
        from services.model_router.models import ModelSelection

        times: List[float] = []
        for _ in range(1000):
            start = time.perf_counter()
            selection = ModelSelection(
                provider="glm",
                model="glm-4-flash",
                model_id="glm:glm-4-flash",
                was_downgraded=False
            )
            times.append((time.perf_counter() - start) * 1000)

        avg_ms = sum(times) / len(times)

        print(f"\n  ModelSelection creation: avg={avg_ms:.6f}ms")

        assert avg_ms < 0.5, f"Selection creation avg {avg_ms:.6f}ms exceeds 0.5ms target"


class TestStreamAdapterBenchmark:
    """流式适配器性能测试"""

    def test_aisdk_v6_adapter(self):
        """AI SDK v6 适配器创建 - 目标: < 1ms"""
        from services.agent.stream_adapter import AISDKv6Adapter, StreamConfig

        times: List[float] = []
        for _ in range(100):
            start = time.perf_counter()
            config = StreamConfig(protocol="ai-sdk-6")
            adapter = AISDKv6Adapter(config)
            times.append((time.perf_counter() - start) * 1000)

        avg_ms = sum(times) / len(times)

        print(f"\n  AI SDK v6 adapter creation: avg={avg_ms:.6f}ms")

        assert avg_ms < 1.0, f"Adapter creation avg {avg_ms:.6f}ms exceeds 1ms target"

    def test_stream_config_creation(self):
        """StreamConfig 创建 - 目标: < 0.1ms"""
        from services.agent.stream_adapter import StreamConfig

        times: List[float] = []
        for _ in range(1000):
            start = time.perf_counter()
            config = StreamConfig(
                protocol="ai-sdk-6",
                include_step_events=True
            )
            times.append((time.perf_counter() - start) * 1000)

        avg_ms = sum(times) / len(times)

        print(f"\n  StreamConfig creation: avg={avg_ms:.6f}ms")

        assert avg_ms < 0.1, f"StreamConfig creation avg {avg_ms:.6f}ms exceeds 0.1ms target"
