"""
Ingestion Worker - Background document processing

Uses PostgreSQL SKIP LOCKED for task queue (replaces Celery)
- Claim pending documents
- Convert to Markdown
- Chunk intelligently
- Generate embeddings
- Store in database
"""
import asyncio
import logging
import os
import uuid
from typing import Optional

from stores.db import get_connection
from services.knowledge.embedding import EmbeddingService
from .converters import DocumentConverter
from .chunker import SmartChunker

logger = logging.getLogger(__name__)

# Worker configuration
WORKER_ID = f"worker-{uuid.uuid4().hex[:8]}"
POLL_INTERVAL = 5  # seconds between polls when idle
BATCH_SIZE = 10    # max embeddings per batch
LOCK_TIMEOUT = 30  # minutes before lock expires


class IngestionWorker:
    """
    Background worker for document ingestion

    Uses PostgreSQL's SKIP LOCKED pattern for distributed task queue
    """

    def __init__(
        self,
        worker_id: Optional[str] = None,
        poll_interval: int = POLL_INTERVAL,
    ):
        self.worker_id = worker_id or WORKER_ID
        self.poll_interval = poll_interval
        self.converter = DocumentConverter()
        self.chunker = SmartChunker()
        self._running = False
        self._task: Optional[asyncio.Task] = None

    async def start(self):
        """Start the worker loop"""
        if self._running:
            logger.warning(f"Worker {self.worker_id} already running")
            return

        self._running = True
        logger.info(f"Starting ingestion worker: {self.worker_id}")
        self._task = asyncio.create_task(self._run_loop())

    async def stop(self):
        """Stop the worker gracefully"""
        self._running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info(f"Worker {self.worker_id} stopped")

    async def _run_loop(self):
        """Main worker loop"""
        while self._running:
            try:
                # Try to claim a document
                doc = await self._claim_document()

                if doc:
                    await self._process_document(doc)
                else:
                    # No work available, sleep
                    await asyncio.sleep(self.poll_interval)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Worker loop error: {e}", exc_info=True)
                await asyncio.sleep(self.poll_interval)

    async def _claim_document(self) -> Optional[dict]:
        """Claim a pending document using SKIP LOCKED"""
        async with get_connection() as conn:
            row = await conn.fetchrow(
                "SELECT * FROM claim_pending_document($1, $2)",
                self.worker_id,
                LOCK_TIMEOUT
            )
            if row:
                return dict(row)
            return None

    async def _process_document(self, doc: dict):
        """Process a single document"""
        doc_id = doc["id"]
        file_path = doc["file_path"]
        skill_id = doc["skill_id"]

        logger.info(f"Processing: {doc['filename']} (skill: {skill_id})")

        try:
            # Step 1: Convert to Markdown
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"File not found: {file_path}")

            md_content = self.converter.convert(file_path)
            logger.debug(f"Converted to {len(md_content)} chars of Markdown")

            # Step 2: Chunk
            chunks = self.chunker.chunk(md_content)
            logger.debug(f"Split into {len(chunks)} chunks")

            if not chunks:
                logger.warning(f"No chunks generated for {doc['filename']}")
                await self._complete_document(doc_id, 0, md_content)
                return

            # Step 3: Generate embeddings in batches
            chunk_data = []
            for chunk in chunks:
                # Preprocess for search
                search_text = self.chunker.preprocess_for_search(chunk.content)

                chunk_data.append({
                    "content": chunk.content,
                    "search_text_preprocessed": search_text,
                    "section_path": chunk.section_path,
                    "section_title": chunk.section_title,
                    "has_table": chunk.has_table,
                    "has_list": chunk.has_list,
                    "char_count": chunk.char_count,
                    "chunk_index": chunk.chunk_index,
                })

            # Generate embeddings
            contents = [c["content"] for c in chunk_data]
            embeddings = await self._batch_embed(contents)

            for i, embedding in enumerate(embeddings):
                chunk_data[i]["embedding"] = embedding

            # Step 4: Store in database
            await self._store_chunks(doc_id, skill_id, chunk_data)

            # Step 5: Mark complete
            await self._complete_document(doc_id, len(chunks), md_content)
            logger.info(f"Completed: {doc['filename']} ({len(chunks)} chunks)")

        except Exception as e:
            logger.error(f"Failed to process {doc['filename']}: {e}", exc_info=True)
            await self._fail_document(doc_id, str(e))

    async def _batch_embed(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings in batches"""
        embeddings = []

        for i in range(0, len(texts), BATCH_SIZE):
            batch = texts[i:i + BATCH_SIZE]
            batch_embeddings = await EmbeddingService.embed_batch(batch)
            embeddings.extend(batch_embeddings)

        return embeddings

    async def _store_chunks(
        self,
        doc_id: uuid.UUID,
        skill_id: str,
        chunks: list[dict]
    ):
        """Store chunks in database"""
        async with get_connection() as conn:
            # Delete any existing chunks for this document
            await conn.execute(
                "DELETE FROM knowledge_chunks_v2 WHERE document_id = $1",
                doc_id
            )

            # Insert new chunks
            import json
            for chunk in chunks:
                # Convert embedding list to vector string format for pgvector
                embedding_str = "[" + ",".join(map(str, chunk["embedding"])) + "]"
                await conn.execute(
                    """
                    INSERT INTO knowledge_chunks_v2 (
                        document_id, skill_id, chunk_index, content,
                        section_path, section_title,
                        has_table, has_list, char_count,
                        search_text_preprocessed, embedding,
                        metadata
                    )
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11::vector, $12::jsonb)
                    """,
                    doc_id,
                    skill_id,
                    chunk["chunk_index"],
                    chunk["content"],
                    chunk["section_path"],
                    chunk["section_title"],
                    chunk["has_table"],
                    chunk["has_list"],
                    chunk["char_count"],
                    chunk["search_text_preprocessed"],
                    embedding_str,
                    json.dumps({})  # metadata as JSON string
                )

    async def _complete_document(
        self,
        doc_id: uuid.UUID,
        chunk_count: int,
        md_content: Optional[str] = None
    ):
        """Mark document as completed"""
        async with get_connection() as conn:
            await conn.execute(
                "SELECT complete_document($1, $2, $3)",
                doc_id,
                chunk_count,
                md_content
            )

    async def _fail_document(self, doc_id: uuid.UUID, error_msg: str):
        """Mark document as failed"""
        async with get_connection() as conn:
            await conn.execute(
                "SELECT fail_document($1, $2)",
                doc_id,
                error_msg[:1000]  # Truncate long errors
            )

    # ─────────────────────────────────────────────────────────────────
    # Manual Processing (for CLI/scripts)
    # ─────────────────────────────────────────────────────────────────

    async def process_file(
        self,
        file_path: str,
        skill_id: str,
    ) -> dict:
        """
        Process a single file directly (bypassing queue)

        Returns:
            dict with document_id and chunk_count
        """
        import hashlib
        from pathlib import Path

        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        # Calculate file hash
        with open(file_path, "rb") as f:
            file_hash = hashlib.md5(f.read()).hexdigest()

        file_type = self.converter.get_file_type(file_path)
        if not file_type:
            raise ValueError(f"Unsupported file type: {path.suffix}")

        # Upsert document
        async with get_connection() as conn:
            doc_id = await conn.fetchval(
                "SELECT upsert_document($1, $2, $3, $4, $5, $6)",
                skill_id,
                path.name,
                str(path.absolute()),
                file_hash,
                file_type,
                path.stat().st_size
            )

        # Process immediately
        doc = {
            "id": doc_id,
            "filename": path.name,
            "file_path": str(path.absolute()),
            "skill_id": skill_id,
            "file_type": file_type,
        }

        await self._process_document(doc)

        # Get final chunk count
        async with get_connection() as conn:
            chunk_count = await conn.fetchval(
                "SELECT chunk_count FROM knowledge_documents WHERE id = $1",
                doc_id
            )

        return {
            "document_id": str(doc_id),
            "chunk_count": chunk_count,
        }

    async def get_stats(self) -> dict:
        """Get processing statistics"""
        async with get_connection() as conn:
            rows = await conn.fetch("SELECT * FROM knowledge_stats")
            return [dict(row) for row in rows]
