"""
SkillLoader v7 - æ”¯æŒ Agentic + Rule æ¶æ„çš„ Skill åŠ è½½å™¨

v7 æ¶æ„ï¼š
- SKILL.md: æ ¸å¿ƒå®šä¹‰ï¼ˆä¸“å®¶èº«ä»½ã€èƒ½åŠ›ç´¢å¼•ã€ä¼¦ç†è¾¹ç•Œï¼‰
- rules/*.md: è§„åˆ™æ–‡ä»¶ï¼ˆåˆ†æè¦ç‚¹ã€è¾“å‡ºè¦æ±‚ã€å¸¸è§é—®é¢˜ï¼‰
- scenarios/*.md: (æ—§æ¶æ„å…¼å®¹) MiniSkill åœºæ™¯æ–‡ä»¶
- tools/*.py: å·¥å…·å®šä¹‰ï¼ˆæ”¶é›†/è®¡ç®—/å±•ç¤º/æ£€ç´¢ï¼‰
"""
import re
import json
import logging
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from functools import lru_cache

logger = logging.getLogger(__name__)

SKILLS_DIR = Path(__file__).parent.parent.parent / "skills"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•°æ®ç»“æ„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class SkillConfig:
    """Skill æ ¸å¿ƒé…ç½®ï¼ˆä» SKILL.md åŠ è½½ï¼‰"""
    id: str
    name: str
    description: str
    expert_persona: str
    scenarios: Dict[str, List[str]]  # {entry: [...], standard: [...], professional: [...]}
    ethics: Dict[str, Any]
    tools: List[str]
    default_scenario: str = "basic_reading"
    triggers: List[str] = field(default_factory=list)
    global_tools: List[str] = field(default_factory=list)  # éœ€è¦çš„å…¨å±€å·¥å…·
    # v7.1: SOP é…ç½®é©±åŠ¨
    requires_birth_info: bool = False  # æ˜¯å¦éœ€è¦å‡ºç”Ÿä¿¡æ¯
    requires_compute: bool = False     # æ˜¯å¦éœ€è¦è®¡ç®—ï¼ˆæ’ç›˜ï¼‰
    compute_type: Optional[str] = None # è®¡ç®—ç±»å‹ï¼šbazi/zodiac/tarotï¼ŒNone è¡¨ç¤ºä½¿ç”¨è‡ªèº«
    compute_tool: Optional[str] = None # è®¡ç®—å·¥å…·åï¼ŒNone æ—¶ä½¿ç”¨é»˜è®¤çº¦å®š calculate_{compute_type}
    collect_tool: Optional[str] = None # æ”¶é›†å·¥å…·åï¼ŒNone æ—¶ä½¿ç”¨é»˜è®¤çº¦å®š request_info


@dataclass
class ServiceItem:
    """æœåŠ¡é¡¹ï¼ˆç”¨äºæœåŠ¡ç›®å½•å±•ç¤ºï¼‰"""
    scenario_id: str
    name: str
    icon: str
    description: str
    tier: str  # entry/standard/professional
    billing: str  # å…è´¹/åŸºç¡€/é«˜çº§
    highlights: List[str] = field(default_factory=list)  # ä»·å€¼ç‚¹


@dataclass
class ScenarioConfig:
    """åœºæ™¯é…ç½®ï¼ˆä» scenarios/*.md åŠ è½½ï¼‰"""
    id: str
    name: str
    level: str  # entry/standard/professional
    billing: str  # free/basic/premium
    description: str
    triggers: Dict[str, List[str]]  # {primary: [...], secondary: [...]}
    prerequisites: List[str]
    sop: List[Dict[str, Any]]  # SOP é˜¶æ®µåˆ—è¡¨
    knowledge_config: Dict[str, Any]  # çŸ¥è¯†æ£€ç´¢é…ç½®
    output_config: Dict[str, Any]  # è¾“å‡ºé…ç½®
    tools: List[str]


@dataclass
class ToolMetadata:
    """å·¥å…·å…ƒæ•°æ®"""
    name: str
    description: str
    tool_type: str  # collect/calculate/display/search
    card_type: Optional[str] = None
    card_props_schema: Optional[Dict] = None
    parameters: Dict[str, Any] = field(default_factory=dict)
    when_to_call: Optional[str] = None


@dataclass
class RuleConfig:
    """è§„åˆ™é…ç½®ï¼ˆä» rules/*.md åŠ è½½ï¼‰- v7 æ–°å¢"""
    id: str
    name: str
    impact: str  # CRITICAL/HIGH/MEDIUM/LOW
    impact_description: str
    tags: List[str]
    content: str  # å®Œæ•´çš„è§„åˆ™å†…å®¹
    analysis_steps: List[Dict[str, Any]]  # åˆ†æè¦ç‚¹è¡¨æ ¼
    output_requirements: str  # è¾“å‡ºè¦æ±‚
    common_questions: str  # å¸¸è§é—®é¢˜


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è§£æå‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_frontmatter(text: str) -> tuple[Dict, str]:
    """è§£æ YAML frontmatter"""
    pattern = r'^---\s*\n(.*?)\n---\s*\n(.*)$'
    match = re.match(pattern, text, re.DOTALL)
    if not match:
        return {}, text

    frontmatter_str, content = match.groups()
    metadata = {}
    current_key = None
    current_list = None

    for line in frontmatter_str.strip().split('\n'):
        stripped = line.strip()
        if not stripped:
            continue
        if stripped.startswith('- ') and current_key:
            if current_list is None:
                current_list = []
            current_list.append(stripped[2:].strip())
            metadata[current_key] = current_list
        elif ':' in line and not line.startswith(' '):
            current_list = None
            key, value = line.split(':', 1)
            current_key = key.strip()
            value = value.strip()
            if value in ('|', ''):
                continue
            elif value.lower() == 'true':
                metadata[current_key] = True
            elif value.lower() == 'false':
                metadata[current_key] = False
            else:
                metadata[current_key] = value

    return metadata, content.strip()


def parse_skill_md(text: str) -> Dict[str, Any]:
    """è§£æ SKILL.md å†…å®¹ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯

    æ”¯æŒä¸¤ç§è§¦å‘è¯æ ¼å¼ï¼š
    1. æ—§æ ¼å¼ï¼šfrontmatter ä¸­çš„ triggers: åˆ—è¡¨
    2. æ–°æ ¼å¼ï¼šdescription ä¸­çš„ "è§¦å‘è¯ï¼šxxxã€xxxã€xxx"
    """
    metadata, content = parse_frontmatter(text)

    # ä» frontmatter è·å–è§¦å‘è¯ï¼ˆæ—§æ ¼å¼ï¼‰
    triggers = metadata.get("triggers", [])

    # å¦‚æœ frontmatter æ²¡æœ‰ triggersï¼Œå°è¯•ä» description ä¸­æå–ï¼ˆæ–°æ ¼å¼ï¼‰
    if not triggers:
        description = metadata.get("description", "")
        # åŒ¹é… "è§¦å‘è¯ï¼šxxxã€xxxã€xxx" æˆ– "è§¦å‘è¯:xxx,xxx,xxx"
        trigger_match = re.search(r'è§¦å‘è¯[ï¼š:]\s*([^ã€‚.]+)', description)
        if trigger_match:
            trigger_str = trigger_match.group(1)
            # æ”¯æŒé¡¿å·ã€é€—å·åˆ†éš”
            triggers = [t.strip() for t in re.split(r'[ã€,ï¼Œ]', trigger_str) if t.strip()]

    result = {
        "id": metadata.get("id", ""),
        "name": metadata.get("name", ""),
        "description": metadata.get("description", ""),
        "triggers": triggers,
        "default_scenario": metadata.get("default_scenario", "basic_reading"),
        "global_tools": metadata.get("global_tools", []),  # ä» frontmatter è¯»å–
        # v7.1: SOP é…ç½®é©±åŠ¨
        "requires_birth_info": metadata.get("requires_birth_info", False),
        "requires_compute": metadata.get("requires_compute", False),
        "compute_type": metadata.get("compute_type", None),
        "compute_tool": metadata.get("compute_tool", None),
        "collect_tool": metadata.get("collect_tool", None),
    }

    # æå–ä¸“å®¶èº«ä»½
    expert_match = re.search(r'## ä¸“å®¶èº«ä»½\s*\n(.*?)(?=\n## |\Z)', content, re.DOTALL)
    result["expert_persona"] = expert_match.group(1).strip() if expert_match else ""

    # æå–åœºæ™¯ç›®å½•
    scenarios = {"entry": [], "standard": [], "professional": []}
    for level in scenarios.keys():
        pattern = rf'### {level.capitalize()}.*?\n\|.*?\n\|.*?\n((?:\|.*?\n)*)'
        match = re.search(pattern, content, re.IGNORECASE | re.DOTALL)
        if match:
            for row in match.group(1).strip().split('\n'):
                cols = [c.strip() for c in row.split('|')[1:-1]]
                if len(cols) >= 2:
                    scenarios[level].append(cols[1])  # æ–‡ä»¶å
    result["scenarios"] = scenarios

    # æå–ä¼¦ç†è¾¹ç•Œ
    ethics = {"forbidden": [], "sensitive": [], "principles": []}
    forbidden_match = re.search(r'### ç»å¯¹ç¦æ­¢\s*\n((?:- .*?\n)*)', content)
    if forbidden_match:
        ethics["forbidden"] = [l[2:].strip() for l in forbidden_match.group(1).strip().split('\n')]
    result["ethics"] = ethics

    # æå–å·¥å…·åˆ—è¡¨
    tools = []
    tools_match = re.search(r'## å·¥å…·åˆ—è¡¨\s*\n\|.*?\n\|.*?\n((?:\|.*?\n)*)', content)
    if tools_match:
        for row in tools_match.group(1).strip().split('\n'):
            cols = [c.strip() for c in row.split('|')[1:-1]]
            if len(cols) >= 1:
                tools.append(cols[0])
    result["tools"] = tools

    return result


def parse_scenario_md(text: str) -> Dict[str, Any]:
    """è§£æ scenario.md å†…å®¹"""
    metadata, content = parse_frontmatter(text)

    result = {
        "id": metadata.get("id", ""),
        "name": metadata.get("name", ""),
        "level": metadata.get("level", "entry"),
        "billing": metadata.get("billing", "free"),
        "description": metadata.get("description", ""),
    }

    # æå–è§¦å‘æ¡ä»¶
    triggers = {"primary": [], "secondary": []}
    primary_match = re.search(r'\*\*ä¸»è¦è§¦å‘è¯\*\*:\s*(.*?)(?:\n|$)', content)
    if primary_match:
        triggers["primary"] = [t.strip() for t in primary_match.group(1).split(',')]
    secondary_match = re.search(r'\*\*æ¬¡è¦è§¦å‘è¯\*\*:\s*(.*?)(?:\n|$)', content)
    if secondary_match:
        triggers["secondary"] = [t.strip() for t in secondary_match.group(1).split(',')]
    result["triggers"] = triggers

    # æå–å‰ç½®è¦æ±‚
    prereq_match = re.search(r'## å‰ç½®è¦æ±‚\s*\n((?:- .*?\n)*)', content)
    result["prerequisites"] = []
    if prereq_match:
        result["prerequisites"] = [l[2:].strip() for l in prereq_match.group(1).strip().split('\n')]

    # æå– SOP é˜¶æ®µ
    sop = []
    phase_pattern = r'### Phase (\d+):\s*(.*?)\n(.*?)(?=### Phase|\Z)'
    for match in re.finditer(phase_pattern, content, re.DOTALL):
        phase_num, phase_name, phase_content = match.groups()
        phase = {
            "phase": int(phase_num),
            "name": phase_name.strip(),
            "content": phase_content.strip()
        }
        # æå–ç±»å‹
        type_match = re.search(r'\*\*ç±»å‹\*\*:\s*(\w+)', phase_content)
        if type_match:
            phase["type"] = type_match.group(1)
        sop.append(phase)
    result["sop"] = sop

    # æå–å·¥å…·åˆ—è¡¨
    tools_match = re.search(r'\*\*ä½¿ç”¨å·¥å…·\*\*:\s*\n((?:- .*?\n)*)', content)
    result["tools"] = []
    if tools_match:
        result["tools"] = [l[2:].strip() for l in tools_match.group(1).strip().split('\n')]

    # çŸ¥è¯†æ£€ç´¢é…ç½®
    result["knowledge_config"] = {}
    result["output_config"] = {}

    return result


def parse_rule_md(text: str) -> Dict[str, Any]:
    """è§£æ rules/*.md å†…å®¹ - v7 æ–°å¢"""
    metadata, content = parse_frontmatter(text)

    # è§£æ tagsï¼ˆæ”¯æŒé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
    tags_raw = metadata.get("tags", [])
    if isinstance(tags_raw, str):
        tags = [t.strip() for t in re.split(r'[,ï¼Œã€]', tags_raw) if t.strip()]
    else:
        tags = tags_raw

    result = {
        "id": metadata.get("id", ""),
        "name": metadata.get("name", ""),
        "impact": metadata.get("impact", "MEDIUM"),
        "impact_description": metadata.get("impactDescription", ""),
        "tags": tags,
        "content": content,
    }

    # æå–åˆ†æè¦ç‚¹è¡¨æ ¼
    analysis_steps = []
    table_match = re.search(r'## åˆ†æè¦ç‚¹\s*\n\|[^\n]+\n\|[-\s|]+\n((?:\|[^\n]+\n)*)', content)
    if table_match:
        for row in table_match.group(1).strip().split('\n'):
            cols = [c.strip() for c in row.split('|')[1:-1]]
            if len(cols) >= 4:
                analysis_steps.append({
                    "step": cols[0],
                    "point": cols[1],
                    "query": cols[2],
                    "priority": cols[3],
                })
    result["analysis_steps"] = analysis_steps

    # æå–è¾“å‡ºè¦æ±‚
    output_match = re.search(r'## è¾“å‡ºè¦æ±‚\s*\n(.*?)(?=\n## |\Z)', content, re.DOTALL)
    result["output_requirements"] = output_match.group(1).strip() if output_match else ""

    # æå–å¸¸è§é—®é¢˜
    faq_match = re.search(r'## å¸¸è§é—®é¢˜\s*\n(.*?)(?=\n## |\Z)', content, re.DOTALL)
    result["common_questions"] = faq_match.group(1).strip() if faq_match else ""

    return result


def parse_skill_services(text: str) -> Dict[str, List[Dict[str, Any]]]:
    """è§£æ SKILL.md ä¸­çš„æœåŠ¡ç›®å½•ï¼Œæå–å±•ç¤ºä¿¡æ¯"""
    services = {"entry": [], "standard": [], "professional": []}

    tier_map = {
        "entry": "entry",
        "standard": "standard",
        "professional": "professional"
    }

    for tier_name, tier_key in tier_map.items():
        # åŒ¹é…è¡¨æ ¼å¤´å’Œå†…å®¹
        pattern = rf'### {tier_name.capitalize()}.*?\n\|[^\n]+\n\|[-\s|]+\n((?:\|[^\n]+\n)*)'
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if not match:
            continue

        table_content = match.group(1).strip()
        if not table_content:
            continue

        for row in table_content.split('\n'):
            cols = [c.strip() for c in row.split('|')[1:-1]]
            if len(cols) < 7:  # è‡³å°‘éœ€è¦ 7 åˆ—ï¼ˆåœºæ™¯ã€æ–‡ä»¶ã€è§¦å‘è¯ã€è®¡è´¹ã€å±•ç¤ºåç§°ã€å›¾æ ‡ã€ç®€ä»‹ï¼‰
                continue

            service = {
                "scenario_id": cols[1].replace('.md', '') if cols[1] else cols[0].lower().replace(' ', '_'),
                "name": cols[4] if len(cols) > 4 and cols[4] else cols[0],
                "icon": cols[5] if len(cols) > 5 and cols[5] else "ğŸ“Œ",
                "description": cols[6] if len(cols) > 6 and cols[6] else "",
                "tier": tier_key,
                "billing": cols[3] if len(cols) > 3 else "å…è´¹",
                "highlights": []
            }

            # è§£æä»·å€¼ç‚¹ï¼ˆç¬¬8åˆ—ï¼Œå¦‚æœå­˜åœ¨ï¼‰
            if len(cols) > 7 and cols[7]:
                service["highlights"] = [h.strip() for h in cols[7].split(',')]

            services[tier_key].append(service)

    return services


def get_skill_services(skill_id: str) -> Optional[Dict[str, Any]]:
    """è·å– Skill çš„æœåŠ¡ç›®å½•æ•°æ®ï¼ˆç”¨äº show_skill_services å·¥å…·ï¼‰"""
    skill_path = SKILLS_DIR / skill_id / "SKILL.md"
    if not skill_path.exists():
        return None

    text = skill_path.read_text(encoding='utf-8')
    metadata, _ = parse_frontmatter(text)
    services = parse_skill_services(text)

    return {
        "skill_id": skill_id,
        "skill_name": metadata.get("name", skill_id),
        "description": metadata.get("description", ""),
        "services": services
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# åŠ è½½å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@lru_cache(maxsize=32)
def load_skill(skill_id: str) -> Optional[SkillConfig]:
    """åŠ è½½ Skill æ ¸å¿ƒé…ç½®"""
    skill_path = SKILLS_DIR / skill_id / "SKILL.md"
    if not skill_path.exists():
        return None

    text = skill_path.read_text(encoding='utf-8')
    data = parse_skill_md(text)

    return SkillConfig(
        id=data.get("id", skill_id),
        name=data.get("name", skill_id),
        description=data.get("description", ""),
        expert_persona=data.get("expert_persona", ""),
        scenarios=data.get("scenarios", {}),
        ethics=data.get("ethics", {}),
        tools=data.get("tools", []),
        default_scenario=data.get("default_scenario", "basic_reading"),
        triggers=data.get("triggers", []),
        global_tools=data.get("global_tools", []),
        # v7.1: SOP é…ç½®é©±åŠ¨
        requires_birth_info=data.get("requires_birth_info", False),
        requires_compute=data.get("requires_compute", False),
        compute_type=data.get("compute_type", None),
        compute_tool=data.get("compute_tool", None),
        collect_tool=data.get("collect_tool", None),
    )


@lru_cache(maxsize=64)
def load_scenario(skill_id: str, scenario_id: str) -> Optional[ScenarioConfig]:
    """åŠ è½½åœºæ™¯é…ç½®"""
    scenario_path = SKILLS_DIR / skill_id / "scenarios" / f"{scenario_id}.md"
    if not scenario_path.exists():
        return None

    text = scenario_path.read_text(encoding='utf-8')
    data = parse_scenario_md(text)

    return ScenarioConfig(
        id=data.get("id", scenario_id),
        name=data.get("name", scenario_id),
        level=data.get("level", "entry"),
        billing=data.get("billing", "free"),
        description=data.get("description", ""),
        triggers=data.get("triggers", {}),
        prerequisites=data.get("prerequisites", []),
        sop=data.get("sop", []),
        knowledge_config=data.get("knowledge_config", {}),
        output_config=data.get("output_config", {}),
        tools=data.get("tools", [])
    )


@lru_cache(maxsize=128)
def load_rule(skill_id: str, rule_id: str) -> Optional[RuleConfig]:
    """åŠ è½½è§„åˆ™é…ç½® - v7 æ–°å¢"""
    rule_path = SKILLS_DIR / skill_id / "rules" / f"{rule_id}.md"
    if not rule_path.exists():
        return None

    text = rule_path.read_text(encoding='utf-8')
    data = parse_rule_md(text)

    return RuleConfig(
        id=data.get("id", rule_id),
        name=data.get("name", rule_id),
        impact=data.get("impact", "MEDIUM"),
        impact_description=data.get("impact_description", ""),
        tags=data.get("tags", []),
        content=data.get("content", ""),
        analysis_steps=data.get("analysis_steps", []),
        output_requirements=data.get("output_requirements", ""),
        common_questions=data.get("common_questions", ""),
    )


def get_skill_rules(skill_id: str) -> List[str]:
    """è·å– Skill çš„æ‰€æœ‰è§„åˆ™ ID - v7 æ–°å¢"""
    rules_dir = SKILLS_DIR / skill_id / "rules"
    if not rules_dir.exists():
        return []
    return [p.stem for p in rules_dir.glob("*.md") if not p.stem.startswith("_")]


def has_rules(skill_id: str) -> bool:
    """æ£€æŸ¥ Skill æ˜¯å¦ä½¿ç”¨ rules æ¶æ„ - v7 æ–°å¢"""
    rules_dir = SKILLS_DIR / skill_id / "rules"
    if not rules_dir.exists():
        return False
    rules = list(rules_dir.glob("*.md"))
    return len(rules) > 0


def get_available_skills() -> List[str]:
    """è·å–æ‰€æœ‰å¯ç”¨çš„ Skill"""
    if not SKILLS_DIR.exists():
        return []
    return [p.name for p in SKILLS_DIR.iterdir()
            if p.is_dir() and (p / "SKILL.md").exists()]


def get_skill_scenarios(skill_id: str) -> List[str]:
    """è·å– Skill çš„æ‰€æœ‰åœºæ™¯

    æ”¯æŒä¸¤ç§æ¶æ„ï¼š
    1. æ—§æ¶æ„ï¼šscenarios/*.md
    2. æ–°æ¶æ„ï¼šrules/*.mdï¼ˆAgentic æ¶æ„ï¼‰
    """
    # ä¼˜å…ˆæ£€æŸ¥ rules ç›®å½•ï¼ˆæ–°æ¶æ„ï¼‰
    rules_dir = SKILLS_DIR / skill_id / "rules"
    if rules_dir.exists():
        rules = [p.stem for p in rules_dir.glob("*.md") if not p.stem.startswith("_")]
        if rules:
            return rules

    # å›é€€åˆ° scenarios ç›®å½•ï¼ˆæ—§æ¶æ„ï¼‰
    scenarios_dir = SKILLS_DIR / skill_id / "scenarios"
    if not scenarios_dir.exists():
        return []
    return [p.stem for p in scenarios_dir.glob("*.md")]


def get_skill_triggers() -> Dict[str, List[str]]:
    """è·å–æ‰€æœ‰ Skill çš„è§¦å‘è¯"""
    triggers = {}
    for skill_id in get_available_skills():
        if skill_id == "core":
            continue
        skill = load_skill(skill_id)
        if skill and skill.triggers:
            triggers[skill_id] = skill.triggers
    return triggers


def get_skill_global_tools(skill_id: str) -> List[str]:
    """è·å– Skill å£°æ˜éœ€è¦çš„å…¨å±€å·¥å…·åˆ—è¡¨"""
    skill = load_skill(skill_id)
    if skill and skill.global_tools:
        return skill.global_tools
    # é»˜è®¤è¿”å›æ‰€æœ‰å…¨å±€å·¥å…·ï¼ˆå‘åå…¼å®¹ï¼‰
    return []


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# v7.1: SOP é…ç½®æŸ¥è¯¢å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def skill_requires_birth_info(skill_id: str) -> bool:
    """æ£€æŸ¥ Skill æ˜¯å¦éœ€è¦å‡ºç”Ÿä¿¡æ¯ï¼ˆé…ç½®é©±åŠ¨ï¼‰"""
    skill = load_skill(skill_id)
    return skill.requires_birth_info if skill else False


def skill_requires_compute(skill_id: str) -> bool:
    """æ£€æŸ¥ Skill æ˜¯å¦éœ€è¦è®¡ç®—/æ’ç›˜ï¼ˆé…ç½®é©±åŠ¨ï¼‰"""
    skill = load_skill(skill_id)
    return skill.requires_compute if skill else False


def get_skill_compute_type(skill_id: str) -> Optional[str]:
    """è·å– Skill çš„è®¡ç®—ç±»å‹ï¼ˆé…ç½®é©±åŠ¨ï¼‰

    è¿”å›å€¼ï¼š
    - None: ä½¿ç”¨è‡ªèº«çš„è®¡ç®—å™¨
    - "zodiac": ä½¿ç”¨ zodiac è®¡ç®—å™¨ï¼ˆå¦‚ jungastroï¼‰
    - "bazi": ä½¿ç”¨ bazi è®¡ç®—å™¨
    """
    skill = load_skill(skill_id)
    return skill.compute_type if skill else None


def get_skill_compute_tool(skill_id: str) -> Optional[str]:
    """è·å– Skill çš„è®¡ç®—å·¥å…·åï¼ˆé…ç½®é©±åŠ¨ï¼‰

    è¿”å›å€¼ï¼š
    - é…ç½®çš„å·¥å…·åï¼ˆå¦‚ "calculate_zodiac"ï¼‰
    - None: ä½¿ç”¨é»˜è®¤çº¦å®š calculate_{compute_type}
    """
    skill = load_skill(skill_id)
    return skill.compute_tool if skill else None


def get_skill_collect_tool(skill_id: str) -> Optional[str]:
    """è·å– Skill çš„æ”¶é›†å·¥å…·åï¼ˆé…ç½®é©±åŠ¨ï¼‰

    è¿”å›å€¼ï¼š
    - é…ç½®çš„å·¥å…·åï¼ˆå¦‚ "collect_birth_info"ï¼‰
    - None: ä½¿ç”¨é»˜è®¤çº¦å®š request_info
    """
    skill = load_skill(skill_id)
    return skill.collect_tool if skill else None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# System Prompt æ„å»º
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_system_prompt(
    skill_id: str,
    scenario_id: Optional[str] = None,
    user_context: Optional[Dict] = None
) -> str:
    """æ„å»ºå®Œæ•´çš„ System Prompt

    v7 æ›´æ–°ï¼šæ”¯æŒ rules æ¶æ„ï¼Œscenario_id å¯ä»¥æ˜¯ rule_id
    """
    parts = []

    # åŠ è½½ Skill
    skill = load_skill(skill_id)
    if not skill:
        return ""

    # ä¸“å®¶èº«ä»½
    parts.append(f"# {skill.name}\n\n{skill.expert_persona}")

    # åŠ è½½åœºæ™¯æˆ–è§„åˆ™
    if scenario_id:
        # v7: ä¼˜å…ˆå°è¯•åŠ è½½è§„åˆ™
        if has_rules(skill_id):
            rule = load_rule(skill_id, scenario_id)
            if rule:
                parts.append(f"\n---\n\n## å½“å‰è§„åˆ™: {rule.name}\n")
                # æ·»åŠ è§„åˆ™å®Œæ•´å†…å®¹
                parts.append(rule.content)
            else:
                # è§„åˆ™ä¸å­˜åœ¨ï¼Œå°è¯•åŠ è½½åœºæ™¯ï¼ˆå‘åå…¼å®¹ï¼‰
                scenario = load_scenario(skill_id, scenario_id)
                if scenario:
                    parts.append(f"\n---\n\n## å½“å‰åœºæ™¯: {scenario.name}\n\n{scenario.description}")
                    if scenario.sop:
                        sop_text = "\n## æœåŠ¡æµç¨‹ (SOP)\n\n"
                        for phase in scenario.sop:
                            sop_text += f"### Phase {phase['phase']}: {phase['name']}\n{phase['content']}\n\n"
                        parts.append(sop_text)
        else:
            # æ—§æ¶æ„ï¼šåŠ è½½åœºæ™¯
            scenario = load_scenario(skill_id, scenario_id)
            if scenario:
                parts.append(f"\n---\n\n## å½“å‰åœºæ™¯: {scenario.name}\n\n{scenario.description}")
                # SOP
                if scenario.sop:
                    sop_text = "\n## æœåŠ¡æµç¨‹ (SOP)\n\n"
                    for phase in scenario.sop:
                        sop_text += f"### Phase {phase['phase']}: {phase['name']}\n{phase['content']}\n\n"
                    parts.append(sop_text)

    # ä¼¦ç†è¾¹ç•Œ
    if skill.ethics.get("forbidden"):
        ethics_text = "\n## ä¼¦ç†è¾¹ç•Œ\n\n### ç»å¯¹ç¦æ­¢\n"
        for item in skill.ethics["forbidden"]:
            ethics_text += f"- {item}\n"
        parts.append(ethics_text)

    # ç”¨æˆ·ä¸Šä¸‹æ–‡ - è‡ªåŠ¨å¤„ç†æ‰€æœ‰å­—æ®µ
    if user_context:
        ctx_text = "\n## ç”¨æˆ·ä¸Šä¸‹æ–‡\n"

        # ç‰¹æ®Šå¤„ç† birth_info
        if user_context.get("birth_info"):
            birth_info = user_context['birth_info']
            ctx_text += f"\n### å‡ºç”Ÿä¿¡æ¯ï¼ˆå·²æ”¶é›†ï¼Œæ— éœ€å†æ¬¡è¯¢é—®ï¼‰\n"
            ctx_text += f"{json.dumps(birth_info, ensure_ascii=False)}\n"
            ctx_text += f"\n**é‡è¦**: ç”¨æˆ·å·²æä¾›å‡ºç”Ÿä¿¡æ¯ï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œæ’ç›˜å’Œåˆ†æï¼Œæ— éœ€è°ƒç”¨ collect_bazi_info å·¥å…·ã€‚\n"

        # ç‰¹æ®Šå¤„ç† portrait
        if user_context.get("portrait"):
            ctx_text += f"\n### ç”¨æˆ·ç”»åƒ\n{user_context['portrait']}\n"

        # ç‰¹æ®Šå¤„ç† extracted (æŠ½å–çš„ç”¨æˆ·ä¿¡æ¯)
        if user_context.get("extracted"):
            extracted = user_context['extracted']
            ctx_text += f"\n### ç”¨æˆ·ä¿¡æ¯ (ä»å†å²å¯¹è¯ä¸­æŠ½å–)\n"
            if extracted.get("facts"):
                ctx_text += f"**åŸºæœ¬ä¿¡æ¯**: {json.dumps(extracted['facts'], ensure_ascii=False)}\n"
            if extracted.get("concerns"):
                ctx_text += f"**å…³æ³¨é¢†åŸŸ**: {', '.join(extracted['concerns'])}\n"
            if extracted.get("goals"):
                ctx_text += f"**å½“å‰ç›®æ ‡**: {', '.join(extracted['goals'])}\n"
            if extracted.get("pain_points"):
                ctx_text += f"**é¢ä¸´å›°éš¾**: {', '.join(extracted['pain_points'])}\n"
            if extracted.get("life_events"):
                events = [f"{e.get('date', '?')}: {e.get('event', '')}" for e in extracted['life_events'][:5]]
                ctx_text += f"**è¿‘æœŸäº‹ä»¶**: {'; '.join(events)}\n"

        # ç‰¹æ®Šå¤„ç† identity_prism
        if user_context.get("identity_prism"):
            prism = user_context['identity_prism']
            ctx_text += f"\n### èº«ä»½ç‰¹å¾\n"
            if prism.get("core"):
                ctx_text += f"**æ ¸å¿ƒç‰¹è´¨**: {prism['core']}\n"
            if prism.get("inner"):
                ctx_text += f"**å†…åœ¨éœ€æ±‚**: {prism['inner']}\n"
            if prism.get("outer"):
                ctx_text += f"**å¤–åœ¨è¡¨ç°**: {prism['outer']}\n"

        # ç‰¹æ®Šå¤„ç† life_context
        if user_context.get("life_context"):
            life_ctx = user_context['life_context']
            if life_ctx.get("concerns") or life_ctx.get("goals") or life_ctx.get("pain_points"):
                ctx_text += f"\n### ç”Ÿæ´»èƒŒæ™¯\n"
                if life_ctx.get("concerns"):
                    ctx_text += f"**å…³æ³¨**: {', '.join(life_ctx['concerns'])}\n"
                if life_ctx.get("goals"):
                    ctx_text += f"**ç›®æ ‡**: {', '.join(life_ctx['goals'])}\n"
                if life_ctx.get("pain_points"):
                    ctx_text += f"**å›°éš¾**: {', '.join(life_ctx['pain_points'])}\n"

        parts.append(ctx_text)

    return "\n".join(parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¼“å­˜ç®¡ç†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def clear_cache():
    """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
    load_skill.cache_clear()
    load_scenario.cache_clear()
    load_rule.cache_clear()
    get_scenario_triggers.cache_clear()
    get_rule_triggers.cache_clear()
    logger.info("Skill cache cleared")


def reload_skill(skill_id: str) -> Optional[SkillConfig]:
    """é‡è½½å•ä¸ª Skill"""
    load_skill.cache_clear()
    load_scenario.cache_clear()
    load_rule.cache_clear()
    return load_skill(skill_id)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# åœºæ™¯è·¯ç”± (å†…å­˜åŒ¹é…)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_scenario_triggers(text: str) -> Dict[str, Dict[str, List[str]]]:
    """ä» SKILL.md è§£æåœºæ™¯ç›®å½•è¡¨æ ¼ï¼Œæå–è§¦å‘è¯"""
    triggers = {}  # {scenario_id: {primary: [...], secondary: [...]}}

    # åŒ¹é…ä¸‰ä¸ªçº§åˆ«çš„è¡¨æ ¼
    for level in ["entry", "standard", "professional"]:
        pattern = rf'### {level.capitalize()}.*?\n\|[^\n]+\n\|[-\s|]+\n((?:\|[^\n]+\n)*)'
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if not match:
            continue

        for row in match.group(1).strip().split('\n'):
            cols = [c.strip() for c in row.split('|')[1:-1]]
            if len(cols) >= 3:
                # cols[1] = æ–‡ä»¶å, cols[2] = è§¦å‘è¯
                scenario_id = cols[1].replace('.md', '')
                trigger_str = cols[2]
                # è§£æè§¦å‘è¯ï¼ˆç”¨é¡¿å·æˆ–é€—å·åˆ†éš”ï¼‰
                trigger_words = [t.strip() for t in re.split(r'[ã€,ï¼Œ]', trigger_str) if t.strip()]
                triggers[scenario_id] = {
                    "primary": trigger_words,
                    "secondary": []
                }

    return triggers


@lru_cache(maxsize=32)
def get_scenario_triggers(skill_id: str) -> Dict[str, Dict[str, List[str]]]:
    """è·å– Skill çš„åœºæ™¯è§¦å‘è¯æ˜ å°„ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    skill_path = SKILLS_DIR / skill_id / "SKILL.md"
    if not skill_path.exists():
        return {}

    text = skill_path.read_text(encoding='utf-8')
    return parse_scenario_triggers(text)


def route_scenario(skill_id: str, message: str) -> Optional[str]:
    """
    å†…å­˜åœºæ™¯è·¯ç”± - æ ¹æ®ç”¨æˆ·æ¶ˆæ¯åŒ¹é…æœ€ä½³åœºæ™¯

    Args:
        skill_id: Skill ID
        message: ç”¨æˆ·æ¶ˆæ¯

    Returns:
        åŒ¹é…çš„ scenario_idï¼Œæ— åŒ¹é…åˆ™è¿”å› None
    """
    triggers = get_scenario_triggers(skill_id)
    if not triggers:
        return None

    best_match = None
    best_score = 0

    for scenario_id, trigger_config in triggers.items():
        score = 0
        # ä¸»è¦è§¦å‘è¯æƒé‡ 1.0
        for word in trigger_config.get("primary", []):
            if word in message:
                score += 1.0
        # æ¬¡è¦è§¦å‘è¯æƒé‡ 0.5
        for word in trigger_config.get("secondary", []):
            if word in message:
                score += 0.5

        if score > best_score:
            best_score = score
            best_match = scenario_id

    return best_match


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è§„åˆ™è·¯ç”± (v7 æ–°å¢)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@lru_cache(maxsize=32)
def get_rule_triggers(skill_id: str) -> Dict[str, List[str]]:
    """è·å– Skill çš„è§„åˆ™è§¦å‘è¯æ˜ å°„ï¼ˆå¸¦ç¼“å­˜ï¼‰- v7 æ–°å¢

    ä» rules/*.md çš„ frontmatter ä¸­è¯»å– tags ä½œä¸ºè§¦å‘è¯
    """
    triggers = {}  # {rule_id: [tag1, tag2, ...]}

    rules_dir = SKILLS_DIR / skill_id / "rules"
    if not rules_dir.exists():
        return triggers

    for rule_path in rules_dir.glob("*.md"):
        if rule_path.stem.startswith("_"):
            continue

        rule = load_rule(skill_id, rule_path.stem)
        if rule and rule.tags:
            triggers[rule.id] = rule.tags

    return triggers


def route_to_rule(skill_id: str, message: str) -> Optional[str]:
    """
    è§„åˆ™è·¯ç”± - æ ¹æ®ç”¨æˆ·æ¶ˆæ¯åŒ¹é…æœ€ä½³è§„åˆ™ - v7 æ–°å¢

    ä½¿ç”¨ tags è¿›è¡Œè¯­ä¹‰åŒ¹é…ï¼Œæ”¯æŒæ›´çµæ´»çš„è·¯ç”±

    Args:
        skill_id: Skill ID
        message: ç”¨æˆ·æ¶ˆæ¯

    Returns:
        åŒ¹é…çš„ rule_idï¼Œæ— åŒ¹é…åˆ™è¿”å› None
    """
    # ä¼˜å…ˆä½¿ç”¨è§„åˆ™è·¯ç”±
    if has_rules(skill_id):
        triggers = get_rule_triggers(skill_id)
        if not triggers:
            return None

        best_match = None
        best_score = 0

        for rule_id, tags in triggers.items():
            score = 0
            for tag in tags:
                if tag in message:
                    score += 1.0

            if score > best_score:
                best_score = score
                best_match = rule_id

        return best_match

    # å›é€€åˆ°åœºæ™¯è·¯ç”±ï¼ˆå…¼å®¹æ—§æ¶æ„ï¼‰
    return route_scenario(skill_id, message)


def build_rule_prompt(skill_id: str, rule_id: str, user_context: Optional[Dict] = None) -> str:
    """æ„å»ºåŒ…å«è§„åˆ™çš„ System Prompt - v7 æ–°å¢

    Args:
        skill_id: Skill ID
        rule_id: Rule ID
        user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡

    Returns:
        å®Œæ•´çš„ system promptï¼ŒåŒ…å«è§„åˆ™å†…å®¹
    """
    parts = []

    # åŠ è½½ Skill
    skill = load_skill(skill_id)
    if not skill:
        return ""

    # ä¸“å®¶èº«ä»½
    parts.append(f"# {skill.name}\n\n{skill.expert_persona}")

    # åŠ è½½è§„åˆ™
    rule = load_rule(skill_id, rule_id)
    if rule:
        parts.append(f"\n---\n\n## å½“å‰è§„åˆ™: {rule.name}\n")

        # æ·»åŠ è§„åˆ™å®Œæ•´å†…å®¹
        parts.append(rule.content)

    # ä¼¦ç†è¾¹ç•Œ
    if skill.ethics.get("forbidden"):
        ethics_text = "\n## ä¼¦ç†è¾¹ç•Œ\n\n### ç»å¯¹ç¦æ­¢\n"
        for item in skill.ethics["forbidden"]:
            ethics_text += f"- {item}\n"
        parts.append(ethics_text)

    # ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
    if user_context:
        ctx_text = "\n## ç”¨æˆ·ä¸Šä¸‹æ–‡\n"

        if user_context.get("birth_info"):
            birth_info = user_context['birth_info']
            ctx_text += f"\n### å‡ºç”Ÿä¿¡æ¯ï¼ˆå·²æ”¶é›†ï¼Œæ— éœ€å†æ¬¡è¯¢é—®ï¼‰\n"
            ctx_text += f"{json.dumps(birth_info, ensure_ascii=False)}\n"
            ctx_text += f"\n**é‡è¦**: ç”¨æˆ·å·²æä¾›å‡ºç”Ÿä¿¡æ¯ï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œæ’ç›˜å’Œåˆ†æï¼Œæ— éœ€è°ƒç”¨ collect_bazi_info å·¥å…·ã€‚\n"

        if user_context.get("portrait"):
            ctx_text += f"\n### ç”¨æˆ·ç”»åƒ\n{user_context['portrait']}\n"

        if user_context.get("extracted"):
            extracted = user_context['extracted']
            ctx_text += f"\n### ç”¨æˆ·ä¿¡æ¯ (ä»å†å²å¯¹è¯ä¸­æŠ½å–)\n"
            if extracted.get("facts"):
                ctx_text += f"**åŸºæœ¬ä¿¡æ¯**: {json.dumps(extracted['facts'], ensure_ascii=False)}\n"
            if extracted.get("concerns"):
                ctx_text += f"**å…³æ³¨é¢†åŸŸ**: {', '.join(extracted['concerns'])}\n"
            if extracted.get("goals"):
                ctx_text += f"**å½“å‰ç›®æ ‡**: {', '.join(extracted['goals'])}\n"
            if extracted.get("pain_points"):
                ctx_text += f"**é¢ä¸´å›°éš¾**: {', '.join(extracted['pain_points'])}\n"

        parts.append(ctx_text)

    return "\n".join(parts)
