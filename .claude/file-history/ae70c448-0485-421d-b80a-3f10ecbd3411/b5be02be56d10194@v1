"""
SkillLoader v7 - æ”¯æŒ Agentic + Rule æ¶æ„çš„ Skill åŠ è½½å™¨

v7 æ¶æ„ï¼š
- SKILL.md: æ ¸å¿ƒå®šä¹‰ï¼ˆä¸“å®¶èº«ä»½ã€èƒ½åŠ›ç´¢å¼•ã€ä¼¦ç†è¾¹ç•Œï¼‰
- rules/*.md: è§„åˆ™æ–‡ä»¶ï¼ˆåˆ†æè¦ç‚¹ã€è¾“å‡ºè¦æ±‚ã€å¸¸è§é—®é¢˜ï¼‰
- scenarios/*.md: (æ—§æ¶æ„å…¼å®¹) MiniSkill åœºæ™¯æ–‡ä»¶
- tools/*.py: å·¥å…·å®šä¹‰ï¼ˆæ”¶é›†/è®¡ç®—/å±•ç¤º/æ£€ç´¢ï¼‰
"""
import re
import json
import logging
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from functools import lru_cache

logger = logging.getLogger(__name__)

SKILLS_DIR = Path(__file__).parent.parent.parent / "skills"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•°æ®ç»“æ„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class SkillConfig:
    """Skill æ ¸å¿ƒé…ç½®ï¼ˆä» SKILL.md åŠ è½½ï¼‰"""
    id: str
    name: str
    description: str
    expert_persona: str
    scenarios: Dict[str, List[str]]  # {entry: [...], standard: [...], professional: [...]}
    ethics: Dict[str, Any]
    tools: List[str]
    default_scenario: str = "basic_reading"
    triggers: List[str] = field(default_factory=list)
    global_tools: List[str] = field(default_factory=list)  # éœ€è¦çš„å…¨å±€å·¥å…·


@dataclass
class ServiceItem:
    """æœåŠ¡é¡¹ï¼ˆç”¨äºæœåŠ¡ç›®å½•å±•ç¤ºï¼‰"""
    scenario_id: str
    name: str
    icon: str
    description: str
    tier: str  # entry/standard/professional
    billing: str  # å…è´¹/åŸºç¡€/é«˜çº§
    highlights: List[str] = field(default_factory=list)  # ä»·å€¼ç‚¹


@dataclass
class ScenarioConfig:
    """åœºæ™¯é…ç½®ï¼ˆä» scenarios/*.md åŠ è½½ï¼‰"""
    id: str
    name: str
    level: str  # entry/standard/professional
    billing: str  # free/basic/premium
    description: str
    triggers: Dict[str, List[str]]  # {primary: [...], secondary: [...]}
    prerequisites: List[str]
    sop: List[Dict[str, Any]]  # SOP é˜¶æ®µåˆ—è¡¨
    knowledge_config: Dict[str, Any]  # çŸ¥è¯†æ£€ç´¢é…ç½®
    output_config: Dict[str, Any]  # è¾“å‡ºé…ç½®
    tools: List[str]


@dataclass
class ToolMetadata:
    """å·¥å…·å…ƒæ•°æ®"""
    name: str
    description: str
    tool_type: str  # collect/calculate/display/search
    card_type: Optional[str] = None
    card_props_schema: Optional[Dict] = None
    parameters: Dict[str, Any] = field(default_factory=dict)
    when_to_call: Optional[str] = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è§£æå‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_frontmatter(text: str) -> tuple[Dict, str]:
    """è§£æ YAML frontmatter"""
    pattern = r'^---\s*\n(.*?)\n---\s*\n(.*)$'
    match = re.match(pattern, text, re.DOTALL)
    if not match:
        return {}, text

    frontmatter_str, content = match.groups()
    metadata = {}
    current_key = None
    current_list = None

    for line in frontmatter_str.strip().split('\n'):
        stripped = line.strip()
        if not stripped:
            continue
        if stripped.startswith('- ') and current_key:
            if current_list is None:
                current_list = []
            current_list.append(stripped[2:].strip())
            metadata[current_key] = current_list
        elif ':' in line and not line.startswith(' '):
            current_list = None
            key, value = line.split(':', 1)
            current_key = key.strip()
            value = value.strip()
            if value in ('|', ''):
                continue
            elif value.lower() == 'true':
                metadata[current_key] = True
            elif value.lower() == 'false':
                metadata[current_key] = False
            else:
                metadata[current_key] = value

    return metadata, content.strip()


def parse_skill_md(text: str) -> Dict[str, Any]:
    """è§£æ SKILL.md å†…å®¹ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯

    æ”¯æŒä¸¤ç§è§¦å‘è¯æ ¼å¼ï¼š
    1. æ—§æ ¼å¼ï¼šfrontmatter ä¸­çš„ triggers: åˆ—è¡¨
    2. æ–°æ ¼å¼ï¼šdescription ä¸­çš„ "è§¦å‘è¯ï¼šxxxã€xxxã€xxx"
    """
    metadata, content = parse_frontmatter(text)

    # ä» frontmatter è·å–è§¦å‘è¯ï¼ˆæ—§æ ¼å¼ï¼‰
    triggers = metadata.get("triggers", [])

    # å¦‚æœ frontmatter æ²¡æœ‰ triggersï¼Œå°è¯•ä» description ä¸­æå–ï¼ˆæ–°æ ¼å¼ï¼‰
    if not triggers:
        description = metadata.get("description", "")
        # åŒ¹é… "è§¦å‘è¯ï¼šxxxã€xxxã€xxx" æˆ– "è§¦å‘è¯:xxx,xxx,xxx"
        trigger_match = re.search(r'è§¦å‘è¯[ï¼š:]\s*([^ã€‚.]+)', description)
        if trigger_match:
            trigger_str = trigger_match.group(1)
            # æ”¯æŒé¡¿å·ã€é€—å·åˆ†éš”
            triggers = [t.strip() for t in re.split(r'[ã€,ï¼Œ]', trigger_str) if t.strip()]

    result = {
        "id": metadata.get("id", ""),
        "name": metadata.get("name", ""),
        "description": metadata.get("description", ""),
        "triggers": triggers,
        "default_scenario": metadata.get("default_scenario", "basic_reading"),
        "global_tools": metadata.get("global_tools", []),  # ä» frontmatter è¯»å–
    }

    # æå–ä¸“å®¶èº«ä»½
    expert_match = re.search(r'## ä¸“å®¶èº«ä»½\s*\n(.*?)(?=\n## |\Z)', content, re.DOTALL)
    result["expert_persona"] = expert_match.group(1).strip() if expert_match else ""

    # æå–åœºæ™¯ç›®å½•
    scenarios = {"entry": [], "standard": [], "professional": []}
    for level in scenarios.keys():
        pattern = rf'### {level.capitalize()}.*?\n\|.*?\n\|.*?\n((?:\|.*?\n)*)'
        match = re.search(pattern, content, re.IGNORECASE | re.DOTALL)
        if match:
            for row in match.group(1).strip().split('\n'):
                cols = [c.strip() for c in row.split('|')[1:-1]]
                if len(cols) >= 2:
                    scenarios[level].append(cols[1])  # æ–‡ä»¶å
    result["scenarios"] = scenarios

    # æå–ä¼¦ç†è¾¹ç•Œ
    ethics = {"forbidden": [], "sensitive": [], "principles": []}
    forbidden_match = re.search(r'### ç»å¯¹ç¦æ­¢\s*\n((?:- .*?\n)*)', content)
    if forbidden_match:
        ethics["forbidden"] = [l[2:].strip() for l in forbidden_match.group(1).strip().split('\n')]
    result["ethics"] = ethics

    # æå–å·¥å…·åˆ—è¡¨
    tools = []
    tools_match = re.search(r'## å·¥å…·åˆ—è¡¨\s*\n\|.*?\n\|.*?\n((?:\|.*?\n)*)', content)
    if tools_match:
        for row in tools_match.group(1).strip().split('\n'):
            cols = [c.strip() for c in row.split('|')[1:-1]]
            if len(cols) >= 1:
                tools.append(cols[0])
    result["tools"] = tools

    return result


def parse_scenario_md(text: str) -> Dict[str, Any]:
    """è§£æ scenario.md å†…å®¹"""
    metadata, content = parse_frontmatter(text)

    result = {
        "id": metadata.get("id", ""),
        "name": metadata.get("name", ""),
        "level": metadata.get("level", "entry"),
        "billing": metadata.get("billing", "free"),
        "description": metadata.get("description", ""),
    }

    # æå–è§¦å‘æ¡ä»¶
    triggers = {"primary": [], "secondary": []}
    primary_match = re.search(r'\*\*ä¸»è¦è§¦å‘è¯\*\*:\s*(.*?)(?:\n|$)', content)
    if primary_match:
        triggers["primary"] = [t.strip() for t in primary_match.group(1).split(',')]
    secondary_match = re.search(r'\*\*æ¬¡è¦è§¦å‘è¯\*\*:\s*(.*?)(?:\n|$)', content)
    if secondary_match:
        triggers["secondary"] = [t.strip() for t in secondary_match.group(1).split(',')]
    result["triggers"] = triggers

    # æå–å‰ç½®è¦æ±‚
    prereq_match = re.search(r'## å‰ç½®è¦æ±‚\s*\n((?:- .*?\n)*)', content)
    result["prerequisites"] = []
    if prereq_match:
        result["prerequisites"] = [l[2:].strip() for l in prereq_match.group(1).strip().split('\n')]

    # æå– SOP é˜¶æ®µ
    sop = []
    phase_pattern = r'### Phase (\d+):\s*(.*?)\n(.*?)(?=### Phase|\Z)'
    for match in re.finditer(phase_pattern, content, re.DOTALL):
        phase_num, phase_name, phase_content = match.groups()
        phase = {
            "phase": int(phase_num),
            "name": phase_name.strip(),
            "content": phase_content.strip()
        }
        # æå–ç±»å‹
        type_match = re.search(r'\*\*ç±»å‹\*\*:\s*(\w+)', phase_content)
        if type_match:
            phase["type"] = type_match.group(1)
        sop.append(phase)
    result["sop"] = sop

    # æå–å·¥å…·åˆ—è¡¨
    tools_match = re.search(r'\*\*ä½¿ç”¨å·¥å…·\*\*:\s*\n((?:- .*?\n)*)', content)
    result["tools"] = []
    if tools_match:
        result["tools"] = [l[2:].strip() for l in tools_match.group(1).strip().split('\n')]

    # çŸ¥è¯†æ£€ç´¢é…ç½®
    result["knowledge_config"] = {}
    result["output_config"] = {}

    return result


def parse_skill_services(text: str) -> Dict[str, List[Dict[str, Any]]]:
    """è§£æ SKILL.md ä¸­çš„æœåŠ¡ç›®å½•ï¼Œæå–å±•ç¤ºä¿¡æ¯"""
    services = {"entry": [], "standard": [], "professional": []}

    tier_map = {
        "entry": "entry",
        "standard": "standard",
        "professional": "professional"
    }

    for tier_name, tier_key in tier_map.items():
        # åŒ¹é…è¡¨æ ¼å¤´å’Œå†…å®¹
        pattern = rf'### {tier_name.capitalize()}.*?\n\|[^\n]+\n\|[-\s|]+\n((?:\|[^\n]+\n)*)'
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if not match:
            continue

        table_content = match.group(1).strip()
        if not table_content:
            continue

        for row in table_content.split('\n'):
            cols = [c.strip() for c in row.split('|')[1:-1]]
            if len(cols) < 7:  # è‡³å°‘éœ€è¦ 7 åˆ—ï¼ˆåœºæ™¯ã€æ–‡ä»¶ã€è§¦å‘è¯ã€è®¡è´¹ã€å±•ç¤ºåç§°ã€å›¾æ ‡ã€ç®€ä»‹ï¼‰
                continue

            service = {
                "scenario_id": cols[1].replace('.md', '') if cols[1] else cols[0].lower().replace(' ', '_'),
                "name": cols[4] if len(cols) > 4 and cols[4] else cols[0],
                "icon": cols[5] if len(cols) > 5 and cols[5] else "ğŸ“Œ",
                "description": cols[6] if len(cols) > 6 and cols[6] else "",
                "tier": tier_key,
                "billing": cols[3] if len(cols) > 3 else "å…è´¹",
                "highlights": []
            }

            # è§£æä»·å€¼ç‚¹ï¼ˆç¬¬8åˆ—ï¼Œå¦‚æœå­˜åœ¨ï¼‰
            if len(cols) > 7 and cols[7]:
                service["highlights"] = [h.strip() for h in cols[7].split(',')]

            services[tier_key].append(service)

    return services


def get_skill_services(skill_id: str) -> Optional[Dict[str, Any]]:
    """è·å– Skill çš„æœåŠ¡ç›®å½•æ•°æ®ï¼ˆç”¨äº show_skill_services å·¥å…·ï¼‰"""
    skill_path = SKILLS_DIR / skill_id / "SKILL.md"
    if not skill_path.exists():
        return None

    text = skill_path.read_text(encoding='utf-8')
    metadata, _ = parse_frontmatter(text)
    services = parse_skill_services(text)

    return {
        "skill_id": skill_id,
        "skill_name": metadata.get("name", skill_id),
        "description": metadata.get("description", ""),
        "services": services
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# åŠ è½½å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@lru_cache(maxsize=32)
def load_skill(skill_id: str) -> Optional[SkillConfig]:
    """åŠ è½½ Skill æ ¸å¿ƒé…ç½®"""
    skill_path = SKILLS_DIR / skill_id / "SKILL.md"
    if not skill_path.exists():
        return None

    text = skill_path.read_text(encoding='utf-8')
    data = parse_skill_md(text)

    return SkillConfig(
        id=data.get("id", skill_id),
        name=data.get("name", skill_id),
        description=data.get("description", ""),
        expert_persona=data.get("expert_persona", ""),
        scenarios=data.get("scenarios", {}),
        ethics=data.get("ethics", {}),
        tools=data.get("tools", []),
        default_scenario=data.get("default_scenario", "basic_reading"),
        triggers=data.get("triggers", []),
        global_tools=data.get("global_tools", [])
    )


@lru_cache(maxsize=64)
def load_scenario(skill_id: str, scenario_id: str) -> Optional[ScenarioConfig]:
    """åŠ è½½åœºæ™¯é…ç½®"""
    scenario_path = SKILLS_DIR / skill_id / "scenarios" / f"{scenario_id}.md"
    if not scenario_path.exists():
        return None

    text = scenario_path.read_text(encoding='utf-8')
    data = parse_scenario_md(text)

    return ScenarioConfig(
        id=data.get("id", scenario_id),
        name=data.get("name", scenario_id),
        level=data.get("level", "entry"),
        billing=data.get("billing", "free"),
        description=data.get("description", ""),
        triggers=data.get("triggers", {}),
        prerequisites=data.get("prerequisites", []),
        sop=data.get("sop", []),
        knowledge_config=data.get("knowledge_config", {}),
        output_config=data.get("output_config", {}),
        tools=data.get("tools", [])
    )


def get_available_skills() -> List[str]:
    """è·å–æ‰€æœ‰å¯ç”¨çš„ Skill"""
    if not SKILLS_DIR.exists():
        return []
    return [p.name for p in SKILLS_DIR.iterdir()
            if p.is_dir() and (p / "SKILL.md").exists()]


def get_skill_scenarios(skill_id: str) -> List[str]:
    """è·å– Skill çš„æ‰€æœ‰åœºæ™¯

    æ”¯æŒä¸¤ç§æ¶æ„ï¼š
    1. æ—§æ¶æ„ï¼šscenarios/*.md
    2. æ–°æ¶æ„ï¼šrules/*.mdï¼ˆAgentic æ¶æ„ï¼‰
    """
    # ä¼˜å…ˆæ£€æŸ¥ rules ç›®å½•ï¼ˆæ–°æ¶æ„ï¼‰
    rules_dir = SKILLS_DIR / skill_id / "rules"
    if rules_dir.exists():
        rules = [p.stem for p in rules_dir.glob("*.md") if not p.stem.startswith("_")]
        if rules:
            return rules

    # å›é€€åˆ° scenarios ç›®å½•ï¼ˆæ—§æ¶æ„ï¼‰
    scenarios_dir = SKILLS_DIR / skill_id / "scenarios"
    if not scenarios_dir.exists():
        return []
    return [p.stem for p in scenarios_dir.glob("*.md")]


def get_skill_triggers() -> Dict[str, List[str]]:
    """è·å–æ‰€æœ‰ Skill çš„è§¦å‘è¯"""
    triggers = {}
    for skill_id in get_available_skills():
        if skill_id == "core":
            continue
        skill = load_skill(skill_id)
        if skill and skill.triggers:
            triggers[skill_id] = skill.triggers
    return triggers


def get_skill_global_tools(skill_id: str) -> List[str]:
    """è·å– Skill å£°æ˜éœ€è¦çš„å…¨å±€å·¥å…·åˆ—è¡¨"""
    skill = load_skill(skill_id)
    if skill and skill.global_tools:
        return skill.global_tools
    # é»˜è®¤è¿”å›æ‰€æœ‰å…¨å±€å·¥å…·ï¼ˆå‘åå…¼å®¹ï¼‰
    return []


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# System Prompt æ„å»º
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_system_prompt(
    skill_id: str,
    scenario_id: Optional[str] = None,
    user_context: Optional[Dict] = None
) -> str:
    """æ„å»ºå®Œæ•´çš„ System Prompt"""
    parts = []

    # åŠ è½½ Skill
    skill = load_skill(skill_id)
    if not skill:
        return ""

    # ä¸“å®¶èº«ä»½
    parts.append(f"# {skill.name}\n\n{skill.expert_persona}")

    # åŠ è½½åœºæ™¯
    if scenario_id:
        scenario = load_scenario(skill_id, scenario_id)
        if scenario:
            parts.append(f"\n---\n\n## å½“å‰åœºæ™¯: {scenario.name}\n\n{scenario.description}")

            # SOP
            if scenario.sop:
                sop_text = "\n## æœåŠ¡æµç¨‹ (SOP)\n\n"
                for phase in scenario.sop:
                    sop_text += f"### Phase {phase['phase']}: {phase['name']}\n{phase['content']}\n\n"
                parts.append(sop_text)

    # ä¼¦ç†è¾¹ç•Œ
    if skill.ethics.get("forbidden"):
        ethics_text = "\n## ä¼¦ç†è¾¹ç•Œ\n\n### ç»å¯¹ç¦æ­¢\n"
        for item in skill.ethics["forbidden"]:
            ethics_text += f"- {item}\n"
        parts.append(ethics_text)

    # ç”¨æˆ·ä¸Šä¸‹æ–‡ - è‡ªåŠ¨å¤„ç†æ‰€æœ‰å­—æ®µ
    if user_context:
        ctx_text = "\n## ç”¨æˆ·ä¸Šä¸‹æ–‡\n"

        # ç‰¹æ®Šå¤„ç† birth_info
        if user_context.get("birth_info"):
            birth_info = user_context['birth_info']
            ctx_text += f"\n### å‡ºç”Ÿä¿¡æ¯ï¼ˆå·²æ”¶é›†ï¼Œæ— éœ€å†æ¬¡è¯¢é—®ï¼‰\n"
            ctx_text += f"{json.dumps(birth_info, ensure_ascii=False)}\n"
            ctx_text += f"\n**é‡è¦**: ç”¨æˆ·å·²æä¾›å‡ºç”Ÿä¿¡æ¯ï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œæ’ç›˜å’Œåˆ†æï¼Œæ— éœ€è°ƒç”¨ collect_bazi_info å·¥å…·ã€‚\n"

        # ç‰¹æ®Šå¤„ç† portrait
        if user_context.get("portrait"):
            ctx_text += f"\n### ç”¨æˆ·ç”»åƒ\n{user_context['portrait']}\n"

        # ç‰¹æ®Šå¤„ç† extracted (æŠ½å–çš„ç”¨æˆ·ä¿¡æ¯)
        if user_context.get("extracted"):
            extracted = user_context['extracted']
            ctx_text += f"\n### ç”¨æˆ·ä¿¡æ¯ (ä»å†å²å¯¹è¯ä¸­æŠ½å–)\n"
            if extracted.get("facts"):
                ctx_text += f"**åŸºæœ¬ä¿¡æ¯**: {json.dumps(extracted['facts'], ensure_ascii=False)}\n"
            if extracted.get("concerns"):
                ctx_text += f"**å…³æ³¨é¢†åŸŸ**: {', '.join(extracted['concerns'])}\n"
            if extracted.get("goals"):
                ctx_text += f"**å½“å‰ç›®æ ‡**: {', '.join(extracted['goals'])}\n"
            if extracted.get("pain_points"):
                ctx_text += f"**é¢ä¸´å›°éš¾**: {', '.join(extracted['pain_points'])}\n"
            if extracted.get("life_events"):
                events = [f"{e.get('date', '?')}: {e.get('event', '')}" for e in extracted['life_events'][:5]]
                ctx_text += f"**è¿‘æœŸäº‹ä»¶**: {'; '.join(events)}\n"

        # ç‰¹æ®Šå¤„ç† identity_prism
        if user_context.get("identity_prism"):
            prism = user_context['identity_prism']
            ctx_text += f"\n### èº«ä»½ç‰¹å¾\n"
            if prism.get("core"):
                ctx_text += f"**æ ¸å¿ƒç‰¹è´¨**: {prism['core']}\n"
            if prism.get("inner"):
                ctx_text += f"**å†…åœ¨éœ€æ±‚**: {prism['inner']}\n"
            if prism.get("outer"):
                ctx_text += f"**å¤–åœ¨è¡¨ç°**: {prism['outer']}\n"

        # ç‰¹æ®Šå¤„ç† life_context
        if user_context.get("life_context"):
            life_ctx = user_context['life_context']
            if life_ctx.get("concerns") or life_ctx.get("goals") or life_ctx.get("pain_points"):
                ctx_text += f"\n### ç”Ÿæ´»èƒŒæ™¯\n"
                if life_ctx.get("concerns"):
                    ctx_text += f"**å…³æ³¨**: {', '.join(life_ctx['concerns'])}\n"
                if life_ctx.get("goals"):
                    ctx_text += f"**ç›®æ ‡**: {', '.join(life_ctx['goals'])}\n"
                if life_ctx.get("pain_points"):
                    ctx_text += f"**å›°éš¾**: {', '.join(life_ctx['pain_points'])}\n"

        parts.append(ctx_text)

    return "\n".join(parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¼“å­˜ç®¡ç†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def clear_cache():
    """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
    load_skill.cache_clear()
    load_scenario.cache_clear()
    get_scenario_triggers.cache_clear()
    logger.info("Skill cache cleared")


def reload_skill(skill_id: str) -> Optional[SkillConfig]:
    """é‡è½½å•ä¸ª Skill"""
    load_skill.cache_clear()
    load_scenario.cache_clear()
    return load_skill(skill_id)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# åœºæ™¯è·¯ç”± (å†…å­˜åŒ¹é…)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_scenario_triggers(text: str) -> Dict[str, Dict[str, List[str]]]:
    """ä» SKILL.md è§£æåœºæ™¯ç›®å½•è¡¨æ ¼ï¼Œæå–è§¦å‘è¯"""
    triggers = {}  # {scenario_id: {primary: [...], secondary: [...]}}

    # åŒ¹é…ä¸‰ä¸ªçº§åˆ«çš„è¡¨æ ¼
    for level in ["entry", "standard", "professional"]:
        pattern = rf'### {level.capitalize()}.*?\n\|[^\n]+\n\|[-\s|]+\n((?:\|[^\n]+\n)*)'
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if not match:
            continue

        for row in match.group(1).strip().split('\n'):
            cols = [c.strip() for c in row.split('|')[1:-1]]
            if len(cols) >= 3:
                # cols[1] = æ–‡ä»¶å, cols[2] = è§¦å‘è¯
                scenario_id = cols[1].replace('.md', '')
                trigger_str = cols[2]
                # è§£æè§¦å‘è¯ï¼ˆç”¨é¡¿å·æˆ–é€—å·åˆ†éš”ï¼‰
                trigger_words = [t.strip() for t in re.split(r'[ã€,ï¼Œ]', trigger_str) if t.strip()]
                triggers[scenario_id] = {
                    "primary": trigger_words,
                    "secondary": []
                }

    return triggers


@lru_cache(maxsize=32)
def get_scenario_triggers(skill_id: str) -> Dict[str, Dict[str, List[str]]]:
    """è·å– Skill çš„åœºæ™¯è§¦å‘è¯æ˜ å°„ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    skill_path = SKILLS_DIR / skill_id / "SKILL.md"
    if not skill_path.exists():
        return {}

    text = skill_path.read_text(encoding='utf-8')
    return parse_scenario_triggers(text)


def route_scenario(skill_id: str, message: str) -> Optional[str]:
    """
    å†…å­˜åœºæ™¯è·¯ç”± - æ ¹æ®ç”¨æˆ·æ¶ˆæ¯åŒ¹é…æœ€ä½³åœºæ™¯

    Args:
        skill_id: Skill ID
        message: ç”¨æˆ·æ¶ˆæ¯

    Returns:
        åŒ¹é…çš„ scenario_idï¼Œæ— åŒ¹é…åˆ™è¿”å› None
    """
    triggers = get_scenario_triggers(skill_id)
    if not triggers:
        return None

    best_match = None
    best_score = 0

    for scenario_id, trigger_config in triggers.items():
        score = 0
        # ä¸»è¦è§¦å‘è¯æƒé‡ 1.0
        for word in trigger_config.get("primary", []):
            if word in message:
                score += 1.0
        # æ¬¡è¦è§¦å‘è¯æƒé‡ 0.5
        for word in trigger_config.get("secondary", []):
            if word in message:
                score += 0.5

        if score > best_score:
            best_score = score
            best_match = scenario_id

    return best_match
